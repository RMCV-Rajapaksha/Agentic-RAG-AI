# Presentation by Malit Jing [Music] Am I audible? You guys can hear me. Okay. Okay, let's get started. So, my name is Malit Jing. I'm the VP of research and AI at WSO2. I've been with the company for almost 10 years now. ## Topic Overview What I will be talking about today is how to take your AI app to production. ### Historical Context Let's get started. I want to tell you a story about how we used to do things back then. This was something that we worked on several years ago. - This was a low code tool where integration developers could come and develop use cases. - Our AI feature that we were trying to implement was to see whether we could predict the next low code element. The underlying programming language here was Ballerina. We wanted to see whether we could predict the next connector, connector action, or statement. ### Initial Steps How did we do this back then? This involved multiple steps. - Of course, large language models weren't that powerful, so we had to start off with a set of Ballerina programs to train a specialized model. - We went to public repositories and collected a Ballerina dataset. - There was heavy pre-processing that we had to do. We needed to get the abstract syntax tree of these Ballerina programs, which is a tree-like representation. - We had to further pre-process these abstract syntax trees to map them to the visual layer to improve accuracy. ### Annotation and Model Training The story didn't end there. We had to go through an annotation step where we were doing extensive labeling of sequences up to the current element and the next element. - This was very time-consuming and costly. - Then we went through the phase of training the model. We trained multiple models, one of which was LSTM, among others. - Only then could we check whether it was accurate or not. If it wasn't accurate enough, we had to go back, collect more data, change our pre-processing steps, and adjust the way we annotated. ### Emergence of Pre-trained Models With the emergence of pre-trained models, AI application development has accelerated. - You don't have to train specialized models for all use cases. - Data pre-processing, annotation, and model training have been minimized for a large number of applications. ### Current Implementation Today, we have the same feature in the Ballerina Integrator. - We try to achieve the same thing. If you go to the Ballerina Integrator in the visual editor, we try to predict the next element. - The way we do it today is through an LLM call, consuming AI as a service and providing the right context, which is the information about the program. - There’s no pre-processing, labeling, or training needed. ### Building AI Applications When it comes to building AI apps, this involves two things: 1. **Building your AI component**: This could be a RAG or agents, or a combination of these patterns. 2. **Integrating this into the larger part of the application**: Building the AI piece can be thought of as an integration problem. The shift has moved from model training to integrating systems. ### Co-pilot Example This is another co-pilot we have built. The previous one was just the LLM call; this is the co-pilot we have in Coro. - It has a complex architecture with multiple layers of agents that are reasoning and acting. - The prototyping of these apps is now relatively easy because of these generative models, agents, and well-known patterns. ### Challenges in Production However, when you try to take it to production, you will face problems. Research shows that 74% of AI projects fail when moving to production. - The reason for this is the various complexities and challenges you will face, such as: - Integration challenges - Evaluation-related issues - Security-related issues - Governance and monitoring For the most part, these are software engineering problems. Addressing these challenges requires a new set of capabilities and abstractions in your product stack. ### Tool Set Decision Where do we start? The first thing is to decide on the tool set. - As a developer, if I want to bring some intelligence into my application, I need to decide what tools to use. - This would depend on your background or your organization’s preferred set of tools. - You may be a low code developer or a pro code developer. If you're low code, you might not want to write code, or you may be an integration developer wanting to use integration tools to bring AI into your application. - If you are an AI developer, you may prefer using Python or frameworks built on top of that, such as Crew AI. ### Integration Tools More and more, we are seeing that this can be mapped to solving a problem in the integration domain. - Integration tools that have AI app building capabilities can accelerate development. - AI app building capabilities include agent building capabilities, RAGs, and so on. - Low code tools are becoming popular because standard patterns are emerging, which can be made available as templates for quick app building. ### Mobile App Example Towards WSO2 Con, we wanted to do something related to AI. - We thought, let's take our existing mobile app, which we had already developed last year, and try to bring some AI into this. - This app had two flows: the registration flow and the ability to get information about the conference. We wanted to give a more personalized user experience by introducing four capabilities: 1. **Chat Experience**: Being able to ask questions about the schedule and speakers. 2. **Session Advisor**: Suggesting sessions based on your profile. 3. **Attendee Connections**: Connecting you with people who have similar backgrounds. 4. **Expert Finder**: Finding related experts at the conference. ### Implementation Highlights Building something like this requires identifying the patterns to implement. - We have been discussing four patterns. In the keynote, Rana presented these key patterns: 1. An application that consumes generative AI APIs for code generation. 2. RAG as part of the application, grounding AI responses with knowledge for higher accuracy. 3. Decision-making on data sources for RAG. 4. Building an agent and deciding on the tools needed. ### Personalization Agent One of the agents we built is the personalization agent. - This agent creates a profile of a person and stores it in a knowledge base for personalized experiences. - We use two tools: the surfer tool for search queries and a web scraper to gather content. ### Integration with Larger Application We integrate this with the registration flow. - Once the user registers through the web interface, it not only stores the information in SQL but also makes a call to the agent for web search and scraping information. ### Dynamic User Consent Now, let's say you want to get more information from LinkedIn. - In this case, obtaining user consent for API access becomes necessary. - The agent must be redirected to obtain user consent before resuming execution. ### AI as a Service AI apps are increasingly accessing external AI models as a service. - In our previous example, we used one model for everything, but now we have multiple models. - Managing connections, securing them, and governance are essential requirements for these AI apps. ### AI API Management This is where AI API management comes into the picture. - Existing API management solutions need to be extended to support AI applications. - Monitoring AI usage, throttling based on tokens, and ensuring PII protection are critical. ### Conclusion In the past, people spent years building models before they could build a prototype. - Today, you can build a prototype, but taking it to production presents struggles due to various complexities. - Integrating AI systems, securing them, and monitoring them are significant challenges. We have made significant progress, but there is still a lot to do. - We are expanding existing capabilities, such as the AI gateway, and introducing new capabilities for evaluating AI systems and agent observability. Thank you very much! [Music]