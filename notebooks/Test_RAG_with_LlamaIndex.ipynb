{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2WqJ0dPPDaF",
        "outputId": "e73a18c5-bc80-411f-fbe3-32d801b6081f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RM_EYWeWPP_-"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUsNG2b5PQCR",
        "outputId": "f059d804-0ca7-4def-8f24-6e001fc6b781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -Uq llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IZOJCPdrPQFH"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "docs = SimpleDirectoryReader(input_dir=\"./data\").load_data()\n",
        "\n",
        "# file name as id\n",
        "# docs_nam_as_id = SimpleDirectoryReader(input_dir=\"./data\", filename_as_id=True).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVHzGiXEPQH3",
        "outputId": "ba56b93d-4589-4a95-c020-04d9fe6a7376"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id_='487b9093-1ad3-4685-ab72-3428a9365856', embedding=None, metadata={'page_label': '1', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='HealthGPT: A Medical Large Vision-Language Model for Unifying\\nComprehension and Generation via Heterogeneous Knowledge Adaptation\\nTianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\\nMengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\\n1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\\n4The Hong Kong University of Science and Technology,5National University of Singapore\\nProject Page\\n Code\\n1. X-Ray \\nComprehension\\n2. CT\\nComprehension\\n3. MRI\\nComprehension\\nComp. Perf.\\n7 Medical Multi-Modal Comprehension Tasks\\nGen. \\nPerformance\\n5 Medical Multi-Modal Generation Tasks\\nList all anatomical locations showing \\npulmonary edema, hazy opacity, or \\nmediastinal displacement.\\nLeft hilar structures, left lung,\\nright hilar structures, right lung.\\nWhich abdominal organ shows any \\nindication of a lesion or abnormality \\nin the CT image?\\nNo abdominal organs show\\nany clear indications of lesions\\nor abnormalities.\\nCould you explain what this mass\\nin the MRI means for my health? \\nIs it very serious?\\nCertainly, the MRI shows a\\ndefined mass in your left nasal\\ncavity. There is no sign of the ...\\n4. Microsopy Comprehension\\n What is the purpose of the different \\nmembrane treatments used in this \\nstudy?\\nThe purpose of the different\\nmembrane treatments used…\\n5. OCT Comprehension\\n What is the purpose of comparing \\nthe OCT structure image and OCTA \\nimage with H&E histology?\\nTo confirm the histological position\\nof the obtained OCT brain images.\\n6. Fundus\\nComprehension\\n What specific findings or pathological \\nchanges can be observed in this \\nfundus image?\\nThe fundus image appears normal with\\nno noticeable signs of pathology…\\n7. Ultrasound\\nComprehension\\n What type of imaging technique \\nis used in this image?\\nThe image is a sagittal gray-\\nscale ultrasonographic…\\n1. CT2MRI\\nGeneration\\nI need a version of this CT representation \\nin MRI.\\nThe image has\\nbeen transformed\\ninto MRI.\\n2. MRI2CT\\nGeneration\\nTransform the MRI display into a \\nCT image.\\nHere is the CT\\nversion of the\\nMRI image.\\n3. Image Reconstruction\\nReconstruct the following \\nmedical images.\\nHere is the reconstructed\\nmedical image you need.\\n4. Super Resolution\\nCould you improve the quality\\nof this MRI image?\\nHere is the image with\\nimproved resolution.\\n5. Report-to-CXR\\nThe X-ray shows no \\npleural effusion or \\npneumothorax.\\nHere is the\\nchest X-ray\\nimage for\\nyou.\\nGen. Perf.\\nFigure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\\nunified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\\nplex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\\nAbstract\\nWe present HealthGPT, a powerful Medical Large Vision-\\nLanguage Model (Med-LVLM) that integrates medical vi-\\nsual comprehension and generation capabilities within a uni-\\nfied autoregressive paradigm. Our bootstrapping philosophy\\nis to progressively adapt heterogeneous comprehension and\\ngeneration knowledge to pre-trained large language mod-\\nels (LLMs). This is achieved through a novel heterogeneous\\nlow-rank adaptation (H-LoRA) technique, which is com-\\nplemented by a tailored hierarchical visual perception ap-\\nproach and a three-stage learning strategy. To effectively\\nlearn the HealthGPT, we devise a comprehensive medi-\\ncal domain-specific comprehension and generation dataset\\ncalled VL-Health. Experimental results demonstrate ex-\\nceptional performance and scalability of HealthGPT in\\nmedical visual unified tasks. Our project can be accessed at\\nhttps://github.com/DCDmllm/HealthGPT.\\n1 Introduction\\nLarge Vision-Language Models (LVLMs) (Liu et al. 2023;\\nOpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\\ndemonstrated outstanding open-world visual comprehension\\nand reasoning abilities through language-based interactive\\ndialogue over the past years, simultaneously opening up\\nnew opportunities for applications in specialized domains.\\n1\\narXiv:2502.09838v3  [cs.CV]  21 Feb 2025', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9582fe07-8d83-4ad4-a92e-e9058d05b328', embedding=None, metadata={'page_label': '2', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Specifically, recent studies (Li et al. 2024a; Tu et al. 2024)\\nhave utilized pre-trained large language models (LLMs) and\\nvisual instruction data to build interactive diagnostic tools\\nand treatment planning systems, revealing the immense po-\\ntential of LVLMs in medical scenarios. However, these stud-\\nies primarily concentrate on visual comprehension tasks that\\nproduce text-based outputs, such as medical visual ques-\\ntion answering (Li et al. 2024a) or report generation (Nath\\net al. 2024), and deficient the “drawing” capability needed\\nfor medical visual generation. In practice, integrating visual\\ncomprehension and generation can significantly enhance the\\nmultifunctionality of medical LVLMs.\\nRecent studies have increasingly focused on developing\\nunified LVLMs capable of comprehending and generating\\ncontent across diverse visual modalities. Earlier approaches\\npredominantly utilized continuous visual tokens fed into\\nLLMs, using the LLMs themselves as conditional genera-\\ntors for external generative models (Ge et al. 2024; Wu et al.\\n2023; Dong et al. 2023). More recent research has explored\\nthe use of discrete visual tokens for image representation and\\ngeneration within a fully autoregressive framework (Team\\n2024; Wang et al. 2024a; Xie et al. 2024). These meth-\\nods not only enhance controllability but also demonstrate\\nearly success in open-world, any-to-any tasks, highlighting\\nthe preliminary potential of a unified autoregressive learning\\nparadigm in multi-modal tasks.\\nWhile unified LVLMs have achieved initial success in\\ngeneral scenarios, such a unified framework remains under-\\nexplored in the medical domain. Adapting the aforemen-\\ntioned general unified model paradigm to the medical do-\\nmain presents two major challenges: (i) High-scale and\\n-quality Data Limitations . Open-world models necessi-\\ntate extensive pre-training on billions or even more diverse,\\nmulti-modal data samples for comprehension and genera-\\ntion tasks (Lu et al. 2024; Team 2024). However, the ac-\\ncessible medical data significantly lacks in scale and qual-\\nity compared to natural multi-modal datasets. Its special-\\nized and domain-specific characteristics make it challenging\\nto develop a unified medical model from scratch. (ii) Con-\\nflicts between Comprehension and Generation . Compre-\\nhension tasks often strip away visual details to focus on\\nabstraction, while generation tasks require detailed preser-\\nvation, making tokens sensitive to all visual alterations. As\\nshown in Figure 2, which features experiments conducted on\\nmedical images, the performance in comprehension (or gen-\\neration) tasks steadily decreases as the proportion of genera-\\ntion (or comprehension) data increases, and vice versa. This\\nhighlights a dilemma in autoregressive multi-modal training,\\nstemming from the need to maintain consistency between\\npre- and post-LVLMs. While some methods have explored\\nmutual enhancement between comprehension and genera-\\ntion (Pan et al. 2024; Tong et al. 2024), improvements still\\nexhibit diminishing returns, with performance degradation\\nremaining a significant issue.\\n(a) (b)\\nFigure 2: With a fixed amount of comprehension (genera-\\ntion) data, increasing the proportion of the other type leads\\nto significant performance degradation.\\nTo tackle the aforementioned challenges, we propose\\nHealthGPT (see Figure 1) , which progressively adapts a\\npre-trained LLM as an unified medical multi-modal model\\nwith a small amount of visual instruction data. We de-\\nvise innovative Parameter-Efficient Fine-Tuning (PEFT) ap-\\nproach (Ding et al. 2023), calledHeterogeneous Low-Rank\\nAdaptation (H-LoRA), which decouples the learning pro-\\ncess of LVLMs for comprehension and generation tasks. In-\\nspired by the plug-and-play nature of LoRA (Hu et al. 2021),\\nH-LoRA enables the model to store heterogeneous compre-\\nhension and generation knowledge in independent “plug-\\nins”, thus avoiding joint optimization issues caused by con-\\nflicts between comprehension and generation tasks. In addi-\\ntion, we also consider the variety of sub-tasks among com-\\nprehension or generation tasks. Qualitative research high-\\nlights the limitations of a single LoRA in handling multi-\\ndimensional task scenarios, mainly due to catastrophic for-\\ngetting and interference (Liu et al. 2024d; Lin et al. 2024).\\nTo address this, we draw on the concept of Mixture of Ex-\\nperts (MoE) (Masoudnia and Ebrahimpour 2014) and in-\\ntroduce LoRA experts. The aim is to dynamically transfer\\ntask-shared knowledge to adapt to downstream tasks. Unlike\\nMoELoRA (Luo et al. 2024a), H-LoRA employs reversible\\nmatrix block multiplication to combine LoRA experts, sig-\\nnificantly reducing the overhead of multiple matrix multi-\\nplications. Notably, when using four experts, it requires\\nonly 67% of the MoELoRA training time.\\nTo effectively leverage H-LoRA inHealthGPT, we fur-\\nther introduce a Hierarchical Visual Perception (HVP)\\nand devise a correspondingThree-stage Learning Strategy\\n(TLS). HVP: we separate visual details learning from Vi-\\nsion transformer (ViT) for comprehension and generation.\\nAs is widely recognized, the ViT encodes visual concepts\\nwith increasing abstraction, generally, becoming finer as we\\nprogress over levels (Vig 2019). Thus, we maintain the vi-\\nsual features of the anterior and posterior layers to accom-\\nmodate the differing requirements for visual granularity in\\ncomprehension and generation tasks while preventing po-\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='009972f7-4fc4-436e-82c5-cbb05e68c6fa', embedding=None, metadata={'page_label': '3', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tential task interference. TLS: In the first and second stages,\\ngiven the heterogeneity between comprehension and gener-\\nation tasks, we first train H-LoRA plugins for HealthGPT\\nto incorporate both medical comprehension and generation\\nknowledge, thus endowing the LLMs with capabilities for\\nvision-language alignment and vision-to-vision reconstruc-\\ntion. Additionally, through minimal mixed-task training, we\\nbuilt fusion embedding layers and output heads that merge\\ntext and visual tokens, establishing a unified LVLM founda-\\ntion for visual instruction fine-tuning. In the third stage, by\\nonly training the H-LoRA plugins, HealthGPT is able to\\nrapidly adapt to a wide range of downstream medical tasks,\\ncovering various types of medical comprehension and gen-\\neration tasks.\\nTo effectively implement our approach, we have cu-\\nrated a dataset for training unified medical LVLMs, called\\nVL-Health, including seven comprehension tasks and five\\ngeneration tasks (Figure 1). Through quantitative analysis\\nand validation on multi-modal tasks, the results demonstrate\\nthat HealthGPT is capable of unifying medical multi-\\nmodal abilities in data-constrained scenarios, achieving per-\\nformance comparable to or better than existing state-of-the-\\nart (SOTA) models across multiple metrics. Overall, the\\nmain contributions of this paper are summarized as follows:\\n• Unified Med-LVLM. We introduce HealthGPT,\\nwhich, to the best of our knowledge, is the first unified\\nframework for multi-modal comprehension and genera-\\ntion in complex medical scenarios.\\n• Effective Learning Paradigm. We present H-LoRA, an\\noptimized multi-LoRA PEFT architecture based on task-\\ngated decoupling, is designed to effectively mitigate data\\nconflict issues.\\n• Holistic Training Dataset. We curated VL-Health, a\\ncomprehensive dataset designed for both comprehension\\nand generation tasks.\\n• Superior Downstream Improvements : Extensive ex-\\nperiments are conducted and the results confirm\\nHealthGPT’s effectiveness in medical vision-language\\ncomprehension and generation.\\n2 Related Work\\nMedical Vision Large Language Models. Recently, medi-\\ncal vision large language models (Med-VLLMs) have made\\nsignificant progress, demonstrating excellent performance\\nin understanding medical images and responding to human\\nqueries based on these images (Zhou et al. 2023; Tian et al.\\n2023). XrayGPT (Thawkar et al. 2023) combines a med-\\nical visual encoder (MedClip) (Wang et al. 2022) with a\\nfine-tuned LLM , using a simple linear transformation layer\\nto achieve alignment between visual and textual informa-\\ntion, significantly enhancing the understanding of medical\\nimages. On this basis, LLaV A-Med (Li et al. 2024b) fur-\\nther enhances visual-text alignment in medical contexts by\\nselecting high-quality image-text pairs from PubMed pa-\\npers and synthesized VQA datasets. BiomedGPT (Luo et al.\\n2024b) employs a BERT-style encoder and GPT-style de-\\ncoder architecture, pre-trained on interdisciplinary datasets.\\nCompared to commercial models like Med-PaLM (Singhal\\net al. 2023), BiomedGPT significantly reduces model size\\nwhile maintaining superior performance. However, issues\\nof language adaptability and dataset specificity still remain.\\nTo address these, HuatuoGPT-Vision (Chen et al. 2024a)\\nintroduces the PubMedVision dataset, which contains 1.3\\nmillion high-quality medical samples, significantly improv-\\ning the model’s adaptability across diverse medical applica-\\ntions. However, current Med-VLLMs mainly focus on med-\\nical comprehension and lack the capability for the medical\\nvision-language generation.\\nUnified Visual Comprehension and Generation Mod-\\nels. Recent research has increasingly concentrated on cre-\\nating unified LVLMs that are adept at understanding and\\nproducing content across various visual modalities. NExT-\\nGPT (Wu et al. 2023) achieves perception and generation for\\narbitrary combinations of multi-modal inputs and outputs by\\naligning LLMs. Similarly, SEED (Ge et al. 2023), SEED-\\nX (Ge et al. 2024), and DreamLLM (Dong et al. 2023) em-\\nploy learnable queries and leverage next-token prediction to\\ngenerate visual tokens, providing conditional inputs to exter-\\nnal generation modules. Unlike these methods, which func-\\ntion as external conditioners, Unified-IO (Lu et al. 2022),\\nUnified-IO 2 (Lu et al. 2024), and Chameleon (Team 2024)\\ninternalize multi-modal generation tasks within a unified\\nTransformer architecture by extending multi-modal vocab-\\nularies, enabling direct generation based on next-token pre-\\ndiction. Building on this concept, Lumina-mGPT (Liu et al.\\n2024a) and ANOLE (Chern et al. 2024) further enhance the\\ngeneration capabilities of unified models using high-quality\\ndata, particularly improving the quality and flexibility of im-\\nage generation.\\n3 Preliminaries\\nLarge Vision-Language Models.The input to a LVLM typ-\\nically consists of an image ximg and a discrete text sequence\\nxtxt. The visual encoder Eimg converts the input image ximg\\ninto a sequence of visual tokens V = [ vi]Nv\\ni=1, while the\\ntext sequence xtxt is mapped into a sequence of text to-\\nkens T = [ ti]Nt\\ni=1 using an embedding function Etxt. The\\nLLM MLLM(·|θ) models the joint probability of the token\\nsequence U = {V, T }, which is expressed as:\\nPθ(R|U) =\\nNrY\\ni=1\\nPθ(ri|{U, r<i}), (1)\\nwhere R = [ri]Nr\\ni=1 is the text response sequence. The LVLM\\niteratively generates the next token ri based on r<i. The op-\\ntimization objective is to minimize the cross-entropy loss of\\nthe response R. It is worth noting that most LVLMs adopt\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9d24e4e3-4794-421f-87d8-932b3548a4a9', embedding=None, metadata={'page_label': '4', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3: The HealthGPT architecture integrates hierarchical visual perception and H-LoRA, employing a task-specific hard\\nrouter to select visual features and H-LoRA plugins, ultimately generating outputs with an autoregressive manner.\\na design paradigm based on ViT, alignment adapters, and\\npre-trained LLMs(Liu et al. 2023, 2024b), enabling quick\\nadaptation to downstream tasks.\\nVQGAN. VQGAN (Esser, Rombach, and Ommer 2021)\\nemploys latent space compression and indexing mechanisms\\nto effectively learn a complete discrete representation of im-\\nages. VQGAN first maps the input imageximg to a latent rep-\\nresentation z = E(x) through a encoder E. Then, the latent\\nrepresentation is quantized using a codebookZ = {zk}K\\nk=1,\\ngenerating a discrete index sequence I = [im]N\\nm=1, where\\nim ∈ Zrepresents the quantized code index:\\nI = Quantize(z|Z) = arg min\\nzk∈Z\\n∥z − zk∥2. (2)\\nIn our approach, the discrete index sequence I serves as\\na supervisory signal for the generation task, enabling the\\nmodel to predict the index sequence ˆI from input conditions\\nsuch as text or other modality signals. Finally, the predicted\\nindex sequence ˆI is upsampled by the VQGAN decoder G,\\ngenerating the high-quality image ˆximg = G(ˆI).\\nLow Rank Adaptation. LoRA(Hu et al. 2021) effectively\\ncaptures the characteristics of downstream tasks by intro-\\nducing low-rank adapters. The core idea is to decompose\\nthe bypass weight matrix ∆W ∈ Rdin×dout\\ninto two low-\\nrank matrices {A ∈ Rdin×r, B ∈ Rr×dout\\n}, where r ≪\\nmin{din, dout}, significantly reducing learnable parameters.\\nThe output with the LoRA adapter for the input x is then\\ngiven by:\\nh = xW0 + αx∆W/r = xW0 + αxAB/r, (3)\\nwhere matrix A is initialized with a Gaussian distribution,\\nwhile the matrixB is initialized as a zero matrix. The scaling\\nfactor α/r controls the impact of ∆W on the model.\\n4 HealthGPT\\n4.1 Unified Autoregressive Generation.\\nHealthGPT (Figure 3) utilizes a discrete token representa-\\ntion that covers both text and visual outputs, unifying visual\\ncomprehension and generation as an autoregressive task. For\\ncomprehension, Mllm receives the input joint sequence U\\nand outputs a series of text token R = [ r1, r2, . . . , rNr ],\\nwhere ri ∈ Vtxt, and Vtxt represents the LLM’s vocabulary:\\nPθ(R | U) =\\nNrY\\ni=1\\nPθ(ri | U, r<i). (4)\\nFor generation, Mllm first receives a special start token\\n⟨START IMG⟩, then generates a series of tokens corre-\\nsponding to the VQGAN indices I = [ i1, i2, . . . , iNi ],\\nwhere ij ∈ Vvq, and Vvq represents the index range of VQ-\\nGAN. Upon completion of generation, the LLM outputs an\\nend token ⟨END IMG⟩:\\nPθ(I | U) =\\nNiY\\nj=1\\nPθ(ij | U, i<j). (5)\\nFinally, the generated index sequence I is fed into the de-\\ncoder G, which reconstructs the target image ˆximg = G(I).\\n4.2 Hierarchical Visual Perception\\nGiven the differences in visual perception between compre-\\nhension and generation tasks—where the former focuses on\\nabstract semantics and the latter emphasizes complete se-\\nmantics—we employ ViT to compress the image into dis-\\ncrete visual tokens at multiple hierarchical levels. Specif-\\nically, the image is converted into a series of features\\n{f1, f2, . . . , fL} as it passes through L ViT blocks.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b8f18ed1-d6d6-45a9-b396-528ddbe7acea', embedding=None, metadata={'page_label': '5', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='To address the needs of various tasks, the hidden states\\nare divided into two types: (i) Concrete-grained features\\nFCon = {f1, f2, . . . , fk}, k < L, derived from the shal-\\nlower layers of ViT, containing sufficient global features,\\nsuitable for generation tasks; (ii) Abstract-grained features\\nFAbs = {fk+1, fk+2, . . . , fL}, derived from the deeper\\nlayers of ViT, which contain abstract semantic information\\ncloser to the text space, suitable for comprehension tasks.\\nThe task type T (comprehension or generation) deter-\\nmines which set of features is selected as the input for the\\ndownstream large language model:\\nFimg\\nT =\\n(\\nFCon, if T = generation task\\nFAbs, if T = comprehension task (6)\\nWe integrate the image featuresFimg\\nT and text featuresT into\\na joint sequence through simple concatenation, which is then\\nfed into the LLM Mllm for autoregressive generation.\\n4.3 Heterogeneous Knowledge Adaptation\\nWe devise H-LoRA, which stores heterogeneous knowledge\\nfrom comprehension and generation tasks in separate mod-\\nules and dynamically routes to extract task-relevant knowl-\\nedge from these modules. At the task level, for each task type\\nT, we dynamically assign a dedicated H-LoRA submodule\\nθT , which is expressed as:\\nR = MLLM(U|θ, θT ), θ T = {AT , BT , RT\\nouter}. (7)\\nAt the feature level for a single task, H-LoRA integrates the\\nidea of Mixture of Experts (MoE) (Masoudnia and Ebrahim-\\npour 2014) and designs an efficient matrix merging and rout-\\ning weight allocation mechanism, thus avoiding the signif-\\nicant computational delay introduced by matrix splitting in\\nexisting MoELoRA (Luo et al. 2024a). Specifically, we first\\nmerge the low-rank matrices (rank = r) of k LoRA experts\\ninto a unified matrix:\\nAmerged, Bmerged = Concat({Ai}k\\n1 ), Concat({Bi}k\\n1 ), (8)\\nwhere Amerged ∈ Rdin×rk and Bmerged ∈ Rrk×dout\\n. The\\nk-dimension routing layer generates expert weights W ∈\\nRtoken num×k based on the input hidden state x, and these are\\nexpanded to Rtoken num×rk as follows:\\nWexpanded = αkW/r ⊗ 1r, (9)\\nwhere ⊗ denotes the replication operation. The overall out-\\nput of H-LoRA is computed as:\\nOH-LoRA = (xAmerged ⊙ Wexpanded)Bmerged, (10)\\nwhere ⊙ represents element-wise multiplication. Finally, the\\noutput of H-LoRA is added to the frozen pre-trained weights\\nto produce the final output:\\nO = xW0 + OH-LoRA. (11)\\n900\\n800\\n700\\n600\\n500\\n400\\n300\\n200\\n100\\n0\\nComp. Gen.\\n(a) (b)\\n783K765K\\n（K）\\nFigure 4: Data statistics of VL-Health.\\n4.4 Training Pipeline\\n1st Stage: Multi-modal Alignment. In the first stage, we\\ndesign separate visual adapters and H-LoRA submodules for\\nmedical unified tasks. For the medical comprehension task,\\nwe train abstract-grained visual adapters using high-quality\\nimage-text pairs to align visual embeddings with textual\\nembeddings, thereby enabling the model to accurately de-\\nscribe medical visual content. During this process, the pre-\\ntrained LLM and its corresponding H-LoRA submodules\\nremain frozen. In contrast, the medical generation task re-\\nquires training concrete-grained adapters and H-LoRA sub-\\nmodules while keeping the LLM frozen. Meanwhile, we ex-\\ntend the textual vocabulary to include multimodal tokens,\\nenabling the support of additional VQGAN vector quanti-\\nzation indices. The model trains on image-VQ pairs, en-\\ndowing the pre-trained LLM with the capability for image\\nreconstruction. This design ensures pixel-level consistency\\nof pre- and post-LVLM. The processes establish the initial\\nalignment between the LLM’s outputs and the visual inputs.\\n2nd Stage: Heterogeneous H-LoRA Plugin Adaptation.\\nThe submodules of H-LoRA share the word embedding\\nlayer and output head but may encounter issues such as\\nbias and scale inconsistencies during training across dif-\\nferent tasks. To ensure that the multiple H-LoRA plugins\\nseamlessly interface with the LLMs and form a unified base,\\nwe fine-tune the word embedding layer and output head us-\\ning a small amount of mixed data to maintain consistency\\nin the model weights. Specifically, during this stage, all H-\\nLoRA submodules for different tasks are kept frozen, with\\nonly the word embedding layer and output head being op-\\ntimized. Through this stage, the model accumulates foun-\\ndational knowledge for unified tasks by adapting H-LoRA\\nplugins.\\n3rd Stage: Visual Instruction Fine-Tuning. In the third\\nstage, we introduce additional task-specific data to fur-\\nther optimize the model and enhance its adaptability to\\ndownstream tasks such as medical visual comprehension\\n(e.g., medical QA, medical dialogues, and report generation)\\nor generation tasks (e.g., super-resolution, denoising, and\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6d5a0570-973b-42f7-806f-dd3a7431788c', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Comparison of HealthGPT with other LVLMs and unified multi-modal models on medical visual comprehension\\ntasks. Bold and underlined text indicates the best performance and second-best performance, respectively.\\nVQA-RAD↑ SLAKE↑ PathVQA↑Type Model # Params Medical\\nLVLM close all close all close all\\nMMMU\\n-Med ↑ OMVQA↑ Avg.↑\\nComp. Only\\nMed-Flamingo 8.3B ✓ 58.6 43.0 47.0 25.5 61.9 31.3 28.7 34.9 41.4\\nLLaV A-Med 7B ✓ 60.2 48.1 58.4 44.8 62.3 35.7 30.0 41.3 47.6\\nHuatuoGPT-Vision 7B ✓ 66.9 53.0 59.8 49.1 52.9 32.0 42.0 50.0 50.7\\nBLIP-2 6.7B ✗ 43.4 36.8 41.6 35.3 48.5 28.8 27.3 26.9 36.1\\nLLaV A-v1.5 7B ✗ 51.8 42.8 37.1 37.7 53.5 31.4 32.7 44.7 41.5\\nInstructBLIP 7B ✗ 61.0 44.8 66.8 43.3 56.0 32.3 25.3 29.0 44.8\\nYi-VL 6B ✗ 52.6 42.1 52.4 38.4 54.9 30.9 38.0 50.2 44.9\\nInternVL2 8B ✗ 64.9 49.0 66.6 50.1 60.0 31.9 43.3 54.5 52.5\\nLlama-3.2 11B ✗ 68.9 45.5 72.4 52.1 62.8 33.6 39.3 63.2 54.7\\nComp. & Gen.\\nShow-o 1.3B ✗ 50.6 33.9 31.5 17.9 52.9 28.2 22.7 45.7 42.6\\nUnified-IO 2 7B ✗ 46.2 32.6 35.9 21.9 52.5 27.0 25.3 33.0 33.8\\nJanus 1.3B ✗ 70.9 52.8 34.7 26.9 51.9 27.9 30.0 26.8 33.5\\nHealthGPT-M3 3.8B ✓ 73.7 55.9 74.6 56.4 78.7 39.7 43.3 68.5 61.3\\nHealthGPT-L14 14B ✓ 77.7 58.3 76.4 64.5 85.9 44.4 49.2 74.4 66.4\\nTable 2: The experimental results for the four modality conversion tasks.\\nCT to MRI (Brain) CT to MRI (Pelvis) MRI to CT (Brain) MRI to CT (Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\npix2pix 71.09 32.65 36.85 59.17 31.02 51.91 78.79 33.85 28.33 72.31 32.98 36.19\\nCycleGAN 54.76 32.23 40.56 54.54 30.77 55.00 63.75 31.02 52.78 50.54 29.89 67.78\\nBBDM 71.69 32.91 34.44 57.37 31.37 48.06 86.40 34.12 26.61 79.26 33.15 33.60\\nVmanba 69.54 32.67 36.42 63.01 31.47 46.99 79.63 34.12 26.49 77.45 33.53 31.85\\nDiffMa 71.47 32.74 35.77 62.56 31.43 47.38 79.00 34.13 26.45 78.53 33.68 30.51\\nHealthGPT-M3 79.38 33.03 33.48 71.81 31.83 43.45 85.06 34.40 25.49 84.23 34.29 27.99\\nHealthGPT-L14 79.73 33.10 32.96 71.92 31.87 43.09 85.31 34.29 26.20 84.96 34.14 28.13\\nmodality conversion). Notably, by this stage, the word em-\\nbedding layer and output head have been fine-tuned, only the\\nH-LoRA modules and adapter modules need to be trained.\\nThis strategy significantly improves the model’s adaptability\\nand flexibility across different tasks.\\n5 Experiments\\n5.1 Data and Experimental Setup\\nData Details. We curate VL-Health dataset (see Fig-\\nure 4). For medical visual comprehension, we leverage\\nmultiple medical-specific datasets, including PubMedVi-\\nsion (Chen et al. 2024a), LLaV A-Med (Li et al. 2024b),\\nPathVQA (He et al. 2020), MIMIC-CXR-VQA (Bae et al.\\n2024), SLAKE (Liu et al. 2021), and VQA-RAD (Lau\\net al. 2018). Additionally, we incorporate high-quality open-\\nworld data from LLaV A-1.5 (Liu et al. 2024b) to preserve\\nthe model’s general knowledge and instruction-following\\ncapabilities. For generation tasks, we construct a recon-\\nstruction dataset based on LLaV A-558k (Liu et al. 2024b),\\nand also explore two key tasks in personalized medical\\nimage enhancement—super-resolution and modality con-\\nversion—using the IXI (Davies et al. 2014) and Syn-\\nthRAD2023 (Thummerer et al. 2023) datasets. Detailed data\\nselection and instruction templates are in the Appendix.\\nModel Details. We select CLIP-L/14 (Radford et al. 2021)\\nas the visual encoder and used the hidden states of its\\nsecond and penultimate layers as concrete-grained and\\nabstract-grained features for model’s dynamic hierarchical\\nvisual perception. Drawing on the successful experiences of\\nLLaV A, we employ a MLP to align the multi-modal fea-\\nture embeddings. We choose the parameter-efficient phi-3-\\nmini (Abdin et al. 2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al. 2023), LLaV A-Med (Li et al.\\n2024b), HuatuoGPT-Vision (Chen et al. 2024a)) as well\\nas recent open-world LVLMs (e.g., BLIP-2 (Li et al.\\n2023b), LLaV A-v1.5 (Liu et al. 2024b), InstructBLIP (Dai\\net al. 2023), Yi-VL (Young et al. 2024), InternVL2 (Chen\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='de4841da-cca7-4622-af87-3588becbeae1', embedding=None, metadata={'page_label': '7', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 3: Comparison results of super-resolution task.\\nModel SSIM↑ PSNR↑ MSE↓ LPIPS↓\\nSRGAN 71.34 32.01 41.27 24.50\\nDASR 71.57 32.34 38.25 19.17\\nReal-ESRGAN 67.30 31.87 42.57 20.64\\nLIIF 73.27 32.13 40.14 22.93\\nBSRGAN 69.97 31.97 41.52 28.72\\nHealthGPT-M3 78.19 32.76 34.47 12.02\\nHealthGPT-L14 77.94 32.71 35.19 12.43\\nFigure 5: Performance comparison of LoRA, MoELoRA,\\nand H-LoRA under different rank settings.\\net al. 2024b), Llama-3.2 (Dubey et al. 2024)). Addition-\\nally, we test several SOTA unified visual comprehension\\nand generation models, including Show-o (Xie et al. 2024),\\nUnified-IO 2 (Lu et al. 2024), and Janus (Wu et al.\\n2024). The experimental results are shown in Table 1, with\\nthe following key observations: (i) SOTA Results Com-\\npared with LVLMs: In medical visual comprehension\\ntasks, HealthGPT demonstrates superior performance,\\nsignificantly outperforming both medical-specific models\\n(e.g., HuatuoGPT-Vision) and general-purpose models (e.g.,\\nLlama-3.2). (ii) Surpassing Current Unified LVLMs: De-\\nspite being trained on billions of data points, unified mod-\\nels still exhibit poor generalization performance in medi-\\ncal visual comprehension. For instance, Unified-IO 2 scored\\nonly 33.8. In contrast,HealthGPT-M3, with only 3.8B pa-\\nrameters, scored 61.3 on the medical multi-modal unified\\ntask, significantly outperforming existing unified models in\\nmedical downstream scenarios. (iii) Stable Improvement\\nwith Large Base Model: Our method demonstrates excel-\\nlent scalability, with HealthGPT-L14 achieving a score\\nof 66.4 in the larger model configuration. This result signif-\\nicantly outperforms all other models, highlighting the effec-\\ntiveness of scaling up the base model for enhanced perfor-\\nmance in medical tasks.\\nGeneration. We study three key tasks in medical imag-\\ning. (i) Modality Conversion: In this task, we focus on\\nthe conversion between CT and MRI modalities for the\\nbrain and pelvic regions, designing four specific sub-tasks.\\nAll comparative models (Pix2Pix (Isola et al. 2017), Cy-\\ncleGAN (Zhu et al. 2017), BBDM (Li et al. 2023a),\\n(%)\\n(%)\\n（a） （b）\\nFigure 6: The loss visualization (a) and performance com-\\nparison (b) with respect to different visual perceptions.\\nVmamba (Liu et al. 2024e), and DiffMa (Wang et al.\\n2024b)) trained a separate model for each sub-task, while\\nHealthGPT unify all tasks into a single training process.\\nThe experimental results, shown in Table 11, demonstrate\\nthat our approach outperforms other methods across multi-\\nple evaluation metrics. For instance, in the CT2MRI-Brain\\ntask, HealthGPT-M3 achieves an SSIM of 79.38, signif-\\nicantly surpassing traditional methods like Pix2Pix (71.09)\\nand the recent DiffMa (71.47). (ii) Super-Resolution: We\\nconduct 4× super-resolution experiments on the IXI dataset,\\nwith the results presented in Table 3. Notably, most exist-\\ning methods fail to fully leverage the prior knowledge of key\\nstructures in medical images, resulting in significant short-\\ncomings in detail recovery. In contrast, our method signif-\\nicantly mitigates this issue. Specifically, HealthGPT-M3\\nexcels in key metrics such as SSIM, PSNR, and ISE, achiev-\\ning scores of 78.19, 32.76, and 34.47, respectively. Ad-\\nditionally, HealthGPT-M3 achieves the lowest score of\\n12.34, further validating its exceptional performance in hu-\\nman visual perception. (iii) Reconstruction: We compare\\nHealthGPT-M3 with unified models with reconstruction\\ncapabilities, such as Unified-IO 2 and SEED-X. The results\\nshow that our approach performs better controllability for vi-\\nsual reconstruction. We also trainHealthGPT-L14 with a\\nsimilar number of trainable parameters to the M3 version.\\nHence, the similar performance between the two models\\nmeets our expectations. Details are in the Appendix.\\n5.3 In-Depth Study\\nEffect of Heterogeneous Low-Rank Adaptation.H-LoRA\\nprovides an optimized multi-LoRA architecture for multi-\\ntask learning. We conduct extensive validation of this struc-\\nture, with results presented in Table 4, comparing the per-\\nformance of LoRA, MoELoRA, and H-LoRA in medical\\nunified comprehension and generation tasks. In the majority\\nof comprehension tasks and all generation tasks, H-LoRA\\ndemonstrates superior performance, particularly in the Om-\\nniMedVQA benchmark, where it improved from 64.90 to\\n68.50. Notably, despite some applications of MoELoRA in\\ncertain scenarios, it do not show advantages in this task and\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f3f31dff-a830-4267-9c9a-cf4c96410395', embedding=None, metadata={'page_label': '8', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 4: We present the performance and speed differences of LoRA, MoELoRA (n=4), and H-LoRA (n=4) on medical visual\\ncomprehension and generation tasks.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQAModel\\nclose all close all close all\\nMMMU\\n-Med OMVQA RECOM MTRANS SR\\nTraining\\nTime\\nHealthGPT w/\\n+LoRA 71.3 57.2 70.0 53.4 76.4 38.6 41.30 65.10 62.67 59.99 65.88 1.00×\\n+MoELoRA 72.5 57.2 66.4 52.4 73.2 36.0 39.30 64.90 67.31 59.76 65.91 1.49×\\n+H-LoRA 73.7 55.9 74.6 56.4 78.7 39.7 43.30 68.50 67.69 60.30 66.14 1.00×\\nTable 5: Comparison between the H-LoRA-based Three-Stage Learning Strategy and the mixed-training approach.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQA CT MRITraining Strategy\\nclose all close all close all\\nMMMU\\n-Med OMVQA Brain Pelvis Brain Pelvis\\nMixed-Training 56.6 37.9 45.0 32.9 65.7 33.6 44.0 48.9 65.64 62.75 56.61 50.77HealthGPT w/ 3-stage-Training 72.5 55.2 77.9 59.6 79.7 49.0 42.7 68.5 70.84 72.99 65.26 61.33\\nFigure 7: Case study of report-to-CXR under different instructions. (a) shows a normal CXR image for comparison. (b) and (c)\\nillustrate generated cases with varying severity and affected regions. The graffiti areas indicate abnormal conditions.\\nhad a training time approximately 50% longer than LoRA.\\nFigure 5 illustrates the performance of the three PEFT meth-\\nods in medical visual comprehension and generation tasks\\nacross different ranks, with H-LoRA consistently outper-\\nforming the other methods in all scenarios, demonstrating\\nsignificant advantages in handling diverse tasks.\\nDifferent Learning Strategy. We propose a three-stage\\nlearning strategy for H-LoRA that decouples comprehension\\nand generation tasks. Unlike methods that train both tasks\\nsimultaneously, our approach reduces performance degra-\\ndation from task conflicts (see Table 5). In the medical vi-\\nsual comprehension task, mixed training causes catastrophic\\nforgetting and degrades visual reconstruction, whereas our\\nstrategy effectively uses the medical embedding knowledge\\nin pre-trained LLMs to mitigate these conflicts. Meanwhile,\\nwe examine how fusing heterogeneous H-LoRA plugins in\\nthe second training stage results in minimal performance\\ndegradation. Detailed results are in the Appendix.\\nHierarchical Visual Perception Analysis. We conduct an\\nablation analysis on visual perceptual inputs for comprehen-\\nsion and generation tasks. Figure 6 shows that comprehen-\\nsion tasks converge more efficiently with abstract-grained\\ninputs, while generation tasks perform better with concrete-\\ngrained inputs. This highlights the importance of the hier-\\narchical visual perception we propose, suggesting that tai-\\nloring visual inputs for specific tasks at different hierarchies\\ncan significantly improve efficiency.\\nReport-to-CXR Task. We further explore the medical im-\\nage generation task without reference images, using a small\\namount of MIMIC-CXR data (Johnson et al. 2019) for in-\\nstruction fine-tuning. Figure 7 annotates images with vary-\\ning injury degrees and locations, comparing them to healthy\\nCXR images. We observe thatHealthGPT effectively gen-\\nerates CXR images based on the instructions, showcasing its\\npotential in healthcare education and auxiliary diagnosis.\\n6 Conclusion\\nIn this paper, we introduceHealthGPT, a Med-LVLM that\\nunifies medical vision-language comprehension and gen-\\neration through a novel heterogeneous knowledge adap-\\ntation approach. Experimental results demonstrate that\\nHealthGPT achieves significant performance improve-\\nments across multiple medical comprehension and genera-\\ntion tasks, showcasing its potential for healthcare applica-\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d62a7ebd-46e5-4e42-b68a-23024ada525f', embedding=None, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tions.\\nReferences\\nAbdin, M.; Aneja, J.; Behl, H.; Bubeck, S.; Eldan, R.; Gu-\\nnasekar, S.; Harrison, M.; Hewett, R. J.; Javaheripi, M.;\\nKauffmann, P.; et al. 2024. Phi-4 technical report. arXiv\\npreprint arXiv:2412.08905.\\nBae, S.; Kyung, D.; Ryu, J.; Cho, E.; Lee, G.; Kweon, S.;\\nOh, J.; JI, L.; Chang, E.; Kim, T.; et al. 2024. MIMIC-Ext-\\nMIMIC-CXR-VQA: A Complex, Diverse, And Large-Scale\\nVisual Question Answering Dataset for Chest X-ray Images.\\nChen, J.; Gui, C.; Ouyang, R.; Gao, A.; Chen, S.; Chen,\\nG. H.; Wang, X.; Zhang, R.; Cai, Z.; Ji, K.; et al.\\n2024a. Huatuogpt-vision, towards injecting medical visual\\nknowledge into multimodal llms at scale. arXiv preprint\\narXiv:2406.19280.\\nChen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong,\\nW.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024b. How far are we to\\ngpt-4v? closing the gap to commercial multimodal models\\nwith open-source suites. arXiv preprint arXiv:2404.16821.\\nChern, E.; Su, J.; Ma, Y .; and Liu, P. 2024. Anole:\\nAn open, autoregressive, native large multimodal mod-\\nels for interleaved image-text generation. arXiv preprint\\narXiv:2407.06135.\\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\\nGeneral-purpose Vision-Language Models with Instruction\\nTuning. arXiv:2305.06500.\\nDavies, R. L.; Royston, P. A.; Leung, M. S.; Haider, M. E.\\nA. M. J.; Barkhof, S. G. A. L.; and B., P. E. T. M. 2014. The\\nIXI Dataset. Accessed: 2025-01-30.\\nDing, N.; Qin, Y .; Yang, G.; Wei, F.; Yang, Z.; Su, Y .;\\nHu, S.; Chen, Y .; Chan, C.-M.; Chen, W.; et al. 2023.\\nParameter-efficient fine-tuning of large-scale pre-trained\\nlanguage models. Nature Machine Intelligence, 5(3): 220–\\n235.\\nDong, R.; Han, C.; Peng, Y .; Qi, Z.; Ge, Z.; Yang, J.; Zhao,\\nL.; Sun, J.; Zhou, H.; Wei, H.; et al. 2023. Dreamllm:\\nSynergistic multimodal comprehension and creation. arXiv\\npreprint arXiv:2309.11499.\\nDubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.;\\nLetman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;\\net al. 2024. The llama 3 herd of models. arXiv preprint\\narXiv:2407.21783.\\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming trans-\\nformers for high-resolution image synthesis. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern\\nrecognition, 12873–12883.\\nGe, Y .; Ge, Y .; Zeng, Z.; Wang, X.; and Shan, Y . 2023. Plant-\\ning a seed of vision in large language model. arXiv preprint\\narXiv:2307.08041.\\nGe, Y .; Zhao, S.; Zhu, J.; Ge, Y .; Yi, K.; Song, L.; Li, C.;\\nDing, X.; and Shan, Y . 2024. Seed-x: Multimodal models\\nwith unified multi-granularity comprehension and genera-\\ntion. arXiv preprint arXiv:2404.14396.\\nHe, X.; Zhang, Y .; Mou, L.; Xing, E.; and Xie, P. 2020.\\nPathvqa: 30000+ questions for medical visual question an-\\nswering. arXiv preprint arXiv:2003.10286.\\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\\nof large language models. arXiv preprint arXiv:2106.09685.\\nHu, Y .; Li, T.; Lu, Q.; Shao, W.; He, J.; Qiao, Y .; and Luo,\\nP. 2024. Omnimedvqa: A new large-scale comprehensive\\nevaluation benchmark for medical lvlm. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 22170–22183.\\nIsola, P.; Zhu, J.-Y .; Zhou, T.; and Efros, A. A. 2017. Image-\\nto-image translation with conditional adversarial networks.\\nIn Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 1125–1134.\\nJohnson, A. E.; Pollard, T. J.; Greenbaum, N. R.; Lungren,\\nM. P.; Deng, C.-y.; Peng, Y .; Lu, Z.; Mark, R. G.; Berkowitz,\\nS. J.; and Horng, S. 2019. MIMIC-CXR-JPG, a large pub-\\nlicly available database of labeled chest radiographs. arXiv\\npreprint arXiv:1901.07042.\\nLau, J. J.; Gayen, S.; Ben Abacha, A.; and Demner-\\nFushman, D. 2018. A dataset of clinically generated visual\\nquestions and answers about radiology images. Scientific\\ndata, 5(1): 1–10.\\nLi, B.; Xue, K.; Liu, B.; and Lai, Y .-K. 2023a. Bbdm: Image-\\nto-image translation with brownian bridge diffusion models.\\nIn Proceedings of the IEEE/CVF conference on computer\\nvision and pattern Recognition, 1952–1961.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024a. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024b. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. Blip-2:\\nBootstrapping language-image pre-training with frozen im-\\nage encoders and large language models. In International\\nconference on machine learning, 19730–19742. PMLR.\\nLin, T.; Liu, J.; Zhang, W.; Li, Z.; Dai, Y .; Li, H.; Yu, Z.; He,\\nW.; Li, J.; Jiang, H.; et al. 2024. Teamlora: Boosting low-\\nrank adaptation with expert collaboration and competition.\\narXiv preprint arXiv:2408.09856.\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f8441ba4-0487-4eb3-a2e0-19131d9ace1b', embedding=None, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Liu, B.; Zhan, L.-M.; Xu, L.; Ma, L.; Yang, Y .; and Wu,\\nX.-M. 2021. Slake: A semantically-labeled knowledge-\\nenhanced dataset for medical visual question answering. In\\n2021 IEEE 18th International Symposium on Biomedical\\nImaging (ISBI), 1650–1654. IEEE.\\nLiu, D.; Zhao, S.; Zhuo, L.; Lin, W.; Qiao, Y .; Li, H.; and\\nGao, P. 2024a. Lumina-mgpt: Illuminate flexible photore-\\nalistic text-to-image generation with multimodal generative\\npretraining. arXiv preprint arXiv:2408.02657.\\nLiu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2024b. Improved\\nbaselines with visual instruction tuning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 26296–26306.\\nLiu, H.; Li, C.; Li, Y .; Li, B.; Zhang, Y .; Shen, S.; and Lee,\\nY . J. 2024c. LLaV A-NeXT: Improved reasoning, OCR, and\\nworld knowledge. https://llava-vl.github.io/blog/2024-01-\\n30-llava-next/.\\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruc-\\ntion Tuning. In NeurIPS.\\nLiu, Q.; Wu, X.; Zhao, X.; Zhu, Y .; Xu, D.; Tian, F.; and\\nZheng, Y . 2024d. When moe meets llms: Parameter efficient\\nfine-tuning for multi-task medical applications. In Proceed-\\nings of the 47th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval, 1104–\\n1114.\\nLiu, Y .; Tian, Y .; Zhao, Y .; Yu, H.; Xie, L.; Wang, Y .; Ye,\\nQ.; and Liu, Y . 2024e. VMamba: Visual State Space Model.\\narXiv preprint arXiv:2401.10166.\\nLu, J.; Clark, C.; Lee, S.; Zhang, Z.; Khosla, S.; Marten, R.;\\nHoiem, D.; and Kembhavi, A. 2024. Unified-IO 2: Scaling\\nAutoregressive Multimodal Models with Vision Language\\nAudio and Action. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , 26439–\\n26455.\\nLu, J.; Clark, C.; Zellers, R.; Mottaghi, R.; and Kembhavi,\\nA. 2022. Unified-io: A unified model for vision, language,\\nand multi-modal tasks. In The Eleventh International Con-\\nference on Learning Representations.\\nLuo, T.; Lei, J.; Lei, F.; Liu, W.; He, S.; Zhao, J.; and Liu,\\nK. 2024a. Moelora: Contrastive learning guided mixture of\\nexperts on parameter-efficient fine-tuning for large language\\nmodels. arXiv preprint arXiv:2402.12851.\\nLuo, Y .; Zhang, J.; Fan, S.; Yang, K.; Hong, M.; Wu, Y .;\\nQiao, M.; and Nie, Z. 2024b. Biomedgpt: An open multi-\\nmodal large language model for biomedicine. IEEE Journal\\nof Biomedical and Health Informatics.\\nMasoudnia, S.; and Ebrahimpour, R. 2014. Mixture of ex-\\nperts: a literature survey. Artificial Intelligence Review, 42:\\n275–293.\\nMoor, M.; Huang, Q.; Wu, S.; Yasunaga, M.; Dalmia, Y .;\\nLeskovec, J.; Zakka, C.; Reis, E. P.; and Rajpurkar, P. 2023.\\nMed-flamingo: a multimodal medical few-shot learner. In\\nMachine Learning for Health (ML4H), 353–367. PMLR.\\nNath, V .; Li, W.; Yang, D.; Myronenko, A.; Zheng, M.; Lu,\\nY .; Liu, Z.; Yin, H.; Law, Y . M.; Tang, Y .; et al. 2024. Vila-\\nm3: Enhancing vision-language models with medical expert\\nknowledge. arXiv preprint arXiv:2411.12915.\\nOpenAI. 2023. GPT-4V(ision) System Card. https://cdn.\\nopenai.com/papers/GPTV System Card.pdf.\\nPan, K.; Tang, S.; Li, J.; Fan, Z.; Chow, W.; Yan, S.;\\nChua, T.-S.; Zhuang, Y .; and Zhang, H. 2024. Auto-\\nEncoding Morph-Tokens for Multimodal LLM. arXiv\\npreprint arXiv:2405.01926.\\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\\net al. 2021. Learning transferable visual models from nat-\\nural language supervision. In International conference on\\nmachine learning, 8748–8763. PMLR.\\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\\net al. 2023. Large language models encode clinical knowl-\\nedge. Nature, 620(7972): 172–180.\\nTeam, C. 2024. Chameleon: Mixed-modal early-fusion\\nfoundation models. arXiv preprint arXiv:2405.09818.\\nThawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal,\\nH.; Anwer, R. M.; Khan, S.; Laaksonen, J.; and Khan,\\nF. S. 2023. Xraygpt: Chest radiographs summarization\\nusing medical vision-language models. arXiv preprint\\narXiv:2306.07971.\\nThummerer, A.; van der Bijl, E.; Galapon Jr, A.; Verhoeff,\\nJ. J.; Langendijk, J. A.; Both, S.; van den Berg, C. N. A.;\\nand Maspero, M. 2023. SynthRAD2023 Grand Challenge\\ndataset: Generating synthetic CT for radiotherapy. Medical\\nphysics, 50(7): 4664–4674.\\nTian, D.; Jiang, S.; Zhang, L.; Lu, X.; and Xu, Y . 2023. The\\nrole of large language models in medical image processing:\\na narrative review. Quantitative Imaging in Medicine and\\nSurgery, 14(1): 1108.\\nTong, S.; Fan, D.; Zhu, J.; Xiong, Y .; Chen, X.; Sinha, K.;\\nRabbat, M.; LeCun, Y .; Xie, S.; and Liu, Z. 2024. Meta-\\nMorph: Multimodal Understanding and Generation via In-\\nstruction Tuning. arXiv preprint arXiv:2412.14164.\\nTu, T.; Azizi, S.; Driess, D.; Schaekermann, M.; Amin, M.;\\nChang, P.-C.; Carroll, A.; Lau, C.; Tanno, R.; Ktena, I.; et al.\\n2024. Towards generalist biomedical AI. NEJM AI, 1(3):\\nAIoa2300138.\\nVig, J. 2019. A multiscale visualization of attention in the\\ntransformer model. arXiv preprint arXiv:1906.05714.\\nWang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y .; Wang, J.;\\nZhang, F.; Wang, Y .; Li, Z.; Yu, Q.; et al. 2024a. Emu3:\\nNext-token prediction is all you need. arXiv preprint\\narXiv:2409.18869.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='eafae00c-fd06-4349-839e-280d6fccf61c', embedding=None, metadata={'page_label': '11', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Wang, Z.; Wu, Z.; Agarwal, D.; and Sun, J. 2022. Medclip:\\nContrastive learning from unpaired medical images and text.\\narXiv preprint arXiv:2210.10163.\\nWang, Z.; Zhang, L.; Wang, L.; and Zhang, Z. 2024b. Soft\\nMasked Mamba Diffusion Model for CT to MRI Conver-\\nsion. arXiv preprint arXiv:2406.15910.\\nWu, C.; Chen, X.; Wu, Z.; Ma, Y .; Liu, X.; Pan, Z.; Liu, W.;\\nXie, Z.; Yu, X.; Ruan, C.; and Luo, P. 2024. Janus: Decou-\\npling Visual Encoding for Unified Multimodal Understand-\\ning and Generation. arXiv:2410.13848.\\nWu, S.; Fei, H.; Qu, L.; Ji, W.; and Chua, T.-S. 2023.\\nNext-gpt: Any-to-any multimodal llm. arXiv preprint\\narXiv:2309.05519.\\nXie, J.; Mao, W.; Bai, Z.; Zhang, D. J.; Wang, W.; Lin, K. Q.;\\nGu, Y .; Chen, Z.; Yang, Z.; and Shou, M. Z. 2024. Show-o:\\nOne single transformer to unify multimodal understanding\\nand generation. arXiv preprint arXiv:2408.12528.\\nYoung, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.; Zhang,\\nG.; Li, H.; Zhu, J.; Chen, J.; Chang, J.; et al. 2024.\\nYi: Open foundation models by 01. ai. arXiv preprint\\narXiv:2403.04652.\\nZhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li,\\nY .; Chen, S. S.; Zhou, P.; Liu, J.; et al. 2023. A survey of\\nlarge language models in medicine: Progress, application,\\nand challenge. arXiv preprint arXiv:2311.05112.\\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\\npaired image-to-image translation using cycle-consistent ad-\\nversarial networks. InProceedings of the IEEE international\\nconference on computer vision, 2223–2232.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a271bd41-2149-4376-808e-f5dc9e5c8ed4', embedding=None, metadata={'page_label': '12', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix\\nThis is the Appendix for “HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation\\nvia Heterogeneous Knowledge Adaptation”. This Appendix is organized as follows:\\n• Section A presents the experimental implementation details, the training process of HealthGPT, and the specifics of\\nVL-Health.\\n• Section B systematically provides an analysis of Heterogeneous Low-Rank Adaptation.\\n• Section C shows supplementary experimental results to validate the effectiveness ofHealthGPT.\\nA Implementation Details\\nA.1 Model Details\\nWe employ CLIP-L/14 (Radford et al. 2021) as the visual feature extractor, extracting both shallow and deep features to serve as\\nvisual tokens. The model uses alignment adapters, implemented with two-layer MLPs, to align shallow features, representing\\nconcrete visual granularity, and deep features, representing abstract visual granularity. These visual tokens are concatenated\\nwith text tokens and input into the large language models (LLMs).\\nHealthGPT offers two versions: HealthGPT-M3 and HealthGPT-L14, which are based on Phi-3-mini (Abdin et al.\\n2024) and Phi-4 (Abdin et al. 2024) as the pre-trained LLMs, respectively. In addition, we expand the LLM vocabulary with\\n8192 VQ indices derived from VQGAN-f8-8192 (Esser, Rombach, and Ommer 2021), serving as multi-modal tokens to further\\naugment the model’s capacity for understanding both visual and textual input. Figure 6 shows the details.\\nTable 6: Overview of the Components of HealthGPT.\\nModel ViT Adapter MLP-dims Model dims LLM Params Vocab Size H-LoRA Rank\\nHealthGPT-M3 CLIP-L/14 2-layer MLP 1024 3072 Phi-3-mini 3.8B 40206 16(Comp.), 64(Gen.)\\nHealthGPT-L14 CLIP-L/14 2-layer MLP 1024 5120 Phi-4 14B 108547 8(Comp.), 32(Gen.)\\nA.2 Training Details\\nIn this study, we propose a three-stage learning strategy that is compatible with our innovative heterogeneous low-rank adapta-\\ntion (H-LoRA). We provide a detailed hyperparameter configuration for the model’s three-stage training process. The specific\\nhyperparameter settings used are listed in Table 7. These hyperparameters are crucial for ensuring the model’s learning efficacy\\nand final performance.\\nTable 7: Overview of Hyperparameter Configurations.\\nHealthGPT-M3 HealthGPT-L14\\nStage-1 Stage-2 Stage-3 Stage-1 Stage-2 Stage-3Hyperparameter\\nComp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen.\\nOptimizer AdamW AdamW AdamW AdamW AdamW AdamW\\nAdapter LR 1e-3 2e-5 2e-5 2e-5 1e-3 2e-5 2e-5 2e-5\\nLearning Rate / 2e-4 2e-4 2e-4 / 1e-4 2e-4 2e-4\\nGlobal Batch Size 256 64 32 128 64 256 64 32 128 64\\nWeight Decay 0 0 0 0 0 0\\nDropout Rate 0 0.05 0.05 0.05 0 0.05 0.05 0.05\\nLR Scheduler Warm Up Constant Warm Up Warm Up Constant Warm Up\\nMax Sequence Length 2048 2048 2048 2048 2048 2048\\nIt is worth noting that we sometimes observe instances of loss spikes during the training of medical visual comprehension\\nand generation tasks. Through repeated validation, we discovered that larger model parameters and learning rates tend to lead to\\nthis issue, which is the reason for the slight differences in hyperparameters betweenHealthGPT-M3 and HealthGPT-L14.\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='ba538698-7cc4-4ecb-afcb-6813a2391de6', embedding=None, metadata={'page_label': '13', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='（a） （b）\\nFigure 8: VL-Health dataset collection distribution.\\nA.3 VL-Health\\nThe construction of theVL-Health dataset involves two key steps:(i) data collection, (ii) data processing, as detailed below:\\nData Collection: During the collection phase, we carefully considered the diversity of medical images and the complexity of\\nthe tasks, selecting appropriate subsets for comprehension and generation tasks. For comprehension tasks, we selected datasets\\nsuch as VQA-RAD (Lau et al. 2018), SLAKE (Liu et al. 2021), PathVQA (He et al. 2020), and MIMIC-CXR-VQA (Bae et al.\\n2024), which cover various medical imaging modalities like radiology and pathology, and include professional annotations\\nto assist the model in learning tasks such as lesion detection and disease diagnosis. Additionally, large-scale multi-modal\\ndatasets like LLaV A-Med (Li et al. 2024b) and PubMedVision (Chen et al. 2024a) were included to provide broader medical\\nknowledge support and facilitate the training of complex reasoning tasks. For generation tasks, we focused on four mainstream\\ntask categories: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction.\\nThe IXI (Davies et al. 2014) dataset, containing a large number of healthy brain MRI images, is suitable for training super-\\nresolution models; the MIMIC-CHEST-XRAY (Bae et al. 2024) dataset, with X-ray images and their corresponding textual\\nreports, is appropriate for text-to-image generation tasks; the SynthRAD2023 (Thummerer et al. 2023) dataset provides a large\\nnumber of paired CT and MRI images, supporting modality conversion model training; for image reconstruction tasks, we\\nrewrote and adjusted the LLaV A-558k (Liu et al. 2024b) dataset.\\nData Processing: After data collection, we performed filtering and processing of the raw data. For VisualQA tasks, we stan-\\ndardized the data entries into two forms: open-ended questions and single-choice questions, enabling flexible training and\\nevaluation. Additionally, considering that multi-image data has a minimal impact on performance but introduces extra padding\\nand training time, we excluded multi-image data. For the scanned image data in generation tasks, we applied slicing extrac-\\ntion, image registration, data augmentation, and normalization to treat 2D images as visual inputs for model training or used\\nVQGAN-generated indices to supervise the generation tasks.\\nData Statistics This section provides detailed statistical information about the VL-Health dataset to offer a more compre-\\nhensive understanding.\\nData Overview: To ensure a balanced development of the model’s comprehension and generation capabilities, in addition\\nto the LLaV A-558k and PubMedVision-PT datasets used for alignment, the VL-Health dataset ultimately selected 765,802\\nadditional visual question-answering (VQA) training samples (to endow the model with visual comprehension and instruction-\\nfollowing capabilities) and 783,045 generation training samples (to provide the model with reconstruction and visual generation\\ninstruction-following abilities). This contributes to the transfer of knowledge between comprehension and generation tasks, en-\\nhancing the model’s overall performance. For medical image comprehension tasks, images were selected from VQA-RAD (ap-\\nproximately 450 images), SLAKE (approximately 630 images), PathVQA (approximately 2,600 images), MIMIC-CXR-VQA\\n(approximately 52,000 images), LLaV A-Med (approximately 61,000 images), and PubMedVision (approximately 500,000 im-\\nages). Multiple question-answer pairs were retained for each image to enhance the model’s understanding and generalization\\nof the image content. Table 8 shows the data distribution ofVL-Health for three-stage learning strategy, where mixed-47k is\\nbased on the sampling of all data in stage-1.\\nDiversity and Quality Assessment: VL-Health covers 11 modalities, including CT, MRI, X-ray, microscopy, OCT, ultra-\\nsound, and fundus photography, which aids the model in learning features from various modalities. The dataset also encom-\\npasses a wide range of diseases, from common to rare, and from localized lesions to systemic diseases, including pulmonary\\ndiseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides com-\\nprehensive training support to the model, enabling it to learn the characteristics and diagnosis of various diseases.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='1687014f-48f1-44a0-81b3-fdc19b576877', embedding=None, metadata={'page_label': '14', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8: Data distribution of VL-Health in three-stage learning strategy.\\nMedical Task Stage-1 Stage-2\\nComp. LLaV A-558k, PubMedVision-PT Mixed-47kGen. LLaV A-558k\\nMedical Task Stage-3\\nComp. LLaV A Med, MIMIC CXR VQA, PubMedVision-FT, LLaV A-665k, PathVQA, SLAKE, VQA-RAD\\nGen. IXI, SynthRAD2023, MIMIC-CHEST-XRAY\\nData Format. All data samples are converted into a unified instruction-response format for training and evaluation. Specifi-\\ncally, the VL-Health dataset consists of the following components:\\n• Task Type: Specifies the granularity of visual features output by the visual encoder and selects the corresponding H-\\nLoRA submodule. For generation tasks, the response also includes multi-modal tokens corresponding to VQ indices.\\n• Task Instruction: Guides the model to interpret the image and generate a response, covering various aspects of the\\nimage and specifying the output format.\\n• Response: The textual output generated based on the task instruction and input image, ensuring it meets the question and\\nformatting requirements.\\n• Input Image: Provides the visual signal for the model to process.\\n• Target Image Index: In generation tasks, this is added as a multi-modal token to the response for autoregressive\\ngeneration.\\nB Analysis of Heterogeneous Low-Rank Adaptation\\nWe propose H-LoRA, which utilizes hard routing selection to allocate plugins for knowledge learning and representation across\\ntasks, thereby preventing conflicts arising from heterogeneous knowledge. Furthermore, within each task, we optimized based\\non MoELoRA, enhancing performance while reducing computational overhead. The pseudocode is detailed Algorithm 1.\\nAlgorithm 1: H-LoRA Algorithm\\nInput: concrete-grained visual features FCon, abstract-grained visual featuresFAbs, comprehension-based H-LoRA modules\\n({AComp.\\ni }k\\ni=1, RComp.\\nouter ), generation-based H-LoRA modules({AGen.\\ni }k\\ni=1, RGen.\\nouter), task type T (comprehension or generation),\\nnumber of LoRA experts k, origin linear layer weights W0, text features T , hidden state h\\nOutput: final output O\\n// Select task-specific image features\\nif T = generation task then\\nFimg ← FCon\\nelse if T = comprehension task then\\nFimg ← FAbs\\nend if\\nU ←concat(Fimg, T ) // Concatenate image features and text features\\n{Ai}k\\ni=1, {Bi}k\\ni=1, Router ← {AT\\ni }k\\ni=1, {BT\\ni }k\\ni=1, RT\\nouter // Assign task-specific H-LoRA submodule\\n// Merge LoRA experts’ matrices\\nAmerged ← concat({Ai}k\\ni=1)\\nBmerged ← concat({Bi}k\\ni=1)\\nW ←R(h) // Generate routing weights based on input hidden state x\\nWexpanded ← α × W/r ⊗ 1r // Expand routing weights to match merged matrices\\nOH-LoRA ← (x · Amerged ⊙ Wexpanded) · Bmerged // Compute H-LoRA output using element-wise multiplication\\nO ←x · W0 + OH-LoRA // Add H-LoRA output to pre-trained weights to get final output\\nReturn O\\nWe further analyzed the computational overhead differences between MoELoRA and H-LoRA. Assuming that both methods\\nuse the same number of LoRA experts k, we can compare their time complexity from the perspective of the operational steps\\ninvolved.\\nComputational Overhead of MoELoRA. In MoELoRA, the operations involving the expert matrix mainly include the fol-\\nlowing steps: (i) Expert Multiplication : MoELoRA requires 2k multiplications with the LoRA experts. (ii) Router Multi-\\nplication: One multiplication with the Router is required. (iii) Router Output Expansion : MoELoRA needs to perform k\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='2176f658-0034-4503-9b51-710dc7a262dc', embedding=None, metadata={'page_label': '15', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='expansion operations on the Router’s output weights to generate the appropriate shapes that match the dimensions of the in-\\nput and LoRA experts while iterating through the experts. (iv) Dot Product: For each expanded Router weight, a dot product\\nwith the intermediate state of the expert is required, resulting in k multiplications. (v) Addition: Finally, k addition operations\\nare required to accumulate the results from each LoRA expert into the final output. Assuming the time complexity of each\\noperation is the same, the additional time complexity introduced when equipping a fully connected layer with MoELoRA is:\\nO(2k+1+ k+k+k) = O(5k+1). Thus, MoELoRA introduces an additional time overhead ofO(5k+1) during computation.\\nH-LoRA. In contrast to MoELoRA, H-LoRA reduces the computational overhead by concatenating the LoRA expert matrices.\\nSpecifically: (i) Expert Multiplication: H-LoRA merges all LoRA experts by directly creating a larger A and B matrix, instead\\nof performing independent operations for each expert. This process can be implemented through matrix initialization without\\nadditional concatenation operations. Therefore, only 2 multiplications with the LoRA experts are required. (ii) Router Multi-\\nplication: H-LoRA still requires one multiplication with the Router. (iii) Router Output Expansion : H-LoRA only requires\\none expansion operation on the Router’s output weights. (iv) Dot Product: H-LoRA only requires one dot product between\\nthe Router’s output and the expert’s intermediate state. (v) Addition: Finally, H-LoRA only requires one addition operation\\nto accumulate the LoRA expert results into the intermediate state. Therefore, the additional time complexity introduced by\\nH-LoRA is: O(2 + 1 + 1 + 1 + 1) =O(6).\\nComparing the two, we see that MoELoRA introduces a linear increase in additional time complexity with respect to the\\nnumber of experts k, resulting in a complexity of O(5k + 1), while H-LoRA’s additional time complexity is fixed at O(6),\\nindependent of k. We observe that when k is small, the time complexity differences between MoELoRA and H-LoRA are\\nnegligible. However, as k increases, MoELoRA’s computational overhead grows linearly, while H-LoRA’s remains constant.\\nThis makes H-LoRA significantly more computationally efficient than MoELoRA, particularly in large-scale tasks. We will\\nfurther demonstrate the significant advantage of H-LoRA in training time in subsequent experiments, validating its efficiency\\nin practical applications.\\nC Supplemental Experimental Results\\nIn this section, we include additional experiments to demonstrate the superiority of HealthGPT and articulate our design\\nphilosophy.\\nC.1 Results: OmniMedVQA Benchmark\\nOmniMedVQA (Hu et al. 2024) is a novel, large-scale medical visual question answering (VQA) benchmark designed to\\nencompass various modalities and anatomical regions by collecting diverse images from multiple medical datasets. Our exper-\\nimental results are presented in Table 9.\\nTable 9: Performance comparison of OmniMedVQA Benchmark.\\nOmniMedVQA↑Type Model # Params Medical\\nLVLM CT X-ray FDM MiS OCT MRI USS Avg.\\nComp. Only\\nMed-Flamingo 8.3B ✓ 30.1 33.9 25.5 37.0 60.0 27.6 30.4 34.9\\nLLaV A-Med 7B ✓ 28.4 32.8 42.7 31.6 55.3 45.0 53.6 41.3\\nHuatuoGPT-Vision 7B ✓ 35.3 41.5 51.4 62.3 59.3 40.4 60.1 50.0\\nBLIP-2 6.7B ✗ 26.6 29.1 22.3 36.9 29.1 22.7 21.4 26.9\\nLLaV A-v1.5 7B ✗ 28.0 55.7 35.5 42.1 49.2 52.9 49.7 44.7\\nInstructBLIP 7B ✗ 20.1 22.2 34.1 30.6 38.6 31.9 25.5 29.0\\nYi-VL 6B ✗ 51.2 47.1 27.7 62.6 67.6 55.0 40.3 50.2\\nInternVL2 8B ✗ 40.2 57.9 53.2 64.0 59.1 58.1 49.1 54.5\\nLlama-3.2 11B ✗ 37.6 55.2 71.4 82.1 62.5 65.2 68.6 63.2\\nComp. & Gen.\\nShow-o 1.3B ✗ 29.0 50.4 30.9 22.0 30.8 34.2 33.8 33.0\\nUnified-IO 2 7B ✗ 10.8 37.7 12.3 25.3 32.6 30.9 37.7 26.8\\nJanus 1.3B ✗ 24.9 54.8 35.9 62.7 54.2 50.7 36.8 45.7\\nHealthGPT-M3 3.8B ✓ 35.3 81.9 54.6 88.2 89.3 78.5 51.4 68.5\\nHealthGPT-L14 14B ✓ 39.0 86.6 64.1 88.6 99.7 80.9 62.2 74.4\\nThrough our analysis, we make the following observations: (i) HealthGPT-M3 outperforms other models in 4 out of 7\\nsub-tasks, achieving an average score that exceeds cutting-edge medical Large Vision-Language Models (LVLMs) as well as\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='28be5ce0-dc80-4d50-b31a-6d36ce4d47d8', embedding=None, metadata={'page_label': '16', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='general LVLMs; (ii) the unified model demonstrates relatively weak performance on OmniMedVQA; however, our approach\\neffectively mitigates performance degradation caused by generation tasks, serving as a unified model; (iii) HealthGPT-L14\\nexcels across all sub-tasks, achieving optimal or near-optimal results with an average score of 74.4, significantly surpassing\\nother models.\\nC.2 Stability Analysis of Number of Experts\\nWe investigated the impact of the number of LoRA experts on model performance within a multi-LoRA architecture, conducting\\nextensive experiments on MoELoRA and H-LoRA with varying numbers of experts. The experimental results are presented\\nin Table 10. As the number of experts increases, the training time for MoELoRA is significantly prolonged. When n = 8 ,\\nthe training time for MoELoRA is twice that of LoRA, whereas H-LoRA incurs no additional training delay and performs\\nbetter. It is estimated that at n = 32, the training time for MoELoRA could reach eight times that of LoRA, preventing it from\\ncompleting training and inference. This result aligns with the analysis in Appendix B, indicating that H-LoRA not only avoids\\nintroducing additional training delays compared to LoRA but also outperforms MoELoRA.\\nTable 10: We explored the performance of MoELoRA and H-LoRA with different numbers of LoRA experts. At n = 32 ,\\nMoELoRA was unable to complete training.\\nn=2 n=4 n=8 n=32Model Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time\\n+MoELoRA 50.3 62.98 1.22 × 50.0 64.33 1.49 × 50.8 63.71 2.09 × / / 5.81 ×HealthGPT w/ +H-LoRA 51.5 63.48 0.99× 52.8 64.71 1.00× 53.6 64.98 0.99× 53.5 64.74 1.01×\\nC.3 Impact of Heterogeneous Knowledge Fusion on Performance\\nTraditional unified models often utilize mixed training methods, which may result in performance degradation due to variations\\nin task modes. To address this, we propose a three-phase learning strategy to support H-LoRA, effectively mitigating inter-task\\nconflicts. Specifically, the second phase (Heterogeneous H-LoRA Plugin Adaptation) integrates LLMs with different H-LoRA\\nplugins into a new unified foundation by mixing the training of the embedding layers and output heads for two tasks. Figure\\n9 illustrates the impact of this phase on the performance of medical comprehension and generation tasks. We observe that the\\nsecond phase effectively unifies the model with minimal impact on overall performance, significantly alleviating the conflict\\nissues arising from mixed training in medical scenarios.\\nC.4 Human Evaluation.\\n65.7 65.4 67.7 67.0\\nFigure 9: Performance changes before and after the\\nstage-2.\\nWe further conduct human evaluation on the VQA-RAD, SLAKE,\\nand PathVQA benchmarks, which contain 1,000 open-ended ques-\\ntions. Specifically, we recruit 5 clinicians to rank the randomly shuf-\\nfled responses from HealthGPT-L14, LLaV A-Med, HuatuoGPT-\\nVision, Llama-3.2, InternVL-2 and Show-o. During the evaluation,\\nquestions were randomly selected, and the model-generated responses\\nwere anonymized and ranked. The results, as shown in Figure 10, in-\\ndicate that HealthGPT was frequently selected as the best answer.\\nThis suggests that HealthGPT has further application potential in\\nmedical care scenarios.\\nC.5 Reconstruction Performance\\nCurrently, unified models that align visual features based on recon-\\nstruction tasks include pre-LVLMs, post-LVLMs, as well as Unified-\\nIO 2 (Lu et al. 2024) and SEED-X (Ge et al. 2024). To investigate the\\ncontrollability of visual generation in rigorous settings such as med-\\nical contexts, we evaluated the performance of these models in med-\\nical image reconstruction in Table 11. Experimental results demon-\\nstrate that HealthGPT exhibits the most stable reconstruction per-\\nformance with a small amount of data.\\n16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5544de1c-168b-4191-8aa5-cd5eb4cdd619', embedding=None, metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='C.6 Case Study\\nFigures 11 and 12 illustrate examples of modality transformation and super-resolution reconstruction. In Figure 11, the results\\ngenerated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively\\nguiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure 12\\ndemonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the\\nimage.\\nTable 11: The experimental results for the four reconstruction tasks.\\nCT(Brain) CT(Pelvis) MRI (Brain) MRI(Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\nSEED-X 20.18 27.66 112.11 21.53 28.02 102.87 4.90 27.62 112.86 6.31 27.89 106.21\\nUnified-IO 2 83.93 36.09 17.95 85.36 35.10 25.46 87.50 34.25 25.47 86.31 33.53 29.80\\nHealthGPT-M3 91.73 36.42 15.46 94.26 37.30 12.53 88.76 33.97 27.05 84.40 33.11 32.62\\n34.08\\n15.87\\n9.16\\n5.62\\n13.33\\n21.94\\nHealthGPT\\nLLaVA-Med\\nHuatuoGPT-Vision\\nLlama-3.2\\nInternVL-2\\nShow-o\\nHuman\\nEvaluation\\n(a) (b)\\nFigure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='0f6b5fc1-8392-44dd-84e8-b46c060254fc', embedding=None, metadata={'page_label': '18', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 11: Case of modality transfer.\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='c9b83d92-ae76-40fe-8cd8-4fe94bd0b932', embedding=None, metadata={'page_label': '19', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 12: Case of MRI image super-resolution.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='a34239c3-9cb7-4c92-91a7-112e1434a421', embedding=None, metadata={'page_label': '1', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' \\n1     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='d0b1409e-b4ad-40a9-9198-000eecc769f9', embedding=None, metadata={'page_label': '2', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only \\nVersion  control     Classification  Level:   Internal   Asset  Owner:   Legal     Asset:     WSO2  LLC  -  Anti-Corruption  Policy     Document  History:  \\nDate  Revision  Author(s)  Description  Reviewed  &  Approved  By  28/07/2022  V1.0  \\nLegal  Team  and  External  Counsel  (Cooley  LLP)   \\nInitial  Version  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n19/10/2023  V1.0  \\nLegal  Team  Reviewed.  No  changes  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n \\n2     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='eb2ae63c-c316-4974-bb5f-9b7b9ca8097b', embedding=None, metadata={'page_label': '3', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only  \\nContents  I.   PURPOSE \\nII.   POLICY  STATEMENTS \\nIII.   ANTI-BRIBERY  PROHIBITIONS \\nIV.   ACCOUNTING  REQUIREMENTS \\nV.   FACILITATION  PAYMENTS \\nVI.   INTERMEDIARIES  AND  BUSINESS  PARTNERS \\nVII.   GIFTS  AND  HOSPITALITIES \\nIX.   OTHER  ACTIVITIES \\nX.   VIOLATIONS  AND  CONSEQUENCES \\nXI.   TRAINING  AND  CERTIFICATION \\nXII.   STATUS \\nXIII.   REPORTING/QUESTIONS \\nIX.   ACKNOWLEDGEMENT   \\nATTACHMENT  1:  ANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL \\n●  UNITED  KINGDOM ●  THE  UK  BRIBERY  ACT  2010 ●  SRI  LANKA ●  BRAZIL o  THE  BRAZILIAN  ANTI  CORRUPTION  ACT  2013 o  THE  BRAZILIAN  IMPROBITY  ACT  1992 o  OTHER  POTENTIAL  LIABILITIES  \\n \\n3     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='9aa353a0-cc99-4018-99e9-5d5d27bce189', embedding=None, metadata={'page_label': '4', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only WSO2\\n \\nLLC   A\\nNTI\\n-C\\nORRUPTION\\n \\nP\\nOLICY\\n  \\nA\\nPPROVED\\n \\nBY\\n \\nTHE\\n \\nB\\nOARD\\n \\nOF\\n \\nD\\nIRECTORS\\n  JULY  12,  2022   \\n  \\nI.   P\\nURPOSE\\n \\n \\nWSO2  LLC  (together  with  its  worldwide  subsidiaries,  “ WSO2 ”  or  the  “ Company ”)  has  implemented  this  \\nAnti-Corruption\\n \\nPolicy\\n \\n(the\\n \\n“\\nPolicy\\n”)\\n \\nfor\\n \\nthe\\n \\npurpose\\n \\nof\\n \\nensuring\\n \\ncompliance\\n \\nwith\\n \\nthe\\n \\nU.S.\\n \\nForeign\\n \\nCorrupt\\n \\nPractices\\n \\nAct\\n \\nof\\n \\n1977,\\n \\nas\\n \\namended\\n \\n(the\\n \\n“\\nFCPA\\n”),\\n \\nthe\\n \\nU.S.\\n \\nTravel\\n \\nAct,\\n \\nthe\\n \\nU.S.\\n \\nDomestic\\n \\nBribery\\n \\nStatute,\\n \\nthe\\n \\nUK\\n \\nBribery\\n \\nAct\\n \\n2010,\\n \\nthe\\n \\nSri\\n \\nLankan\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments),\\n \\nthe\\n \\nBrazilian\\n \\nAnti-corruption\\n \\nAct\\n \\n(Law\\n \\nNo.\\n \\n12,846/2013),\\n \\nthe\\n \\nBrazilian\\n \\nImprobity\\n \\nAct\\n \\n1992\\n \\n(Law\\n \\nNo.\\n \\n8.429/1992),\\n \\nand\\n  \\nall\\n  \\nother\\n  \\nanti-corruption\\n  \\nlaws\\n  \\nand\\n  \\nregulations\\n  \\napplicable\\n  \\nto\\n  \\nWSO2’s\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(collectively,\\n \\n“\\nAnti-Corruption\\n \\nLaws\\n”).\\n  \\nThis\\n \\nPolicy\\n \\napplies\\n \\nto\\n \\nall\\n \\nworld-wide\\n \\ndirectors,\\n \\nofficers,\\n \\nemployees,\\n \\nand\\n \\nindividuals\\n \\nserving\\n \\nas\\n \\nindependent\\n \\ncontractors\\n \\nof\\n \\nWSO2\\n \\n(collectively,\\n \\n“\\nWSO2\\n \\nPersonnel\\n”)\\n \\nto\\n \\ncomply\\n \\nwith\\n \\nthe\\n \\nprinciples\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy.\\n  \\nPlease\\n \\nreport\\n \\nall\\n \\nquestions\\n \\nor\\n \\nconcerns\\n \\nto\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nwhose\\n \\ncontact\\n \\ninformation\\n \\nappears\\n \\nbelow.\\n \\n \\nII.   P\\nOLICY\\n \\nS\\nTATEMENTS\\n \\n \\nWSO2  Personnel  are  strictly  prohibited  from  promising,  offering,  providing,  or  authorizing  cash  \\npayments\\n \\n(such\\n \\nas\\n \\nbribes\\n \\nor\\n \\nkickbacks)\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nalso\\n \\nstrictly\\n \\nprohibited\\n \\nfrom\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue\\n \\nfrom\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n    \\n \\nWSO2  Personnel  must  comply  with  all  of  the  Company’s  internal  controls,  especially  those  designed  \\nto\\n \\nensure\\n \\naccurate\\n \\nand\\n \\ncomplete\\n \\nbooks\\n \\nand\\n \\nrecords,\\n \\nor\\n \\notherwise\\n \\nprevent\\n \\ncorruption,\\n \\nself-dealing,\\n \\nembezzlement,\\n \\nfraud,\\n \\nmoney\\n \\nlaundering,\\n \\nor\\n \\nother\\n \\nimproper\\n \\nactivities.\\n \\n \\nThere  are  no  exceptions  to  this  Policy,  even  if  our  competitors  engage  in  improper  behavior  or  corruption  \\nis\\n \\nan\\n \\naccepted\\n \\npractice\\n \\nin\\n \\na\\n \\ncountry\\n \\nwhere\\n \\nwe\\n \\noperate.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nrequired\\n \\nto\\n \\nadhere\\n \\nto\\n \\nboth\\n \\nthe\\n \\nspirit\\n \\nand\\n \\nthe\\n \\nletter\\n \\nof\\n \\nthis\\n \\nPolicy\\n \\nwith\\n \\nrespect\\n \\nto\\n \\nour\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld.\\n \\n \\nIII.   A\\nNTI\\n-B\\nRIBERY\\n \\nP\\nROHIBITIONS\\n \\n \\nThe  FCPA  and  other  Anti-Corruption  Laws  prohibit  WSO2  and  WSO2  Personnel  from  corruptly  \\npromising,\\n \\noffering,\\n \\nproviding,\\n \\nor\\n \\nauthorizing\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nof\\n \\nvalue\\n \\ndirectly\\n \\nor\\n \\nindirectly\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nand\\n \\ncertain\\n \\nother\\n \\npersons\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose.\\n \\n“Improper\\n \\npurposes”\\n \\ninclude\\n \\ninfluencing\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\nthe\\n \\nrecipient\\n \\nin\\n \\nhis/her\\n \\nofficial\\n \\ncapacity,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ndo\\n \\nor\\n \\nomit\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nin\\n \\nviolation\\n \\nof\\n \\nhis/her\\n \\nlawful\\n \\nduty,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ninfluence\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\na\\n \\ngovernment\\n \\nor\\n \\ninstrumentality\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nor\\n \\nsecuring\\n \\nany\\n \\nimproper\\n \\nadvantage,\\n \\nin\\n \\norder\\n \\nto\\n \\nobtain,\\n \\nretain,\\n \\nor\\n \\ndirect\\n \\nregulatory\\n \\napprovals,\\n \\ncontracts,\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nbenefits.\\n   \\n \\n4     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='6540bb39-89af-46cd-9aa5-8ece52852f2f', embedding=None, metadata={'page_label': '5', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only The  FCPA  prohibits  improper  payments  provided  to  officials  of  governments,  state-affiliated  entities,  and  \\npolitical\\n \\nparties\\n \\noutside\\n \\nthe\\n \\nUnited\\n \\nStates.\\n \\nHowever,\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\ngovernment\\n \\nor\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\nthe\\n \\nUnited\\n \\nStates\\n \\nwill\\n \\nviolate\\n \\nU.S.\\n \\ndomestic\\n \\nbribery\\n \\nstatutes.\\n \\n \\nIn  addition  to  the  United  States,  almost  all  other  countries,  including  the  United  Kingdom,  Brazil,  and  Sri  \\nLanka,\\n \\nhave\\n \\npromulgated\\n \\ntheir\\n \\nown\\n \\nanti-bribery\\n \\nlegislation.\\n \\nMost\\n \\nof\\n \\nthose\\n \\ncountries\\n \\nprohibit\\n \\nmaking\\n \\nimproper\\n \\npayments\\n \\nto\\n \\ngovernment\\n \\nand\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\ntheir\\n \\nborders.\\n \\nHowever,\\n \\nseveral\\n \\ncountries\\n \\nhave\\n \\nalso\\n \\nadopted\\n \\nlegislation\\n \\nsimilar\\n \\nto\\n \\nthe\\n \\nFCPA\\n \\nthat\\n \\nprohibit\\n \\nimproper\\n \\npayments\\n \\noutside\\n \\nthose\\n \\ncountries.\\n  \\nThe\\n \\nexistence\\n \\nof\\n \\nall\\n \\nof\\n \\nthese\\n \\nlaws\\n \\nmeans\\n \\nthat\\n \\nthere\\n \\nis\\n \\npotential\\n \\nfor\\n \\na\\n \\ncompany\\n \\nor\\n \\nan\\n \\nindividual\\n \\nto\\n \\nface\\n \\nliability\\n \\nin\\n \\nseveral\\n \\ncountries\\n \\nfor\\n \\nthe\\n \\nsame\\n \\nsingle\\n \\nact\\n \\nof\\n \\ncorruption.\\n  \\nAttachment\\n \\n1\\n \\ncontains\\n \\nan\\n \\noverview\\n \\nof\\n \\nthe\\n \\nAnti-Corruption\\n \\nLaws\\n \\nof\\n \\nother\\n \\njurisdictions\\n \\nwhich\\n \\nare\\n \\napplicable\\n \\nto\\n \\nWSO2.\\n \\n \\nGiven  the  broad  prohibitions  under  Anti-Corruption  Laws  applicable  to  WSO2,  this  Policy  \\nprohibits\\n \\nbribes,\\n \\nkickbacks,\\n \\nand\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nand\\n \\nadvantages\\n \\nto\\n \\nany\\n \\nperson,\\n \\nentity,\\n \\nor\\n \\norganization,\\n \\nincluding,\\n \\nbut\\n \\nnot\\n \\nlimited\\n \\nto,\\n \\nemployees,\\n \\nofficials,\\n \\nrepresentatives,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\n \\n(i)  government;   \\n(ii)  state-owned  or  affiliated  entity,  including,  but  not  limited  to,  a  state  hospital,  research  \\ninstitution,\\n \\nutility,\\n \\npublic\\n \\nuniversity,\\n \\nor\\n \\nsovereign\\n \\nwealth\\n \\nfund;\\n \\n \\n(iii)  public  international  organization  such  as  the  United  Nations  or  the  World  Bank;    \\n(iv)  political  party,  including  the  party  itself  as  well  as  candidates  for  public  office;    \\n(v)  non-governmental  organization;  or   (vi)   private-sector  company.      \\nOne  may  be  asked  by  certain  parties  to  provide  a  bribe  or  other  improper  benefit  in  exchange  for  the  \\naward\\n \\nof\\n \\na\\n \\ncontract,\\n \\nsponsorship\\n \\nopportunity,\\n \\nor\\n \\nother\\n \\nbusiness;\\n \\nthe\\n \\nissuance\\n \\nor\\n \\nrenewal\\n \\nof\\n \\na\\n \\nconcession,\\n \\nlicense,\\n \\nor\\n \\nbusiness,\\n \\nconstruction,\\n \\nor\\n \\nother\\n \\npermit\\n \\nor\\n \\nregistration;\\n \\nthe\\n \\nsuccessful\\n \\nfiling\\n \\nof\\n \\na\\n \\npatent\\n \\nor\\n \\ntrademark\\n \\napplication;\\n \\nan\\n \\nimpermissible\\n \\nreduction\\n \\nin\\n \\nduties\\n \\nor\\n \\nother\\n \\ntaxes;\\n \\nobtaining\\n \\na\\n \\nfavorable\\n \\ninspection\\n \\nresult\\n \\nor\\n \\ncourt\\n \\ndecision,\\n \\neven\\n \\nif\\n \\nthe\\n \\nfacts\\n \\nor\\n \\ncircumstances\\n \\ndo\\n \\nnot\\n \\nsupport\\n \\nsuch\\n \\na\\n \\nresult;\\n \\nor\\n \\nthe\\n \\ngrant\\n \\nof\\n \\nsome\\n \\nother\\n \\nimproper\\n \\nadvantage.\\n  \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nWSO2\\n \\nPersonnel\\n \\nfrom\\n \\nproviding\\n \\nbribes\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nany\\n \\nof\\n \\nthe\\n \\nabove\\n \\npurposes.\\n \\n \\nA  violation  of  this  Policy  can  occur  even  if  the  bribe  fails  to  achieve  the  purpose  for  which  it  was  \\nintended.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\na\\n \\nperson\\n \\ncan\\n \\nviolate\\n \\nthis\\n \\nPolicy\\n \\nif\\n \\nthat\\n \\nperson\\n \\nprovides\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nbenefit\\n \\nto\\n \\na\\n \\nrecipient\\n \\nand\\n \\nthe\\n \\nrecipient\\n \\ndoes\\n \\nnot\\n \\ngrant\\n \\nany\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nin\\n \\nreturn.\\n   \\nIn\\n \\naddition,\\n \\nthe\\n \\nmere\\n \\noffer\\n \\nor\\n \\npromise\\n \\nof\\n \\na\\n \\nbribe\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefit\\n \\nis\\n \\nsufficient\\n \\nto\\n \\ncause\\n \\na\\n \\nviolation.\\n  \\nAll\\n \\nof\\n \\nthe\\n \\nanti-bribery\\n \\nprohibitions\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy\\n \\napply\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\none\\n \\nuses\\n \\nWSO2\\n \\nfunds\\n \\nor\\n \\npersonal\\n \\nfunds\\n \\nto\\n \\nfinance\\n \\nimproper\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits.\\n \\n \\nThis  Policy  also  prohibits  WSO2  Personnel  from  soliciting  or  accepting  bribes,  kickbacks,  or  other  \\nimproper\\n \\npayments/benefits\\n \\nfrom\\n \\nthe\\n \\nCompany’s\\n \\nvendors\\n \\nor\\n \\nother\\n \\npersons\\n \\nin\\n \\nrelation\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\nFor\\n \\n5     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5131d538-492d-4d7b-876a-4c7b00d3d5b8', embedding=None, metadata={'page_label': '6', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only instance,  a  violation  of  this  Policy  will  occur  if  you  cause  WSO2  to  overpay  a  vendor  and  that  vendor  then  \\nshares\\n \\nall\\n \\nor\\n \\na\\n \\nportion\\n \\nof\\n \\nthat\\n \\noverpayment\\n \\nwith\\n \\nyou.\\n   \\n \\nThis  Policy  requires  WSO2  Personnel  to  adhere  to  high  ethical  standards  and  to  comply  with  all  \\napplicable\\n \\nlaws\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nthe\\n \\nCompany.\\n  \\nAnti-corruption\\n \\nviolations\\n \\ntypically\\n \\ninvolve\\n \\ncircumstances\\n \\nthat\\n \\nalso\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws,\\n \\nincluding\\n \\nthose\\n \\nthat\\n \\naddress\\n \\nmoney\\n \\nlaundering,\\n \\nembezzlement,\\n \\nfraud,\\n \\nexport\\n \\ncontrols,\\n \\nand\\n \\nsanctions/embargoes.\\n \\nGuilty\\n \\npersons\\n \\ncan\\n \\nface\\n \\nmultiple\\n \\ncharges\\n \\nbased\\n \\non\\n \\nthe\\n \\nsame\\n \\nset\\n \\nof\\n \\nfacts.\\n \\n \\nIV.   A\\nCCOUNTING\\n \\nR\\nEQUIREMENTS\\n \\n \\nWSO2  must  maintain  books,  records,  and  accounts,  which,  in  reasonable  detail,  accurately  and  fairly  \\nreflect\\n \\nthe\\n \\nCompany’s\\n \\ntransactions,\\n \\nexpenses,\\n \\nand\\n \\nasset\\n \\ndispositions.\\n \\nWSO2\\n \\nis\\n \\nalso\\n \\ncommitted\\n \\nto\\n \\nmaintaining\\n \\na\\n  \\nsystem\\n \\nof\\n \\ninternal\\n \\naccounting\\n \\ncontrols\\n \\nto\\n \\nprovide\\n \\nreasonable\\n \\nassurances\\n \\nthat\\n \\ntransactions\\n \\nare\\n \\nproperly\\n \\nauthorized\\n \\nby\\n \\nmanagement,\\n \\nexecuted,\\n \\nand\\n \\nrecorded.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\ncomply\\n \\nwith\\n  \\nour\\n \\ninternal\\n \\ncontrols\\n \\nand\\n \\navoid\\n \\nunauthorized\\n \\nactivities\\n \\nor\\n \\nexpenses.\\n  \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\nalso\\n \\ncooperate\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nperiodic\\n \\naudits\\n \\nand\\n \\nother\\n \\nefforts\\n \\nto\\n \\nensure\\n \\nthat\\n \\nour\\n \\ninternal\\n \\ncontrols\\n \\nare\\n \\nbeing\\n \\nobserved.\\n \\n \\nViolations  of  the  above  accounting  standards  can  occur  if  one  conceals  bribes  or  falsifies  other  \\ntransactions\\n \\nor\\n \\nexpenses,\\n \\neven\\n \\nif\\n \\nthey\\n \\nare\\n \\nnot\\n \\nrelated\\n \\nto\\n \\na\\n \\nbribe,\\n \\nin\\n \\nWSO2’s\\n \\nledgers\\n \\nor\\n \\nother\\n \\nrecords.\\n  \\nAlso,\\n \\nthere\\n \\nis\\n \\nno\\n \\nmateriality\\n \\nstandard.\\n \\nThis\\n \\nmeans\\n \\nthat\\n \\neven\\n \\nsmall\\n \\nmisreported\\n \\namounts\\n \\nmay\\n \\nresult\\n \\nin\\n \\nviolations.\\n   \\n \\nV.   F\\nACILITATION\\n \\nP\\nAYMENTS\\n \\n \\n \\nThis  Policy  prohibits  all  corrupt  payments  or  benefits,  including  so-called  grease  or  facilitation  payments  \\nprovided\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\nto\\n \\nexpedite\\n \\nor\\n \\nsecure\\n \\nroutine\\n \\ngovernment\\n \\nactions\\n \\n(collectively,\\n \\n“\\nFacilitation\\n \\nPayments\\n”).\\n  \\nFacilitation\\n \\nPayments\\n \\ninclude\\n \\npayments\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nto\\n \\nexpedite\\n \\nroutine\\n \\nand\\n \\nnondiscretionary\\n \\nactivities,\\n \\nsuch\\n \\nas\\n \\nprocessing\\n \\npermit\\n \\nand\\n \\nlicense\\n \\napplications,\\n \\nscheduling\\n \\ninspections,\\n \\nand/or\\n \\nproviding\\n \\ninfrastructure\\n \\nservices\\n \\n(\\ne.g.\\n,\\n \\nwater,\\n \\nelectricity\\n \\nmail).\\n  \\nWSO2\\n \\nstrictly\\n \\nprohibits\\n \\nthe\\n \\noffer,\\n \\npromise,\\n \\nor\\n \\nprovision\\n \\nof\\n \\nFacilitation\\n \\nPayments\\n \\nto\\n \\nany\\n \\ndomestic\\n \\nor\\n \\nforeign\\n \\nlocal\\n \\nor\\n \\nfederal\\n \\ngovernment\\n \\nofficial,\\n \\nas\\n \\nthey\\n \\ncan\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws\\n \\nand\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndescribed\\n \\nabove.\\n \\n \\nPlease  note  that  in  some  cases,  government  agencies  may  impose  official  fees  that  may  be  paid  directly  in  \\nthe\\n \\nname\\n \\nof\\n \\na\\n \\ngovernmental\\n \\nentity\\n \\nor\\n \\nenterprise\\n \\nitself,\\n \\nas\\n \\nset\\n \\nout\\n \\nin\\n \\npublished\\n \\nfee\\n \\nschedules\\n \\nor\\n \\nother\\n \\nofficial\\n \\ndocuments.\\n  \\nThese\\n \\nofficial\\n \\ngovernment\\n \\nfees\\n \\ncan\\n \\nbe\\n \\npaid\\n \\nto\\n \\nexpedite\\n \\npassports,\\n \\nlicenses,\\n \\nor\\n \\nother\\n \\nservices,\\n \\nprovided\\n \\nthat\\n \\nthey\\n \\nare\\n \\ndeposited\\n \\nin\\n \\nthe\\n \\ntreasury\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nan\\n \\nofficial\\n \\ngovernment\\n \\nreceipt\\n \\nis\\n \\ncollected,\\n \\nand\\n \\nthe\\n \\nexpense\\n \\nis\\n \\naccurately\\n \\nrecorded\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n  \\nHowever,\\n \\nFacilitation\\n \\nPayments\\n \\nprovided\\n \\nfor\\n \\nthe\\n \\nbenefit\\n \\nof\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\n(\\ni.e.\\n,\\n \\nare\\n \\nnot\\n \\ndeposited\\n \\nin\\n \\nan\\n \\nofficial\\n \\ntreasury\\n \\naccount\\n \\nbelonging\\n \\nto\\n \\na\\n \\ngovernment)\\n \\nwill\\n \\nviolate\\n \\nthis\\n \\nPolicy.\\n     \\n \\n \\nVI.   I\\nNTERMEDIARIES\\n \\nAND\\n \\nB\\nUSINESS\\n \\nP\\nARTNERS\\n \\n \\nThis  Policy  prohibits  WSO2  Personnel  from  providing  bribes  or  other  improper  benefits  directly  as  well  \\nas\\n \\nindirectly\\n \\nthrough\\n \\nthird\\n \\nparties.\\n \\nThis\\n \\nrisk\\n \\ncan\\n \\narise\\n \\nin\\n \\ncases\\n \\nwhere\\n \\nthe\\n \\nCompany\\n \\nworks\\n \\nwith\\n \\nagents,\\n 6     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='74c3e57c-f8e3-438c-b95b-bc17adf9d045', embedding=None, metadata={'page_label': '7', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only consultants,  representatives,  lobbyists,  suppliers/vendors,  resellers,  distributors,  customs  or  other  brokers,  \\ncontractors,\\n \\nadvisors,\\n \\nother\\n \\nbusiness\\n \\npartners,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nthat\\n \\nperforms\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\nWSO2\\n \\n(collectively\\n \\n“\\nIntermediaries\\n”).\\n   \\n \\nIn  certain  cases,  WSO2  and  WSO2  Personnel  can  be  held  liable  under  the  FCPA  and  other  laws  even  if  \\none\\n \\ndoes\\n \\nnot\\n \\nexpressly\\n \\nauthorize\\n \\nan\\n \\nIntermediary\\n \\nto\\n \\nengage\\n \\nin\\n \\ncorruption,\\n \\nbut\\n \\nthey\\n \\ndo\\n \\nso\\n \\nanyway.\\n \\nThis\\n \\ncan\\n \\noccur\\n \\nif\\n \\none\\n \\n(i)\\n \\nhas\\n \\nactual\\n \\nknowledge\\n \\nor\\n \\na\\n \\nfirm\\n \\nbelief\\n \\nthat\\n \\na\\n \\nperson\\n \\nwill\\n \\nengage\\n \\nin\\n \\ncorruption\\n \\nor\\n \\n(ii)\\n \\nconsciously\\n \\ndisregards,\\n \\ndeliberately\\n \\nignores,\\n \\nor\\n \\nis\\n \\nwillfully\\n \\nblind\\n \\nto\\n \\nthe\\n \\nIntermediary’s\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\npractices.\\n  \\n \\nGiven  these  risks,  this  Policy  requires  that  (i)  appropriate,  risk-based  anti-corruption  due  diligence  is  \\nperformed\\n \\non\\n \\nIntermediaries\\n \\nto\\n \\nconfirm\\n \\nthat\\n \\nsuch\\n \\nIntermediary\\n \\ndoes\\n \\nnot\\n \\nhave\\n \\na\\n \\nhistory\\n \\nor\\n \\nreputation\\n \\nfor\\n \\ncorruption\\n \\nor\\n \\nsimilar\\n \\nwrong\\n \\ndoing,\\n \\nand\\n \\n(ii)\\n \\nthe\\n \\nIntermediary\\n \\nhas\\n \\nexecuted\\n \\na\\n \\nwritten\\n \\nagreement\\n \\ncontaining\\n \\nanti-corruption\\n \\ncompliance\\n \\nclauses.\\n \\nPlease\\n \\nconsult\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nfor\\n \\ninformation\\n \\nregarding\\n \\nWSO2’s\\n \\nIntermediary\\n \\ndue\\n \\ndiligence\\n \\nprocedures.\\n \\n \\nThroughout  any  relationship  with  an  Intermediary,  WSO2  Personnel  must  monitor  their  performance  to  \\nensure\\n \\nthat\\n \\nthey\\n \\ndo\\n \\nnot\\n \\nengage\\n \\nin\\n \\nactivities\\n \\nthat\\n \\nraise\\n \\ncorruption\\n \\nconcerns.\\n  \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nprovide\\n \\nguidance\\n \\non\\n \\nthe\\n \\ntypes\\n \\nof\\n \\nred\\n \\nflags\\n \\nthat\\n \\none\\n \\nshould\\n \\nmonitor\\n \\nbefore\\n \\nand\\n \\nafter\\n \\nengaging\\n \\nan\\n \\nIntermediary.\\n \\n \\nThis  Policy  requires  WSO2  Personnel  to  notify  the  Compliance  Officer  if  they  learn  of  any  Company  \\nIntermediary\\n \\nthat\\n \\nengages\\n \\nin\\n \\ncorrupt\\n \\nor\\n \\nother\\n \\nimproper\\n \\npractices.\\n \\nAlso,\\n \\nall\\n \\npayments\\n \\nto\\n \\nIntermediaries\\n \\nmust\\n \\nbe\\n \\naccurately\\n \\nreported\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks\\n \\nand\\n \\nrecords\\n \\nin\\n \\naccordance\\n \\nwith\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndiscussed\\n \\nabove.\\n \\n \\nVII.   G\\nIFTS\\n \\nAND\\n \\nH\\nOSPITALITIES\\n \\n \\nAnti-Corruption  Laws  prohibit  the  provision  or  acceptance  of  money  or  things  of  value  for  corrupt  or  \\nimproper\\n \\npurposes.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthis\\n \\nprohibition\\n \\nis\\n \\nlikely\\n \\nin\\n \\ninstances\\n \\nwhere\\n \\npersonal\\n \\nbenefits\\n \\nare\\n \\ngiven\\n \\nor\\n \\naccepted\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nnegotiation\\n \\nor\\n \\ntender\\n \\nbid.\\n \\nHowever,\\n \\nreasonably\\n \\npriced\\n \\ngifts,\\n \\nmeals,\\n \\nentertainment,\\n \\ntravel,\\n \\nand\\n \\nother\\n \\nbenefits\\n \\nprovided\\n \\nfor\\n \\nnon-corrupt\\n \\nbusiness\\n \\npromotion\\n \\nor\\n \\ngoodwill\\n \\npurposes\\n \\nmay\\n \\nbe\\n \\npermissible\\n \\nunder\\n \\nAnti-Corruption\\n \\nLaws\\n \\nin\\n \\ncertain\\n \\ncases.\\n  \\nFor\\n \\ninstance,\\n \\na\\n \\nplastic\\n \\npen,\\n \\na\\n \\nt-shirt,\\n \\na\\n \\ncoffee\\n \\nmug,\\n \\na\\n \\npaper\\n \\nweight,\\n \\nor\\n \\na\\n \\ncap\\n \\nof\\n \\nmoderate\\n \\nvalue\\n \\nand\\n \\nembossed\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nlogo\\n \\nwill\\n \\ngenerally\\n \\nnot\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws.\\n \\nHowever,\\n \\na\\n \\nfur\\n \\ncoat,\\n \\na\\n \\ncar,\\n \\nor\\n \\na\\n \\nvacation\\n \\nwill\\n \\nraise\\n \\nanticorruption\\n \\nconcerns,\\n \\nespecially\\n \\nif\\n \\nsuch\\n \\nbenefits\\n \\nare\\n \\nprovided\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nother\\n \\nperson\\n \\nwho\\n \\nis\\n \\nresponsible\\n \\nfor\\n \\nmaking\\n \\ndecisions\\n \\nin\\n \\nrelation\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n   \\n \\nWSO2  Personnel  must  also  ensure  that  the  provision  of  a  gift  or  other  benefit  does  not  violate  local  laws  \\nor\\n \\npolicies\\n \\nthat\\n \\napply\\n \\nin\\n \\nthe\\n \\ncountry\\n \\nwhere\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nthe\\n \\nbenefit\\n \\nis\\n \\nlocated.\\n  \\nSome\\n \\ncountries\\n \\nimpose\\n \\nexpress\\n \\nlimits\\n \\non\\n \\nthe\\n \\nvalue\\n \\nof\\n \\ngifts/benefits\\n \\nthat\\n \\na\\n \\nrecipient\\n \\ncan\\n \\naccept;\\n \\nother\\n \\ncountries\\n \\nban\\n \\nsuch\\n \\ngifts/benefits\\n \\naltogether\\n \\neven\\n \\nif\\n \\ngiven\\n \\nwith\\n \\nno\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\nintention.\\n \\n \\nWSO2  Personnel  must  obtain  the  approval  of  the  Compliance  Officer  prior  to  providing  gifts,  meals,  \\ntravel\\n \\nbenefits,\\n \\nand\\n \\nother\\n \\nhospitalities\\n \\nto\\n \\nemployees,\\n \\nofficials,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\ngovernment,\\n \\npolitical\\n \\nparty,\\n \\nstateowned\\n \\nentity,\\n \\nor\\n \\npublic\\n \\ninternational\\n \\norganization.\\n \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nhelp\\n \\ndetermine\\n \\n7     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='5e75fc0d-3e5e-4fae-b83d-7225f5dff1a7', embedding=None, metadata={'page_label': '8', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only whether  the  provision  of  the  benefit  is  permissible  under  applicable  Anti-Corruption  Laws.   If  the  expense  \\nis\\n \\napproved,\\n \\nits\\n \\nvalue\\n \\nand\\n \\nbusiness\\n \\npurpose\\n \\nmust\\n \\nbe\\n \\nrecorded\\n \\naccurately\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nCompany\\n \\npersonnel\\n \\nfrom\\n \\nproviding\\n \\ncash\\n \\nor\\n \\ngift\\n \\ncards\\n \\nor\\n \\ngift\\n \\ncertificates\\n \\nthat\\n \\ncan\\n \\neasily\\n \\nbe\\n \\nconverted\\n \\ninto\\n \\ncash.\\n \\n \\nIX.   O\\nTHER\\n \\nA\\nCTIVITIES\\n \\n \\nCorruption  concerns  can  arise  in  a  number  of  other  cases  including,  but  not  limited  to  (i)  joint  ventures  or  \\nteaming\\n \\narrangements\\n \\nwith\\n \\npublic\\n \\nor\\n \\nprivate-sector\\n \\npartners;\\n \\n(ii)\\n \\nmergers\\n \\nand\\n \\nacquisitions,\\n \\nespecially\\n \\nif\\n \\nthe\\n \\ntarget\\n \\nbusiness\\n \\nhas\\n \\nsignificant\\n \\ngovernment\\n \\ninteractions\\n \\nor\\n \\nan\\n \\ninternational\\n \\nprofile;\\n \\nand\\n \\n(iii)\\n \\ncharitable\\n \\nand\\n \\npolitical\\n \\ndonations.\\n \\nPlease\\n \\nconfer\\n \\nwith\\n \\nthe\\n \\nCompliance\\n \\nOfficer\\n \\nbefore\\n \\nengaging\\n \\nin\\n \\nthese\\n \\ntypes\\n \\nof\\n \\nactivities\\n \\nto\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nanti-corruption\\n \\ncompliance\\n \\nmeasures\\n \\nare\\n \\nobserved.\\n \\n \\n \\nX.   V\\nIOLATIONS\\n \\nAND\\n \\nC\\nONSEQUENCES\\n \\n \\nA  violation  of  this  Policy  will  result  in  appropriate  disciplinary  action,  including  demotion,  reassignment,  \\nadditional\\n \\ntraining,\\n \\nprobation,\\n \\nsuspension,\\n \\nor\\n \\neven\\n \\ntermination.\\n \\n \\nBoth  the  Company  and  Company  Personnel  may  be  subject  to  substantial  fines  and  penalties  for  violating  \\nAnti-Corruption\\n \\nLaws.\\n  \\nIn\\n \\nserious\\n \\ncases,\\n \\nindividuals\\n \\nmay\\n \\nface\\n \\nimprisonment,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\nassessment\\n \\nof\\n \\nmonetary\\n \\nfines\\n \\nand\\n \\npenalties.\\n  \\nIn\\n \\naddition,\\n \\nthe\\n \\nCompany\\n \\nmay\\n \\nface\\n \\nsuspension\\n \\nor\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts,\\n \\nthe\\n \\nloss\\n \\nof\\n \\nU.S.\\n \\nexport\\n \\nprivileges,\\n \\nand\\n \\ncertain\\n \\nother\\n \\nconsequences.\\n \\nThese\\n \\nresults\\n \\ncan\\n \\nbe\\n \\ndevastating\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\n \\nXI.   T\\nRAINING\\n \\nAND\\n \\nC\\nERTIFICATION\\n \\n \\n \\nAll  designated  personnel  must  undergo  anti-corruption  training  provided  by  WSO2.   The  nature,  content,  \\nand\\n \\nfrequency\\n \\nof\\n \\nthat\\n \\ntraining\\n \\nwill\\n \\nbe\\n \\ndetermined\\n \\nby\\n \\nWSO2\\n \\nbased\\n \\non\\n \\nrisk\\n \\nprofile.\\n  \\n \\nWSO2  may  require  certain  WSO2  Personnel  to  certify  compliance  with  this  Policy  on  a  periodic  basis.   \\nXII.   S\\nTATUS\\n \\n \\n \\nThe  Compliance  Officer  and/or  outside  counsel  will  review  this  Policy  on  a  periodic  basis  and  update  it,  \\nas\\n \\nappropriate,\\n \\nto\\n \\nreflect\\n \\nany\\n \\nchanges.\\n   \\n \\nThis  Policy  does  not  form  part  of  any  employment  contract  with  you  and  may  be  amended  at  any  time.   \\nThis\\n \\nPolicy\\n \\nshould\\n \\nbe\\n \\nread\\n \\nin\\n \\nconjunction\\n \\nwith\\n \\nWSO2’s\\n \\nother\\n \\npolicies.\\n \\n \\n \\n8     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='43551bf5-0a46-45f8-becb-d4356dd4d8ec', embedding=None, metadata={'page_label': '9', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only XIII.   R\\nEPORTING\\n/Q\\nUESTIONS\\n  \\n \\n \\nWSO2  Personnel  have  an  affirmative  obligation  to  report  all  violations  of  this  Policy  to  the  Compliance  \\nOfficer\\n \\nas\\n \\nfollows:\\n \\n \\nPuny  Navaratne  legal-compliance@wso2.com   \\nReports  may  also  be  submitted  anonymously  by  using  the  Company’s  hotline  number  \\n(800)\\n \\n461-9330\\n \\nor\\n \\nonline\\n \\nat\\n \\nwhistleblower.wso2.com.\\n    \\nHowever,\\n \\nwe\\n \\nencourage\\n \\nyou\\n \\nto\\n \\nconsider\\n \\nrevealing\\n \\nyour\\n \\nidentity\\n \\nso\\n \\nthat\\n \\nwe\\n \\ncan\\n \\nproperly\\n \\nfollow\\n \\nup\\n \\nand\\n \\ninvestigate\\n \\nalleged\\n \\nviolations.\\n \\nThe\\n \\nCompany\\n \\nwill\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nconfidentiality\\n \\nmeasures\\n \\nare\\n \\ntaken\\n \\nand\\n \\nwill\\n \\nnot\\n \\nretaliate\\n \\nagainst\\n \\nany\\n \\nindividual\\n \\nfor\\n \\nreporting\\n \\nviolations\\n \\nin\\n \\ngood\\n \\nfaith.\\n \\n \\nWSO2  Personnel  must  also  notify  the  Compliance  Officer  of  any  corrupt,  improper,  illegal,  or  other  \\nunusual\\n \\nrequests\\n \\nfor\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits\\n \\nmade\\n \\nby\\n \\ncustomers,\\n \\nIntermediaries,\\n \\nvendors,\\n \\nbusiness\\n \\npartners,\\n \\ngovernment\\n \\nofficials,\\n \\nor\\n \\nCompany\\n \\nemployees.\\n   \\nBy\\n \\nreporting\\n \\nsuch\\n \\nmatters,\\n \\nyou\\n \\nwill\\n \\nenable\\n \\nus\\n \\nto\\n \\nexplore\\n \\noptions\\n \\nto\\n \\nachieve\\n \\nour\\n \\nbusiness\\n \\ngoals\\n \\nwithout\\n \\nhaving\\n \\nto\\n \\ninteract\\n \\nwith\\n \\nsuch\\n \\npersons\\n \\nor\\n \\nprovide\\n \\nimproper\\n \\nbenefits.\\n \\n \\nIX.   ACKNOWLEDGEMENT\\n  \\n \\n \\nPlease  click here to  certify  and  acknowledge  that  you  have  read  and  understood  the  contents  of  this  Policy.  \\n9     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b63099a8-4fb0-4c39-ab1c-43ac45eff3aa', embedding=None, metadata={'page_label': '10', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only ATTACHMENT  1   \\nANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL   \\n  \\nUNITED  KINGDOM   \\nT\\nHE\\n \\nUK\\n \\nB\\nRIBERY\\n \\nA\\nCT\\n \\n2010  \\n \\n  Among  various  matters,  the  UK  Bribery  Act  2010  (the  “ UKBA ”)  prohibits  individuals  and  entities  from  \\noffering,\\n \\npromising,\\n \\nor\\n \\ngiving\\n \\n(directly\\n \\nor\\n \\nindirectly\\n \\nthrough\\n \\na\\n \\nthird\\n \\nparty)\\n \\na\\n \\nfinancial\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nto\\n \\na\\n \\nrecipient\\n \\nwith\\n \\n(i)\\n \\nthe\\n \\nintention\\n \\nthat\\n \\nthe\\n \\nadvantage\\n \\ninduce\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\nperform\\n \\nimproperly\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity\\n \\nor\\n \\nto\\n \\nreward\\n \\na\\n \\nperson\\n \\nfor\\n \\nthe\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\nsuch\\n \\nfunction\\n \\nor\\n \\nactivity,\\n \\nor\\n \\n(ii)\\n \\nthe\\n \\nknowledge\\n \\nor\\n \\nbelief\\n \\nthat\\n \\nthe\\n \\nacceptance\\n \\nof\\n \\nthe\\n \\nadvantage\\n \\nwould\\n \\nitself\\n \\nconstitute\\n \\nan\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nUKBA\\n \\nwill\\n \\noccur\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nadvantage\\n \\nis\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nan\\n \\nemployee\\n \\nof\\n \\na\\n \\nprivatesector\\n \\nentity.\\n \\n   The  UKBA  contains  four  principal  offenses  as  follows:  (i)  offering,  promising,  or  giving  of  a  bribe  to  \\nanother\\n \\nperson\\n \\n(Section\\n \\n1);\\n \\n(ii)\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\na\\n \\nbribe\\n \\n(Section\\n \\n2);\\n \\n(iii)\\n \\nbribery\\n \\nof\\n \\na\\n \\nforeign\\n \\n(non-UK)\\n \\npublic\\n \\nofficial\\n \\n(Section\\n \\n6);\\n \\nand\\n \\n(iv)\\n \\nfailure\\n \\nby\\n \\ncertain\\n \\ncommercial\\n \\norganizations\\n \\nto\\n \\nprevent\\n \\nSection\\n \\n1\\n \\nor\\n \\n6\\n \\nbribery\\n \\noffenses\\n \\nby\\n \\ntheir\\n \\nassociated\\n \\npersons\\n \\n(including\\n \\nemployees,\\n \\ncontractors,\\n \\nIntermediaries,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\na\\n \\ncompany)\\n \\nof\\n \\nany\\n \\nnationality\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(Section\\n \\n7).\\n  \\nThe\\n \\nUKBA\\n \\nprovides\\n \\na\\n \\nstatutory\\n \\ndefense\\n \\nto\\n \\na\\n \\nSection\\n \\n7\\n \\nviolation\\n \\nfor\\n \\ncompanies\\n \\nthat\\n \\ncan\\n \\ndemonstrate\\n \\nthat\\n \\nthey\\n \\nhad\\n \\nin\\n \\nplace\\n \\nadequate\\n \\nsystems\\n \\nand\\n \\ncontrols\\n \\ndesigned\\n \\nto\\n \\nprevent\\n \\noffenses\\n \\nunder\\n \\nUKBA.\\n \\nThis\\n \\nPolicy\\n \\nis\\n \\npart\\n \\nof\\n \\nthe\\n \\nCompany’s\\n \\noverall\\n \\neffort\\n \\nto\\n \\nestablish\\n \\nsuch\\n \\nsystems\\n \\nand\\n \\ncontrols.\\n  \\n   Courts  in  the  United  Kingdom  exercise  broad  jurisdiction  over  UK  as  well  as  non-UK  persons  who  \\ncommit\\n \\nUKBA\\n \\noffenses.\\n  \\nThe\\n \\nCompany\\n \\nmaintains\\n \\na\\n \\nUK\\n \\nsubsidiary.\\n \\nIt\\n \\nis\\n \\nclear\\n \\nthat\\n \\nboth\\n \\nthis\\n \\nUK\\n \\nsubsidiary\\n \\nand\\n \\nmost\\n \\nof\\n \\nits\\n \\nemployees\\n \\nwill\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nthe\\n \\nUKBA.\\n  \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-UK\\n \\nentities\\n \\nand\\n \\nemployees\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nUKBA\\n \\njurisdiction.\\n \\n    Under  the  UKBA,  individuals  guilty  of  bribery  may  be  subject  to  imprisonment  for  up  to  10  years  and/or  \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount.\\n \\nCommercial\\n \\norganizations\\n \\nguilty\\n \\nof\\n \\nbribery\\n \\nor\\n \\nfailure\\n \\nto\\n \\nprevent\\n \\nbribery\\n \\nmay\\n \\nalso\\n \\nbe\\n \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount\\n \\nas\\n \\nwell\\n \\nas\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts.\\n  \\nIn\\n \\naddition,\\n \\nUKBA\\n \\noffenses\\n \\ncould\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws\\n \\nsuch\\n \\nas\\n \\nthe\\n \\nUK\\n \\nProceeds\\n \\nof\\n \\nCrime\\n \\nAct\\n \\n2002,\\n \\nwhich\\n \\ncontains\\n \\nthe\\n \\nUK’s\\n \\nprincipal\\n \\nmoney\\n \\nlaundering\\n \\noffenses.\\n \\n    *   *   *   *   *     SRI  LANKA   \\nThe  legal  framework  for  the  prevention,  investigation  and  punishment  of  corruption  is  primarily  reflected  \\nin\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments).\\n  \\n \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='b4764dc2-e803-434c-8331-41fe495858ee', embedding=None, metadata={'page_label': '11', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only   The  law  prohibits  persons  from  offering  gratifications  and  rewards  to  certain  categories  of  persons  as  \\ninducements\\n \\nand\\n \\nrewards\\n \\nfor\\n \\nthe\\n \\nperformance\\n \\nor\\n \\nnonperformance\\n \\nof\\n \\nspecified\\n \\nactivities.\\n \\nThey\\n \\nare\\n \\n(a)\\n \\njudicial\\n \\nofficers\\n \\nand\\n \\nMembers\\n \\nof\\n \\nParliament\\n \\nin\\n \\nrespect\\n \\nof\\n \\ntheir\\n \\nofficial\\n \\nduties;\\n \\n(b)\\n \\npolice\\n \\nofficers,\\n \\npeace\\n \\nofficers\\n \\nor\\n \\nother\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ninterfering\\n \\nwith\\n \\nthe\\n \\ndue\\n \\nadministration\\n \\nof\\n \\njustice,\\n \\nor\\n \\nprocuring\\n \\nor\\n \\nfacilitating\\n \\nthe\\n \\ncommission\\n \\nof\\n \\nany\\n \\noffence,\\n \\nor\\n \\nprotecting\\n \\noffenders\\n \\nfrom\\n \\ndetection\\n \\nor\\n \\npunishment,\\n \\nor\\n \\nabusing\\n \\nofficial\\n \\npowers\\n \\nto\\n \\nthe\\n \\ninjury\\n \\nor\\n \\ndetriment\\n \\nof\\n \\nany\\n \\nperson;\\n \\n(c)\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ntheir\\n \\nassistance\\n \\nor\\n \\ninfluence\\n \\nin\\n \\npromoting\\n \\nthe\\n \\nprocurement\\n \\nof\\n \\nany\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nfor\\n \\nany\\n \\nwork,\\n \\nservice\\n \\nor\\n \\nthe\\n \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial\\n \\nor\\n \\nsubstance,\\n \\nor\\n \\nin\\n \\nthe\\n \\nexecution\\n \\nof\\n \\nany\\n \\ncontract,\\n \\nor\\n \\nin\\n \\nthe\\n \\npayment\\n \\nof\\n \\nthe\\n \\nprice\\n \\nor\\n \\nconsideration\\n \\nor\\n \\nof\\n \\nany\\n \\nsubsidy\\n \\nin\\n \\nrespect\\n \\nthereof;\\n \\n(d)\\n  \\na\\n \\ntenderer\\n \\nfor\\n \\na\\n \\ncontract\\n \\nto\\n \\nwithdraw\\n \\nthe\\n \\ntender,\\n \\nor\\n \\nfor\\n \\nwithdrawing\\n \\na\\n \\ntender\\n \\nmade\\n \\nfor\\n \\na\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nintent\\n \\nof\\n \\nobtaining\\n \\nsuch\\n \\ncontract\\n \\nfor\\n \\nwork,\\n \\nservice\\n \\nor\\n  \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial,\\n \\nor\\n \\nsubstance;\\n \\n(e)\\n  \\npublic\\n \\nofficers\\n \\nto\\n \\nperform,\\n \\nabstain\\n \\nfrom\\n \\nperforming,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact\\n \\nwhether\\n \\nby\\n \\nthat\\n \\npublic\\n \\nofficer\\n \\nor\\n \\nby\\n \\nany\\n \\nother\\n \\npublic\\n \\nofficer,\\n \\nor\\n \\nassisting,\\n \\nfavoring,\\n \\nhindering\\n \\nor\\n \\ndelaying\\n \\nany\\n \\nperson\\n \\nin\\n \\nthe\\n \\ntransaction\\n \\nof\\n \\nany\\n \\nbusiness\\n \\nwith\\n \\nthe\\n \\nGovernment;\\n \\n(f)\\n \\npersons\\n \\nto\\n \\nprocure\\n \\nthe\\n \\nGovernment\\n \\nto\\n \\npay\\n \\nany\\n \\nclaim,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nprevent\\n \\nappointment\\n \\nto\\n \\nany\\n \\noffice,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nsecure\\n \\nany\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nother\\n \\nbenefit\\n \\nfrom\\n \\nthe\\n \\nGovernment,\\n \\nor\\n \\nprevent\\n \\nthe\\n \\nsecuring\\n \\nof\\n \\nany\\n \\nsuch\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nbenefit\\n \\nby\\n \\nsuch\\n \\nother\\n \\nperson;\\n \\n(g)\\n \\npublic\\n \\nofficer\\n \\nemployed\\n \\nin\\n \\na\\n \\ngovernment\\n \\ndepartment,\\n \\noffice\\n \\nor\\n \\nestablishment\\n \\nwhile\\n \\nhaving\\n \\ndealings\\n \\nof\\n \\nany\\n \\nkind\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nthrough\\n \\nsuch\\n \\nan\\n \\nentity,\\n \\nor\\n \\nwithin\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nsuch\\n \\ndealings\\n \\n(provided\\n \\nthat\\n \\nif\\n \\nsuch\\n \\ngratification\\n \\nwas\\n \\npaid\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nthe\\n \\ndealings\\n \\nit\\n \\nshall\\n \\nnot\\n \\nbe\\n \\nconsidered\\n \\nan\\n \\noffence\\n \\nif\\n \\nit\\n \\ncan\\n \\nbe\\n \\nproved\\n \\nthat\\n \\nit\\n \\nwas\\n \\noffered\\n \\nin\\n \\ngood\\n \\nfaith\\n \\nfor\\n \\na\\n \\npurpose\\n \\nnot\\n \\nconnected\\n \\nwith\\n \\nor\\n \\nunrelated\\n \\nto\\n \\nsuch\\n \\ndealings,\\n \\nand\\n \\nwhen\\n \\nit\\n \\nwas\\n \\noffered,\\n \\nthere\\n \\nwas\\n \\nno\\n \\nhope\\n \\nor\\n \\nexpectation\\n \\nof\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nit\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nfor\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nthe\\n \\ngratification\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nor\\n \\nreward\\n \\nfor\\n \\nthe\\n \\npublic\\n \\nofficer\\n \\ndoing\\n \\nor\\n \\nforbearing\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nconnected\\n \\nwith\\n \\nsuch\\n \\ndealings);\\n \\n(h)\\n \\nmembers,\\n \\nofficers\\n \\nor\\n \\nemployees\\n \\nof\\n \\nlocal\\n \\nauthorities\\n \\nor\\n \\nscheduled\\n \\ninstitutions\\n \\nfor\\n \\nvoting\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nvoting\\n \\nat\\n \\nmeetings\\n \\nof\\n \\nsuch\\n \\nbodies\\n \\nfor\\n \\nor\\n \\nagainst\\n \\nmatters\\n \\narising\\n \\nbefore\\n \\nthem,\\n \\nor\\n \\ntheir\\n \\nperformance,\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nperforming,\\n \\nor\\n \\naiding\\n \\nin\\n \\nprocuring,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact,\\n \\nor\\n \\naiding,\\n \\nprocuring,\\n \\nor\\n \\npreventing\\n \\nthe\\n \\npassing\\n \\nof\\n \\nany\\n \\nvote\\n \\nor\\n \\ngranting\\n \\nof\\n \\nany\\n \\ncontract\\n \\nor\\n \\nadvantage\\n \\nin\\n \\nfavor\\n \\nof\\n \\nany\\n \\nperson.\\n  \\n \\nEach  of  the  above  provisions  have  corresponding  offence  in  respect  of  the  receipt  of  gratifications.      The  Bribery  Act  provides  for  imprisonment  of  up  to  seven  years  and  fines  of  up  to  five  thousand  Sri  \\nLankan\\n \\nrupees\\n \\nfor\\n \\nthe\\n \\ncommission\\n \\nof\\n \\noffences.\\n  \\nSri\\n \\nLankan\\n \\ncourts\\n \\nmay\\n \\nalso\\n \\nimpose\\n \\npenalties\\n \\namounting\\n \\nto\\n \\nthe\\n \\nvalue\\n \\nof\\n \\nthe\\n \\ngratification\\n \\nif\\n \\nthe\\n \\nconviction\\n \\nis\\n \\nentered\\n \\nby\\n \\na\\n \\nHigh\\n \\nCourt.\\n   \\n   Similar  offences  have  been  created  in  respect  of  members  of  public  authorities  by  the  Public  Bodies  \\n(Prevention\\n \\nof\\n \\nCorruption)\\n \\nAct\\n \\nNo\\n \\n13\\n \\nof\\n \\n1950.\\n \\n \\nThe  Bribery  Act  is  enforced  through  the  Commission  to  Investigate  Allegations  of  Bribery  or  Corruption  \\nAct\\n \\nNo.\\n \\n19\\n \\nof\\n \\n1994.\\n  \\nThe\\n \\nCommission\\n \\nhas\\n \\nwide\\n \\npowers\\n \\nof\\n \\ninvestigation\\n \\nincluding\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nas\\n \\nwell\\n \\nas\\n \\nof\\n \\nrequiring\\n \\ndeclaration\\n \\nof\\n \\nassets\\n \\nand\\n \\nliabilities\\n \\nby\\n \\nMembers\\n \\nof\\n \\nParliament,\\n \\njudges,\\n \\npublic\\n \\nofficials\\n \\nof\\n \\nGovernment\\n \\ndepartments,\\n \\nministries,\\n \\nand\\n \\nlocal\\n \\nauthorities,\\n \\nchairpersons\\n \\nand\\n \\nstaff\\n \\nof\\n \\npublic\\n \\ncorporations,\\n \\ncandidates\\n \\nfor\\n \\nelected\\n \\npublic\\n \\noffice\\n \\nand\\n \\nelected\\n \\nofficials\\n \\nunder\\n \\nthe\\n \\nDeclaration\\n \\nof\\n \\nAssets\\n \\nand\\n \\nLiabilities\\n \\nLaw,\\n \\nNo.\\n \\n1\\n \\nof\\n \\n1975.\\n  \\n \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='195d34a8-a985-45cb-8cc6-2d2e1d480c51', embedding=None, metadata={'page_label': '12', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only Sri  Lanka  also  has  a  domestic  legal  regime  against  money-laundering  which  includes  the  Prevention  of  \\nMoney-\\n \\nLaundering\\n \\nAct,\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct\\n \\nand\\n \\nthe\\n \\nConvention\\n \\non\\n \\nthe\\n \\nSuppression\\n \\nof\\n \\nTerrorist\\n \\nFinancing\\n \\nAct.\\n  \\nOffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nand\\n \\nother\\n \\ncorruption-related\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nPenal\\n \\nCode\\n \\nare\\n \\nconsidered\\n \\npredicate\\n \\noffences\\n \\nfor\\n \\nthe\\n \\npurposes\\n \\nof\\n \\nthe\\n \\nPrevention\\n \\nof\\n \\nMoney-Laundering\\n \\nAct\\n \\nand\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct.\\n  \\n \\n     *   *   *   *   *     B\\nRAZIL\\n \\n \\nTHE  BRAZILIAN  ANTICORRUPTION  ACT  2013   \\nLaw  No.  12,846/2013  (the  Brazilian  Anticorruption  Act  or  Lei  Anticorrupção  –  “ LAC ”)  provides  for  \\nstrict\\n \\nliability\\n \\nto\\n \\ncompanies,\\n1\\n \\nin\\n \\nthe\\n \\nadministrative\\n \\nand\\n \\ncivil\\n \\nspheres,\\n \\nfor\\n \\nwrongful\\n \\nacts\\n \\ncarried\\n \\nout\\n \\nin\\n \\ntheir\\n \\ninterest\\n \\nor\\n \\nfor\\n \\ntheir\\n \\nbenefit.\\n \\n   The  LAC  is  applicable  to  activities  after  January  2014  and  the  main  offenses  are:  (i)  promising,  offering  \\nor\\n \\ngiving,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nundue\\n \\nadvantage\\n \\nto\\n \\na\\n \\npublic\\n \\nagent\\n \\nor\\n \\nthird\\n \\nperson\\n \\nrelated\\n \\nto\\n \\nit;\\n \\n(ii)\\n \\nfinancing\\n \\nor\\n \\nin\\n \\nany\\n \\nway\\n \\nsponsoring\\n \\nthe\\n \\npractice\\n \\nof\\n \\nwrongdoings\\n \\ndescribed\\n \\nin\\n \\nthe\\n \\nAct;\\n \\n(iii)\\n \\nusing\\n \\na\\n \\nthird\\n \\nparty\\n \\nto\\n \\nconceal\\n \\nor\\n \\nsimulate\\n \\nits\\n \\nactual\\n \\ninterests\\n \\nor\\n \\nthe\\n \\nidentity\\n \\nof\\n \\nthe\\n \\nbeneficiaries\\n \\nof\\n \\nthe\\n \\nillegal\\n \\nacts\\n \\nagainst\\n \\nthe\\n \\npublic\\n \\nadministration;\\n \\n(iv)\\n \\nengaging\\n \\nin\\n \\nfraudulent\\n \\nacts\\n \\nin\\n \\npublic\\n \\ntenders,\\n \\nsuch\\n \\nas\\n \\nparticipating\\n \\nin\\n \\nbid\\n \\nrigging\\n \\nor\\n \\ndisturbing\\n \\nany\\n \\nstep\\n \\nof\\n \\nthe\\n \\npublic\\n \\ntender;\\n \\nand\\n \\n(v)\\n \\nobstructing\\n \\nor\\n \\nhampering\\n \\nthe\\n \\nsurveillance\\n \\nor\\n \\ninvestigations\\n \\nof\\n \\npublic\\n \\nentities.\\n \\n   In  the  administrative  sphere,  legal  entities  that  are  found  guilty  of  breaching  the  LAC  are  subject  to  a  fine  \\nof\\n \\n0.1%\\n \\nto\\n \\n20%\\n \\nof\\n \\nthe\\n \\ngross\\n \\nrevenue,\\n \\nless\\n \\ntaxes,\\n \\nregistered\\n \\nin\\n \\nthe\\n \\nyear\\n \\nprior\\n \\nto\\n \\nthe\\n \\ninitiation\\n \\nof\\n \\nthe\\n \\nadministrative\\n \\nproceedings,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\npublication\\n \\nof\\n \\nthe\\n \\ndecision.\\n \\nMoreover,\\n \\nother\\n \\npenalties\\n \\nmay\\n \\nbe\\n \\nenforced\\n \\nin\\n \\nthe\\n \\ncivil\\n \\nsphere\\n \\nby\\n \\ncourts,\\n \\nsuch\\n \\nas:\\n \\n(i)\\n \\nseizure\\n \\nof\\n \\nassets\\n \\nobtained\\n \\nthrough\\n \\nillegal\\n \\npractice;\\n \\n(ii)\\n \\nsuspension\\n \\nor\\n \\npartial\\n \\nshutdown\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity’s\\n \\nactivities;\\n \\n(iii)\\n \\ncompulsory\\n \\ntermination\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity;\\n \\nand\\n \\n(iv)\\n \\nprohibition\\n \\nof\\n \\nreceiving\\n \\nincentives,\\n \\nsubsidies,\\n \\ngrants,\\n \\ndonations\\n \\nor\\n \\nloans\\n \\nfrom\\n \\npublic\\n \\nbodies\\n \\nor\\n \\nentities\\n \\nor\\n \\nfrom\\n \\npublic\\n \\nfinancial\\n \\ninstitutions\\n \\nor\\n \\npublicly-controlled\\n \\nfinancial\\n \\ninstitutions,\\n \\nfrom\\n \\n1\\n \\nto\\n \\n5\\n \\nyears.\\n \\n   The  existence  of  an  effective  compliance  program  may  be  considered  a  mitigating  factor  in  the  evaluation  \\nof\\n \\nthe\\n \\napplicable\\n \\nfine\\n \\nto\\n \\nbe\\n \\napplied\\n \\nto\\n \\nthe\\n \\nlegal\\n \\nentity.\\n \\n   The  Company  maintains  a  Brazilian  subsidiary.  It  is  clear  that  the  Brazilian  subsidiary  may  be  held  liable  \\nunder\\n \\nthe\\n \\nLAC.\\n \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-Brazilian\\n \\nentities\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nLAC\\n \\njurisdiction.\\n \\n   \\n1\\n  In  this  sense,  the  authorities  are  not  required  to  show  intent  or  fault  of  the  legal  entity.  The  mere  fact  that  there  is  \\nmateriality\\n \\nas\\n \\nto\\n \\nthe\\n \\nviolation\\n \\nand\\n \\nthat\\n \\nthe\\n \\nviolation\\n \\nhappened\\n \\nin\\n \\nthe\\n \\ninterest\\n \\nor\\n \\nbenefit\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity\\n \\nis\\n \\nsufficient\\n \\nto\\n \\nconsider\\n \\nthat\\n \\nthere\\n \\nis\\n \\na\\n \\nbreach\\n \\nto\\n \\nthe\\n \\nAct.\\n \\n  \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='f6cad862-e011-4155-ad8a-495abdfb0e32', embedding=None, metadata={'page_label': '13', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only T\\nHE\\n \\nB\\nRAZILIAN\\n \\nI\\nMPROBITY\\n \\nA\\nCT\\n \\n1992  \\n \\n  In  a  broad  sense,  Law  No.  8,429/1992  (the  Brazilian  Improbity  Act  or  Lei  de  Improbidade  Administrativa   –  “ LIA ”)  is  applicable  to  (i)  facts  that  also  constitute  a  violation  to  the  LAC  but  happened  before  January   \\n  2014;  and  (ii)  facts  that  happened  after  January  2014  and  do  not  constitute  a  breach  to  the  LAC,  but  are  a  \\nbreach\\n \\nof\\n \\nthe\\n \\nLIA.\\n \\n   There  are  three  broad  types  of  misconduct  provided  for  in  the  LIA:  (i)  unjust  enrichment;  (ii)  damage  to  \\nthe\\n \\npublic\\n \\ntreasury;\\n \\nand\\n \\n(iii)\\n \\nacts\\n \\nin\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nprinciples\\n \\nthat\\n \\ngovern\\n \\nthe\\n \\nPublic\\n \\nAdministration.\\n  \\nThe\\n \\nLIA\\n \\nprovides\\n \\nfor\\n \\nsanctions\\n \\non\\n \\npublic\\n \\nagents,\\n \\nas\\n \\nwell\\n \\nas\\n \\non\\n \\nprivate\\n \\nentities\\n \\nand\\n \\nindividuals\\n \\nthat\\n \\nwillfully\\n \\naided\\n \\nor\\n \\nparticipated\\n \\nin\\n \\nimprobity\\n \\nacts.\\n  \\nIts\\n \\nmain\\n \\nsanctions\\n \\nare\\n \\n(i)\\n \\nforfeiture\\n \\nof\\n \\nassets\\n \\nor\\n \\nvalues\\n \\nunlawfully\\n \\nobtained;\\n \\n(ii)\\n \\ndismissal\\n \\nfrom\\n \\npublic\\n \\noffice;\\n \\n(iii)\\n \\npolitical\\n \\nblacklisting;\\n \\n(iv)\\n \\npayment\\n \\nof\\n \\nfines\\n \\nequivalent\\n \\nto\\n \\nthe\\n \\nunlawfully\\n \\nobtained\\n \\namounts;\\n \\n(v)\\n \\nprohibition\\n \\nagainst\\n \\ncontracting\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nor\\n \\nreceiving\\n \\ntax\\n \\nor\\n \\ncredit\\n \\nincentives,\\n \\ndirectly\\n \\nor\\n \\nindirectly;\\n \\nand\\n \\n(vi)\\n \\npayment\\n \\nof\\n \\nfines.\\n  \\nIn\\n \\norder\\n \\nfor\\n \\na\\n \\nsanction\\n \\nto\\n \\nbe\\n \\napplied,\\n \\nit\\n \\nis\\n \\nnecessary\\n \\nto\\n \\nshow\\n \\nintent\\n \\nof\\n \\nthe\\n \\nwrongdoer.\\n \\n   O\\nTHER\\n \\nP\\nOTENTIAL\\n \\nL\\nIABILITIES\\n \\n \\nIn  addition  to  the  LAC  and  the  LIA,  private  and  public  entities  can  request  compensation  for  collective  or  \\nmoral\\n \\ndamages\\n \\nresulting\\n \\nfrom\\n \\ncorruption\\n \\ncases,\\n \\nas\\n \\nprovided\\n \\nby\\n \\nBrazilian\\n \\nClass\\n \\nAction\\n \\nLaw,\\n \\nand\\n \\nentities\\n \\ndeemed\\n \\nto\\n \\nbe\\n \\nharmed/damaged\\n \\nby\\n \\nthe\\n \\nwrongdoing\\n \\ncan\\n \\nfile\\n \\na\\n \\nlawsuit\\n \\nclaiming\\n \\ncompensation\\n \\nfor\\n \\ndamages.\\n \\nAlso,\\n \\nthe\\n \\nBrazilian\\n \\nFederal\\n \\nCourt\\n \\nof\\n \\nAccounts\\n \\ncan\\n \\nimpose\\n \\nsanctions\\n \\nif\\n \\nthey\\n \\nfind\\n \\ncontract\\n \\nfraud\\n \\nwhile\\n \\nauditing\\n \\npublic\\n \\nentities.\\n \\n   Finally,  the  Brazilian  Criminal  Code  sets  forth  that  corruption  may  result  in  imprisonment  for  up  to  16  \\nyears\\n \\nfor\\n \\nindividuals.\\n \\n        \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PVd81aADPQKR"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# file name as id\n",
        "docs_nam_as_id = SimpleDirectoryReader(input_dir=\"./data\", filename_as_id=True).load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2T6J7waPQM6",
        "outputId": "a84c1891-a258-4b26-b91d-406ddaf2cc5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id_='/content/data/2502.09838v3.pdf_part_0', embedding=None, metadata={'page_label': '1', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='HealthGPT: A Medical Large Vision-Language Model for Unifying\\nComprehension and Generation via Heterogeneous Knowledge Adaptation\\nTianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\\nMengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\\n1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\\n4The Hong Kong University of Science and Technology,5National University of Singapore\\nProject Page\\n Code\\n1. X-Ray \\nComprehension\\n2. CT\\nComprehension\\n3. MRI\\nComprehension\\nComp. Perf.\\n7 Medical Multi-Modal Comprehension Tasks\\nGen. \\nPerformance\\n5 Medical Multi-Modal Generation Tasks\\nList all anatomical locations showing \\npulmonary edema, hazy opacity, or \\nmediastinal displacement.\\nLeft hilar structures, left lung,\\nright hilar structures, right lung.\\nWhich abdominal organ shows any \\nindication of a lesion or abnormality \\nin the CT image?\\nNo abdominal organs show\\nany clear indications of lesions\\nor abnormalities.\\nCould you explain what this mass\\nin the MRI means for my health? \\nIs it very serious?\\nCertainly, the MRI shows a\\ndefined mass in your left nasal\\ncavity. There is no sign of the ...\\n4. Microsopy Comprehension\\n What is the purpose of the different \\nmembrane treatments used in this \\nstudy?\\nThe purpose of the different\\nmembrane treatments used…\\n5. OCT Comprehension\\n What is the purpose of comparing \\nthe OCT structure image and OCTA \\nimage with H&E histology?\\nTo confirm the histological position\\nof the obtained OCT brain images.\\n6. Fundus\\nComprehension\\n What specific findings or pathological \\nchanges can be observed in this \\nfundus image?\\nThe fundus image appears normal with\\nno noticeable signs of pathology…\\n7. Ultrasound\\nComprehension\\n What type of imaging technique \\nis used in this image?\\nThe image is a sagittal gray-\\nscale ultrasonographic…\\n1. CT2MRI\\nGeneration\\nI need a version of this CT representation \\nin MRI.\\nThe image has\\nbeen transformed\\ninto MRI.\\n2. MRI2CT\\nGeneration\\nTransform the MRI display into a \\nCT image.\\nHere is the CT\\nversion of the\\nMRI image.\\n3. Image Reconstruction\\nReconstruct the following \\nmedical images.\\nHere is the reconstructed\\nmedical image you need.\\n4. Super Resolution\\nCould you improve the quality\\nof this MRI image?\\nHere is the image with\\nimproved resolution.\\n5. Report-to-CXR\\nThe X-ray shows no \\npleural effusion or \\npneumothorax.\\nHere is the\\nchest X-ray\\nimage for\\nyou.\\nGen. Perf.\\nFigure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\\nunified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\\nplex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\\nAbstract\\nWe present HealthGPT, a powerful Medical Large Vision-\\nLanguage Model (Med-LVLM) that integrates medical vi-\\nsual comprehension and generation capabilities within a uni-\\nfied autoregressive paradigm. Our bootstrapping philosophy\\nis to progressively adapt heterogeneous comprehension and\\ngeneration knowledge to pre-trained large language mod-\\nels (LLMs). This is achieved through a novel heterogeneous\\nlow-rank adaptation (H-LoRA) technique, which is com-\\nplemented by a tailored hierarchical visual perception ap-\\nproach and a three-stage learning strategy. To effectively\\nlearn the HealthGPT, we devise a comprehensive medi-\\ncal domain-specific comprehension and generation dataset\\ncalled VL-Health. Experimental results demonstrate ex-\\nceptional performance and scalability of HealthGPT in\\nmedical visual unified tasks. Our project can be accessed at\\nhttps://github.com/DCDmllm/HealthGPT.\\n1 Introduction\\nLarge Vision-Language Models (LVLMs) (Liu et al. 2023;\\nOpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\\ndemonstrated outstanding open-world visual comprehension\\nand reasoning abilities through language-based interactive\\ndialogue over the past years, simultaneously opening up\\nnew opportunities for applications in specialized domains.\\n1\\narXiv:2502.09838v3  [cs.CV]  21 Feb 2025', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_1', embedding=None, metadata={'page_label': '2', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Specifically, recent studies (Li et al. 2024a; Tu et al. 2024)\\nhave utilized pre-trained large language models (LLMs) and\\nvisual instruction data to build interactive diagnostic tools\\nand treatment planning systems, revealing the immense po-\\ntential of LVLMs in medical scenarios. However, these stud-\\nies primarily concentrate on visual comprehension tasks that\\nproduce text-based outputs, such as medical visual ques-\\ntion answering (Li et al. 2024a) or report generation (Nath\\net al. 2024), and deficient the “drawing” capability needed\\nfor medical visual generation. In practice, integrating visual\\ncomprehension and generation can significantly enhance the\\nmultifunctionality of medical LVLMs.\\nRecent studies have increasingly focused on developing\\nunified LVLMs capable of comprehending and generating\\ncontent across diverse visual modalities. Earlier approaches\\npredominantly utilized continuous visual tokens fed into\\nLLMs, using the LLMs themselves as conditional genera-\\ntors for external generative models (Ge et al. 2024; Wu et al.\\n2023; Dong et al. 2023). More recent research has explored\\nthe use of discrete visual tokens for image representation and\\ngeneration within a fully autoregressive framework (Team\\n2024; Wang et al. 2024a; Xie et al. 2024). These meth-\\nods not only enhance controllability but also demonstrate\\nearly success in open-world, any-to-any tasks, highlighting\\nthe preliminary potential of a unified autoregressive learning\\nparadigm in multi-modal tasks.\\nWhile unified LVLMs have achieved initial success in\\ngeneral scenarios, such a unified framework remains under-\\nexplored in the medical domain. Adapting the aforemen-\\ntioned general unified model paradigm to the medical do-\\nmain presents two major challenges: (i) High-scale and\\n-quality Data Limitations . Open-world models necessi-\\ntate extensive pre-training on billions or even more diverse,\\nmulti-modal data samples for comprehension and genera-\\ntion tasks (Lu et al. 2024; Team 2024). However, the ac-\\ncessible medical data significantly lacks in scale and qual-\\nity compared to natural multi-modal datasets. Its special-\\nized and domain-specific characteristics make it challenging\\nto develop a unified medical model from scratch. (ii) Con-\\nflicts between Comprehension and Generation . Compre-\\nhension tasks often strip away visual details to focus on\\nabstraction, while generation tasks require detailed preser-\\nvation, making tokens sensitive to all visual alterations. As\\nshown in Figure 2, which features experiments conducted on\\nmedical images, the performance in comprehension (or gen-\\neration) tasks steadily decreases as the proportion of genera-\\ntion (or comprehension) data increases, and vice versa. This\\nhighlights a dilemma in autoregressive multi-modal training,\\nstemming from the need to maintain consistency between\\npre- and post-LVLMs. While some methods have explored\\nmutual enhancement between comprehension and genera-\\ntion (Pan et al. 2024; Tong et al. 2024), improvements still\\nexhibit diminishing returns, with performance degradation\\nremaining a significant issue.\\n(a) (b)\\nFigure 2: With a fixed amount of comprehension (genera-\\ntion) data, increasing the proportion of the other type leads\\nto significant performance degradation.\\nTo tackle the aforementioned challenges, we propose\\nHealthGPT (see Figure 1) , which progressively adapts a\\npre-trained LLM as an unified medical multi-modal model\\nwith a small amount of visual instruction data. We de-\\nvise innovative Parameter-Efficient Fine-Tuning (PEFT) ap-\\nproach (Ding et al. 2023), calledHeterogeneous Low-Rank\\nAdaptation (H-LoRA), which decouples the learning pro-\\ncess of LVLMs for comprehension and generation tasks. In-\\nspired by the plug-and-play nature of LoRA (Hu et al. 2021),\\nH-LoRA enables the model to store heterogeneous compre-\\nhension and generation knowledge in independent “plug-\\nins”, thus avoiding joint optimization issues caused by con-\\nflicts between comprehension and generation tasks. In addi-\\ntion, we also consider the variety of sub-tasks among com-\\nprehension or generation tasks. Qualitative research high-\\nlights the limitations of a single LoRA in handling multi-\\ndimensional task scenarios, mainly due to catastrophic for-\\ngetting and interference (Liu et al. 2024d; Lin et al. 2024).\\nTo address this, we draw on the concept of Mixture of Ex-\\nperts (MoE) (Masoudnia and Ebrahimpour 2014) and in-\\ntroduce LoRA experts. The aim is to dynamically transfer\\ntask-shared knowledge to adapt to downstream tasks. Unlike\\nMoELoRA (Luo et al. 2024a), H-LoRA employs reversible\\nmatrix block multiplication to combine LoRA experts, sig-\\nnificantly reducing the overhead of multiple matrix multi-\\nplications. Notably, when using four experts, it requires\\nonly 67% of the MoELoRA training time.\\nTo effectively leverage H-LoRA inHealthGPT, we fur-\\nther introduce a Hierarchical Visual Perception (HVP)\\nand devise a correspondingThree-stage Learning Strategy\\n(TLS). HVP: we separate visual details learning from Vi-\\nsion transformer (ViT) for comprehension and generation.\\nAs is widely recognized, the ViT encodes visual concepts\\nwith increasing abstraction, generally, becoming finer as we\\nprogress over levels (Vig 2019). Thus, we maintain the vi-\\nsual features of the anterior and posterior layers to accom-\\nmodate the differing requirements for visual granularity in\\ncomprehension and generation tasks while preventing po-\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_2', embedding=None, metadata={'page_label': '3', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tential task interference. TLS: In the first and second stages,\\ngiven the heterogeneity between comprehension and gener-\\nation tasks, we first train H-LoRA plugins for HealthGPT\\nto incorporate both medical comprehension and generation\\nknowledge, thus endowing the LLMs with capabilities for\\nvision-language alignment and vision-to-vision reconstruc-\\ntion. Additionally, through minimal mixed-task training, we\\nbuilt fusion embedding layers and output heads that merge\\ntext and visual tokens, establishing a unified LVLM founda-\\ntion for visual instruction fine-tuning. In the third stage, by\\nonly training the H-LoRA plugins, HealthGPT is able to\\nrapidly adapt to a wide range of downstream medical tasks,\\ncovering various types of medical comprehension and gen-\\neration tasks.\\nTo effectively implement our approach, we have cu-\\nrated a dataset for training unified medical LVLMs, called\\nVL-Health, including seven comprehension tasks and five\\ngeneration tasks (Figure 1). Through quantitative analysis\\nand validation on multi-modal tasks, the results demonstrate\\nthat HealthGPT is capable of unifying medical multi-\\nmodal abilities in data-constrained scenarios, achieving per-\\nformance comparable to or better than existing state-of-the-\\nart (SOTA) models across multiple metrics. Overall, the\\nmain contributions of this paper are summarized as follows:\\n• Unified Med-LVLM. We introduce HealthGPT,\\nwhich, to the best of our knowledge, is the first unified\\nframework for multi-modal comprehension and genera-\\ntion in complex medical scenarios.\\n• Effective Learning Paradigm. We present H-LoRA, an\\noptimized multi-LoRA PEFT architecture based on task-\\ngated decoupling, is designed to effectively mitigate data\\nconflict issues.\\n• Holistic Training Dataset. We curated VL-Health, a\\ncomprehensive dataset designed for both comprehension\\nand generation tasks.\\n• Superior Downstream Improvements : Extensive ex-\\nperiments are conducted and the results confirm\\nHealthGPT’s effectiveness in medical vision-language\\ncomprehension and generation.\\n2 Related Work\\nMedical Vision Large Language Models. Recently, medi-\\ncal vision large language models (Med-VLLMs) have made\\nsignificant progress, demonstrating excellent performance\\nin understanding medical images and responding to human\\nqueries based on these images (Zhou et al. 2023; Tian et al.\\n2023). XrayGPT (Thawkar et al. 2023) combines a med-\\nical visual encoder (MedClip) (Wang et al. 2022) with a\\nfine-tuned LLM , using a simple linear transformation layer\\nto achieve alignment between visual and textual informa-\\ntion, significantly enhancing the understanding of medical\\nimages. On this basis, LLaV A-Med (Li et al. 2024b) fur-\\nther enhances visual-text alignment in medical contexts by\\nselecting high-quality image-text pairs from PubMed pa-\\npers and synthesized VQA datasets. BiomedGPT (Luo et al.\\n2024b) employs a BERT-style encoder and GPT-style de-\\ncoder architecture, pre-trained on interdisciplinary datasets.\\nCompared to commercial models like Med-PaLM (Singhal\\net al. 2023), BiomedGPT significantly reduces model size\\nwhile maintaining superior performance. However, issues\\nof language adaptability and dataset specificity still remain.\\nTo address these, HuatuoGPT-Vision (Chen et al. 2024a)\\nintroduces the PubMedVision dataset, which contains 1.3\\nmillion high-quality medical samples, significantly improv-\\ning the model’s adaptability across diverse medical applica-\\ntions. However, current Med-VLLMs mainly focus on med-\\nical comprehension and lack the capability for the medical\\nvision-language generation.\\nUnified Visual Comprehension and Generation Mod-\\nels. Recent research has increasingly concentrated on cre-\\nating unified LVLMs that are adept at understanding and\\nproducing content across various visual modalities. NExT-\\nGPT (Wu et al. 2023) achieves perception and generation for\\narbitrary combinations of multi-modal inputs and outputs by\\naligning LLMs. Similarly, SEED (Ge et al. 2023), SEED-\\nX (Ge et al. 2024), and DreamLLM (Dong et al. 2023) em-\\nploy learnable queries and leverage next-token prediction to\\ngenerate visual tokens, providing conditional inputs to exter-\\nnal generation modules. Unlike these methods, which func-\\ntion as external conditioners, Unified-IO (Lu et al. 2022),\\nUnified-IO 2 (Lu et al. 2024), and Chameleon (Team 2024)\\ninternalize multi-modal generation tasks within a unified\\nTransformer architecture by extending multi-modal vocab-\\nularies, enabling direct generation based on next-token pre-\\ndiction. Building on this concept, Lumina-mGPT (Liu et al.\\n2024a) and ANOLE (Chern et al. 2024) further enhance the\\ngeneration capabilities of unified models using high-quality\\ndata, particularly improving the quality and flexibility of im-\\nage generation.\\n3 Preliminaries\\nLarge Vision-Language Models.The input to a LVLM typ-\\nically consists of an image ximg and a discrete text sequence\\nxtxt. The visual encoder Eimg converts the input image ximg\\ninto a sequence of visual tokens V = [ vi]Nv\\ni=1, while the\\ntext sequence xtxt is mapped into a sequence of text to-\\nkens T = [ ti]Nt\\ni=1 using an embedding function Etxt. The\\nLLM MLLM(·|θ) models the joint probability of the token\\nsequence U = {V, T }, which is expressed as:\\nPθ(R|U) =\\nNrY\\ni=1\\nPθ(ri|{U, r<i}), (1)\\nwhere R = [ri]Nr\\ni=1 is the text response sequence. The LVLM\\niteratively generates the next token ri based on r<i. The op-\\ntimization objective is to minimize the cross-entropy loss of\\nthe response R. It is worth noting that most LVLMs adopt\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_3', embedding=None, metadata={'page_label': '4', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3: The HealthGPT architecture integrates hierarchical visual perception and H-LoRA, employing a task-specific hard\\nrouter to select visual features and H-LoRA plugins, ultimately generating outputs with an autoregressive manner.\\na design paradigm based on ViT, alignment adapters, and\\npre-trained LLMs(Liu et al. 2023, 2024b), enabling quick\\nadaptation to downstream tasks.\\nVQGAN. VQGAN (Esser, Rombach, and Ommer 2021)\\nemploys latent space compression and indexing mechanisms\\nto effectively learn a complete discrete representation of im-\\nages. VQGAN first maps the input imageximg to a latent rep-\\nresentation z = E(x) through a encoder E. Then, the latent\\nrepresentation is quantized using a codebookZ = {zk}K\\nk=1,\\ngenerating a discrete index sequence I = [im]N\\nm=1, where\\nim ∈ Zrepresents the quantized code index:\\nI = Quantize(z|Z) = arg min\\nzk∈Z\\n∥z − zk∥2. (2)\\nIn our approach, the discrete index sequence I serves as\\na supervisory signal for the generation task, enabling the\\nmodel to predict the index sequence ˆI from input conditions\\nsuch as text or other modality signals. Finally, the predicted\\nindex sequence ˆI is upsampled by the VQGAN decoder G,\\ngenerating the high-quality image ˆximg = G(ˆI).\\nLow Rank Adaptation. LoRA(Hu et al. 2021) effectively\\ncaptures the characteristics of downstream tasks by intro-\\nducing low-rank adapters. The core idea is to decompose\\nthe bypass weight matrix ∆W ∈ Rdin×dout\\ninto two low-\\nrank matrices {A ∈ Rdin×r, B ∈ Rr×dout\\n}, where r ≪\\nmin{din, dout}, significantly reducing learnable parameters.\\nThe output with the LoRA adapter for the input x is then\\ngiven by:\\nh = xW0 + αx∆W/r = xW0 + αxAB/r, (3)\\nwhere matrix A is initialized with a Gaussian distribution,\\nwhile the matrixB is initialized as a zero matrix. The scaling\\nfactor α/r controls the impact of ∆W on the model.\\n4 HealthGPT\\n4.1 Unified Autoregressive Generation.\\nHealthGPT (Figure 3) utilizes a discrete token representa-\\ntion that covers both text and visual outputs, unifying visual\\ncomprehension and generation as an autoregressive task. For\\ncomprehension, Mllm receives the input joint sequence U\\nand outputs a series of text token R = [ r1, r2, . . . , rNr ],\\nwhere ri ∈ Vtxt, and Vtxt represents the LLM’s vocabulary:\\nPθ(R | U) =\\nNrY\\ni=1\\nPθ(ri | U, r<i). (4)\\nFor generation, Mllm first receives a special start token\\n⟨START IMG⟩, then generates a series of tokens corre-\\nsponding to the VQGAN indices I = [ i1, i2, . . . , iNi ],\\nwhere ij ∈ Vvq, and Vvq represents the index range of VQ-\\nGAN. Upon completion of generation, the LLM outputs an\\nend token ⟨END IMG⟩:\\nPθ(I | U) =\\nNiY\\nj=1\\nPθ(ij | U, i<j). (5)\\nFinally, the generated index sequence I is fed into the de-\\ncoder G, which reconstructs the target image ˆximg = G(I).\\n4.2 Hierarchical Visual Perception\\nGiven the differences in visual perception between compre-\\nhension and generation tasks—where the former focuses on\\nabstract semantics and the latter emphasizes complete se-\\nmantics—we employ ViT to compress the image into dis-\\ncrete visual tokens at multiple hierarchical levels. Specif-\\nically, the image is converted into a series of features\\n{f1, f2, . . . , fL} as it passes through L ViT blocks.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_4', embedding=None, metadata={'page_label': '5', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='To address the needs of various tasks, the hidden states\\nare divided into two types: (i) Concrete-grained features\\nFCon = {f1, f2, . . . , fk}, k < L, derived from the shal-\\nlower layers of ViT, containing sufficient global features,\\nsuitable for generation tasks; (ii) Abstract-grained features\\nFAbs = {fk+1, fk+2, . . . , fL}, derived from the deeper\\nlayers of ViT, which contain abstract semantic information\\ncloser to the text space, suitable for comprehension tasks.\\nThe task type T (comprehension or generation) deter-\\nmines which set of features is selected as the input for the\\ndownstream large language model:\\nFimg\\nT =\\n(\\nFCon, if T = generation task\\nFAbs, if T = comprehension task (6)\\nWe integrate the image featuresFimg\\nT and text featuresT into\\na joint sequence through simple concatenation, which is then\\nfed into the LLM Mllm for autoregressive generation.\\n4.3 Heterogeneous Knowledge Adaptation\\nWe devise H-LoRA, which stores heterogeneous knowledge\\nfrom comprehension and generation tasks in separate mod-\\nules and dynamically routes to extract task-relevant knowl-\\nedge from these modules. At the task level, for each task type\\nT, we dynamically assign a dedicated H-LoRA submodule\\nθT , which is expressed as:\\nR = MLLM(U|θ, θT ), θ T = {AT , BT , RT\\nouter}. (7)\\nAt the feature level for a single task, H-LoRA integrates the\\nidea of Mixture of Experts (MoE) (Masoudnia and Ebrahim-\\npour 2014) and designs an efficient matrix merging and rout-\\ning weight allocation mechanism, thus avoiding the signif-\\nicant computational delay introduced by matrix splitting in\\nexisting MoELoRA (Luo et al. 2024a). Specifically, we first\\nmerge the low-rank matrices (rank = r) of k LoRA experts\\ninto a unified matrix:\\nAmerged, Bmerged = Concat({Ai}k\\n1 ), Concat({Bi}k\\n1 ), (8)\\nwhere Amerged ∈ Rdin×rk and Bmerged ∈ Rrk×dout\\n. The\\nk-dimension routing layer generates expert weights W ∈\\nRtoken num×k based on the input hidden state x, and these are\\nexpanded to Rtoken num×rk as follows:\\nWexpanded = αkW/r ⊗ 1r, (9)\\nwhere ⊗ denotes the replication operation. The overall out-\\nput of H-LoRA is computed as:\\nOH-LoRA = (xAmerged ⊙ Wexpanded)Bmerged, (10)\\nwhere ⊙ represents element-wise multiplication. Finally, the\\noutput of H-LoRA is added to the frozen pre-trained weights\\nto produce the final output:\\nO = xW0 + OH-LoRA. (11)\\n900\\n800\\n700\\n600\\n500\\n400\\n300\\n200\\n100\\n0\\nComp. Gen.\\n(a) (b)\\n783K765K\\n（K）\\nFigure 4: Data statistics of VL-Health.\\n4.4 Training Pipeline\\n1st Stage: Multi-modal Alignment. In the first stage, we\\ndesign separate visual adapters and H-LoRA submodules for\\nmedical unified tasks. For the medical comprehension task,\\nwe train abstract-grained visual adapters using high-quality\\nimage-text pairs to align visual embeddings with textual\\nembeddings, thereby enabling the model to accurately de-\\nscribe medical visual content. During this process, the pre-\\ntrained LLM and its corresponding H-LoRA submodules\\nremain frozen. In contrast, the medical generation task re-\\nquires training concrete-grained adapters and H-LoRA sub-\\nmodules while keeping the LLM frozen. Meanwhile, we ex-\\ntend the textual vocabulary to include multimodal tokens,\\nenabling the support of additional VQGAN vector quanti-\\nzation indices. The model trains on image-VQ pairs, en-\\ndowing the pre-trained LLM with the capability for image\\nreconstruction. This design ensures pixel-level consistency\\nof pre- and post-LVLM. The processes establish the initial\\nalignment between the LLM’s outputs and the visual inputs.\\n2nd Stage: Heterogeneous H-LoRA Plugin Adaptation.\\nThe submodules of H-LoRA share the word embedding\\nlayer and output head but may encounter issues such as\\nbias and scale inconsistencies during training across dif-\\nferent tasks. To ensure that the multiple H-LoRA plugins\\nseamlessly interface with the LLMs and form a unified base,\\nwe fine-tune the word embedding layer and output head us-\\ning a small amount of mixed data to maintain consistency\\nin the model weights. Specifically, during this stage, all H-\\nLoRA submodules for different tasks are kept frozen, with\\nonly the word embedding layer and output head being op-\\ntimized. Through this stage, the model accumulates foun-\\ndational knowledge for unified tasks by adapting H-LoRA\\nplugins.\\n3rd Stage: Visual Instruction Fine-Tuning. In the third\\nstage, we introduce additional task-specific data to fur-\\nther optimize the model and enhance its adaptability to\\ndownstream tasks such as medical visual comprehension\\n(e.g., medical QA, medical dialogues, and report generation)\\nor generation tasks (e.g., super-resolution, denoising, and\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_5', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Comparison of HealthGPT with other LVLMs and unified multi-modal models on medical visual comprehension\\ntasks. Bold and underlined text indicates the best performance and second-best performance, respectively.\\nVQA-RAD↑ SLAKE↑ PathVQA↑Type Model # Params Medical\\nLVLM close all close all close all\\nMMMU\\n-Med ↑ OMVQA↑ Avg.↑\\nComp. Only\\nMed-Flamingo 8.3B ✓ 58.6 43.0 47.0 25.5 61.9 31.3 28.7 34.9 41.4\\nLLaV A-Med 7B ✓ 60.2 48.1 58.4 44.8 62.3 35.7 30.0 41.3 47.6\\nHuatuoGPT-Vision 7B ✓ 66.9 53.0 59.8 49.1 52.9 32.0 42.0 50.0 50.7\\nBLIP-2 6.7B ✗ 43.4 36.8 41.6 35.3 48.5 28.8 27.3 26.9 36.1\\nLLaV A-v1.5 7B ✗ 51.8 42.8 37.1 37.7 53.5 31.4 32.7 44.7 41.5\\nInstructBLIP 7B ✗ 61.0 44.8 66.8 43.3 56.0 32.3 25.3 29.0 44.8\\nYi-VL 6B ✗ 52.6 42.1 52.4 38.4 54.9 30.9 38.0 50.2 44.9\\nInternVL2 8B ✗ 64.9 49.0 66.6 50.1 60.0 31.9 43.3 54.5 52.5\\nLlama-3.2 11B ✗ 68.9 45.5 72.4 52.1 62.8 33.6 39.3 63.2 54.7\\nComp. & Gen.\\nShow-o 1.3B ✗ 50.6 33.9 31.5 17.9 52.9 28.2 22.7 45.7 42.6\\nUnified-IO 2 7B ✗ 46.2 32.6 35.9 21.9 52.5 27.0 25.3 33.0 33.8\\nJanus 1.3B ✗ 70.9 52.8 34.7 26.9 51.9 27.9 30.0 26.8 33.5\\nHealthGPT-M3 3.8B ✓ 73.7 55.9 74.6 56.4 78.7 39.7 43.3 68.5 61.3\\nHealthGPT-L14 14B ✓ 77.7 58.3 76.4 64.5 85.9 44.4 49.2 74.4 66.4\\nTable 2: The experimental results for the four modality conversion tasks.\\nCT to MRI (Brain) CT to MRI (Pelvis) MRI to CT (Brain) MRI to CT (Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\npix2pix 71.09 32.65 36.85 59.17 31.02 51.91 78.79 33.85 28.33 72.31 32.98 36.19\\nCycleGAN 54.76 32.23 40.56 54.54 30.77 55.00 63.75 31.02 52.78 50.54 29.89 67.78\\nBBDM 71.69 32.91 34.44 57.37 31.37 48.06 86.40 34.12 26.61 79.26 33.15 33.60\\nVmanba 69.54 32.67 36.42 63.01 31.47 46.99 79.63 34.12 26.49 77.45 33.53 31.85\\nDiffMa 71.47 32.74 35.77 62.56 31.43 47.38 79.00 34.13 26.45 78.53 33.68 30.51\\nHealthGPT-M3 79.38 33.03 33.48 71.81 31.83 43.45 85.06 34.40 25.49 84.23 34.29 27.99\\nHealthGPT-L14 79.73 33.10 32.96 71.92 31.87 43.09 85.31 34.29 26.20 84.96 34.14 28.13\\nmodality conversion). Notably, by this stage, the word em-\\nbedding layer and output head have been fine-tuned, only the\\nH-LoRA modules and adapter modules need to be trained.\\nThis strategy significantly improves the model’s adaptability\\nand flexibility across different tasks.\\n5 Experiments\\n5.1 Data and Experimental Setup\\nData Details. We curate VL-Health dataset (see Fig-\\nure 4). For medical visual comprehension, we leverage\\nmultiple medical-specific datasets, including PubMedVi-\\nsion (Chen et al. 2024a), LLaV A-Med (Li et al. 2024b),\\nPathVQA (He et al. 2020), MIMIC-CXR-VQA (Bae et al.\\n2024), SLAKE (Liu et al. 2021), and VQA-RAD (Lau\\net al. 2018). Additionally, we incorporate high-quality open-\\nworld data from LLaV A-1.5 (Liu et al. 2024b) to preserve\\nthe model’s general knowledge and instruction-following\\ncapabilities. For generation tasks, we construct a recon-\\nstruction dataset based on LLaV A-558k (Liu et al. 2024b),\\nand also explore two key tasks in personalized medical\\nimage enhancement—super-resolution and modality con-\\nversion—using the IXI (Davies et al. 2014) and Syn-\\nthRAD2023 (Thummerer et al. 2023) datasets. Detailed data\\nselection and instruction templates are in the Appendix.\\nModel Details. We select CLIP-L/14 (Radford et al. 2021)\\nas the visual encoder and used the hidden states of its\\nsecond and penultimate layers as concrete-grained and\\nabstract-grained features for model’s dynamic hierarchical\\nvisual perception. Drawing on the successful experiences of\\nLLaV A, we employ a MLP to align the multi-modal fea-\\nture embeddings. We choose the parameter-efficient phi-3-\\nmini (Abdin et al. 2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al. 2023), LLaV A-Med (Li et al.\\n2024b), HuatuoGPT-Vision (Chen et al. 2024a)) as well\\nas recent open-world LVLMs (e.g., BLIP-2 (Li et al.\\n2023b), LLaV A-v1.5 (Liu et al. 2024b), InstructBLIP (Dai\\net al. 2023), Yi-VL (Young et al. 2024), InternVL2 (Chen\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_6', embedding=None, metadata={'page_label': '7', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 3: Comparison results of super-resolution task.\\nModel SSIM↑ PSNR↑ MSE↓ LPIPS↓\\nSRGAN 71.34 32.01 41.27 24.50\\nDASR 71.57 32.34 38.25 19.17\\nReal-ESRGAN 67.30 31.87 42.57 20.64\\nLIIF 73.27 32.13 40.14 22.93\\nBSRGAN 69.97 31.97 41.52 28.72\\nHealthGPT-M3 78.19 32.76 34.47 12.02\\nHealthGPT-L14 77.94 32.71 35.19 12.43\\nFigure 5: Performance comparison of LoRA, MoELoRA,\\nand H-LoRA under different rank settings.\\net al. 2024b), Llama-3.2 (Dubey et al. 2024)). Addition-\\nally, we test several SOTA unified visual comprehension\\nand generation models, including Show-o (Xie et al. 2024),\\nUnified-IO 2 (Lu et al. 2024), and Janus (Wu et al.\\n2024). The experimental results are shown in Table 1, with\\nthe following key observations: (i) SOTA Results Com-\\npared with LVLMs: In medical visual comprehension\\ntasks, HealthGPT demonstrates superior performance,\\nsignificantly outperforming both medical-specific models\\n(e.g., HuatuoGPT-Vision) and general-purpose models (e.g.,\\nLlama-3.2). (ii) Surpassing Current Unified LVLMs: De-\\nspite being trained on billions of data points, unified mod-\\nels still exhibit poor generalization performance in medi-\\ncal visual comprehension. For instance, Unified-IO 2 scored\\nonly 33.8. In contrast,HealthGPT-M3, with only 3.8B pa-\\nrameters, scored 61.3 on the medical multi-modal unified\\ntask, significantly outperforming existing unified models in\\nmedical downstream scenarios. (iii) Stable Improvement\\nwith Large Base Model: Our method demonstrates excel-\\nlent scalability, with HealthGPT-L14 achieving a score\\nof 66.4 in the larger model configuration. This result signif-\\nicantly outperforms all other models, highlighting the effec-\\ntiveness of scaling up the base model for enhanced perfor-\\nmance in medical tasks.\\nGeneration. We study three key tasks in medical imag-\\ning. (i) Modality Conversion: In this task, we focus on\\nthe conversion between CT and MRI modalities for the\\nbrain and pelvic regions, designing four specific sub-tasks.\\nAll comparative models (Pix2Pix (Isola et al. 2017), Cy-\\ncleGAN (Zhu et al. 2017), BBDM (Li et al. 2023a),\\n(%)\\n(%)\\n（a） （b）\\nFigure 6: The loss visualization (a) and performance com-\\nparison (b) with respect to different visual perceptions.\\nVmamba (Liu et al. 2024e), and DiffMa (Wang et al.\\n2024b)) trained a separate model for each sub-task, while\\nHealthGPT unify all tasks into a single training process.\\nThe experimental results, shown in Table 11, demonstrate\\nthat our approach outperforms other methods across multi-\\nple evaluation metrics. For instance, in the CT2MRI-Brain\\ntask, HealthGPT-M3 achieves an SSIM of 79.38, signif-\\nicantly surpassing traditional methods like Pix2Pix (71.09)\\nand the recent DiffMa (71.47). (ii) Super-Resolution: We\\nconduct 4× super-resolution experiments on the IXI dataset,\\nwith the results presented in Table 3. Notably, most exist-\\ning methods fail to fully leverage the prior knowledge of key\\nstructures in medical images, resulting in significant short-\\ncomings in detail recovery. In contrast, our method signif-\\nicantly mitigates this issue. Specifically, HealthGPT-M3\\nexcels in key metrics such as SSIM, PSNR, and ISE, achiev-\\ning scores of 78.19, 32.76, and 34.47, respectively. Ad-\\nditionally, HealthGPT-M3 achieves the lowest score of\\n12.34, further validating its exceptional performance in hu-\\nman visual perception. (iii) Reconstruction: We compare\\nHealthGPT-M3 with unified models with reconstruction\\ncapabilities, such as Unified-IO 2 and SEED-X. The results\\nshow that our approach performs better controllability for vi-\\nsual reconstruction. We also trainHealthGPT-L14 with a\\nsimilar number of trainable parameters to the M3 version.\\nHence, the similar performance between the two models\\nmeets our expectations. Details are in the Appendix.\\n5.3 In-Depth Study\\nEffect of Heterogeneous Low-Rank Adaptation.H-LoRA\\nprovides an optimized multi-LoRA architecture for multi-\\ntask learning. We conduct extensive validation of this struc-\\nture, with results presented in Table 4, comparing the per-\\nformance of LoRA, MoELoRA, and H-LoRA in medical\\nunified comprehension and generation tasks. In the majority\\nof comprehension tasks and all generation tasks, H-LoRA\\ndemonstrates superior performance, particularly in the Om-\\nniMedVQA benchmark, where it improved from 64.90 to\\n68.50. Notably, despite some applications of MoELoRA in\\ncertain scenarios, it do not show advantages in this task and\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_7', embedding=None, metadata={'page_label': '8', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 4: We present the performance and speed differences of LoRA, MoELoRA (n=4), and H-LoRA (n=4) on medical visual\\ncomprehension and generation tasks.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQAModel\\nclose all close all close all\\nMMMU\\n-Med OMVQA RECOM MTRANS SR\\nTraining\\nTime\\nHealthGPT w/\\n+LoRA 71.3 57.2 70.0 53.4 76.4 38.6 41.30 65.10 62.67 59.99 65.88 1.00×\\n+MoELoRA 72.5 57.2 66.4 52.4 73.2 36.0 39.30 64.90 67.31 59.76 65.91 1.49×\\n+H-LoRA 73.7 55.9 74.6 56.4 78.7 39.7 43.30 68.50 67.69 60.30 66.14 1.00×\\nTable 5: Comparison between the H-LoRA-based Three-Stage Learning Strategy and the mixed-training approach.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQA CT MRITraining Strategy\\nclose all close all close all\\nMMMU\\n-Med OMVQA Brain Pelvis Brain Pelvis\\nMixed-Training 56.6 37.9 45.0 32.9 65.7 33.6 44.0 48.9 65.64 62.75 56.61 50.77HealthGPT w/ 3-stage-Training 72.5 55.2 77.9 59.6 79.7 49.0 42.7 68.5 70.84 72.99 65.26 61.33\\nFigure 7: Case study of report-to-CXR under different instructions. (a) shows a normal CXR image for comparison. (b) and (c)\\nillustrate generated cases with varying severity and affected regions. The graffiti areas indicate abnormal conditions.\\nhad a training time approximately 50% longer than LoRA.\\nFigure 5 illustrates the performance of the three PEFT meth-\\nods in medical visual comprehension and generation tasks\\nacross different ranks, with H-LoRA consistently outper-\\nforming the other methods in all scenarios, demonstrating\\nsignificant advantages in handling diverse tasks.\\nDifferent Learning Strategy. We propose a three-stage\\nlearning strategy for H-LoRA that decouples comprehension\\nand generation tasks. Unlike methods that train both tasks\\nsimultaneously, our approach reduces performance degra-\\ndation from task conflicts (see Table 5). In the medical vi-\\nsual comprehension task, mixed training causes catastrophic\\nforgetting and degrades visual reconstruction, whereas our\\nstrategy effectively uses the medical embedding knowledge\\nin pre-trained LLMs to mitigate these conflicts. Meanwhile,\\nwe examine how fusing heterogeneous H-LoRA plugins in\\nthe second training stage results in minimal performance\\ndegradation. Detailed results are in the Appendix.\\nHierarchical Visual Perception Analysis. We conduct an\\nablation analysis on visual perceptual inputs for comprehen-\\nsion and generation tasks. Figure 6 shows that comprehen-\\nsion tasks converge more efficiently with abstract-grained\\ninputs, while generation tasks perform better with concrete-\\ngrained inputs. This highlights the importance of the hier-\\narchical visual perception we propose, suggesting that tai-\\nloring visual inputs for specific tasks at different hierarchies\\ncan significantly improve efficiency.\\nReport-to-CXR Task. We further explore the medical im-\\nage generation task without reference images, using a small\\namount of MIMIC-CXR data (Johnson et al. 2019) for in-\\nstruction fine-tuning. Figure 7 annotates images with vary-\\ning injury degrees and locations, comparing them to healthy\\nCXR images. We observe thatHealthGPT effectively gen-\\nerates CXR images based on the instructions, showcasing its\\npotential in healthcare education and auxiliary diagnosis.\\n6 Conclusion\\nIn this paper, we introduceHealthGPT, a Med-LVLM that\\nunifies medical vision-language comprehension and gen-\\neration through a novel heterogeneous knowledge adap-\\ntation approach. Experimental results demonstrate that\\nHealthGPT achieves significant performance improve-\\nments across multiple medical comprehension and genera-\\ntion tasks, showcasing its potential for healthcare applica-\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_8', embedding=None, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tions.\\nReferences\\nAbdin, M.; Aneja, J.; Behl, H.; Bubeck, S.; Eldan, R.; Gu-\\nnasekar, S.; Harrison, M.; Hewett, R. J.; Javaheripi, M.;\\nKauffmann, P.; et al. 2024. Phi-4 technical report. arXiv\\npreprint arXiv:2412.08905.\\nBae, S.; Kyung, D.; Ryu, J.; Cho, E.; Lee, G.; Kweon, S.;\\nOh, J.; JI, L.; Chang, E.; Kim, T.; et al. 2024. MIMIC-Ext-\\nMIMIC-CXR-VQA: A Complex, Diverse, And Large-Scale\\nVisual Question Answering Dataset for Chest X-ray Images.\\nChen, J.; Gui, C.; Ouyang, R.; Gao, A.; Chen, S.; Chen,\\nG. H.; Wang, X.; Zhang, R.; Cai, Z.; Ji, K.; et al.\\n2024a. Huatuogpt-vision, towards injecting medical visual\\nknowledge into multimodal llms at scale. arXiv preprint\\narXiv:2406.19280.\\nChen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong,\\nW.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024b. How far are we to\\ngpt-4v? closing the gap to commercial multimodal models\\nwith open-source suites. arXiv preprint arXiv:2404.16821.\\nChern, E.; Su, J.; Ma, Y .; and Liu, P. 2024. Anole:\\nAn open, autoregressive, native large multimodal mod-\\nels for interleaved image-text generation. arXiv preprint\\narXiv:2407.06135.\\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\\nGeneral-purpose Vision-Language Models with Instruction\\nTuning. arXiv:2305.06500.\\nDavies, R. L.; Royston, P. A.; Leung, M. S.; Haider, M. E.\\nA. M. J.; Barkhof, S. G. A. L.; and B., P. E. T. M. 2014. The\\nIXI Dataset. Accessed: 2025-01-30.\\nDing, N.; Qin, Y .; Yang, G.; Wei, F.; Yang, Z.; Su, Y .;\\nHu, S.; Chen, Y .; Chan, C.-M.; Chen, W.; et al. 2023.\\nParameter-efficient fine-tuning of large-scale pre-trained\\nlanguage models. Nature Machine Intelligence, 5(3): 220–\\n235.\\nDong, R.; Han, C.; Peng, Y .; Qi, Z.; Ge, Z.; Yang, J.; Zhao,\\nL.; Sun, J.; Zhou, H.; Wei, H.; et al. 2023. Dreamllm:\\nSynergistic multimodal comprehension and creation. arXiv\\npreprint arXiv:2309.11499.\\nDubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.;\\nLetman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;\\net al. 2024. The llama 3 herd of models. arXiv preprint\\narXiv:2407.21783.\\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming trans-\\nformers for high-resolution image synthesis. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern\\nrecognition, 12873–12883.\\nGe, Y .; Ge, Y .; Zeng, Z.; Wang, X.; and Shan, Y . 2023. Plant-\\ning a seed of vision in large language model. arXiv preprint\\narXiv:2307.08041.\\nGe, Y .; Zhao, S.; Zhu, J.; Ge, Y .; Yi, K.; Song, L.; Li, C.;\\nDing, X.; and Shan, Y . 2024. Seed-x: Multimodal models\\nwith unified multi-granularity comprehension and genera-\\ntion. arXiv preprint arXiv:2404.14396.\\nHe, X.; Zhang, Y .; Mou, L.; Xing, E.; and Xie, P. 2020.\\nPathvqa: 30000+ questions for medical visual question an-\\nswering. arXiv preprint arXiv:2003.10286.\\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\\nof large language models. arXiv preprint arXiv:2106.09685.\\nHu, Y .; Li, T.; Lu, Q.; Shao, W.; He, J.; Qiao, Y .; and Luo,\\nP. 2024. Omnimedvqa: A new large-scale comprehensive\\nevaluation benchmark for medical lvlm. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 22170–22183.\\nIsola, P.; Zhu, J.-Y .; Zhou, T.; and Efros, A. A. 2017. Image-\\nto-image translation with conditional adversarial networks.\\nIn Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 1125–1134.\\nJohnson, A. E.; Pollard, T. J.; Greenbaum, N. R.; Lungren,\\nM. P.; Deng, C.-y.; Peng, Y .; Lu, Z.; Mark, R. G.; Berkowitz,\\nS. J.; and Horng, S. 2019. MIMIC-CXR-JPG, a large pub-\\nlicly available database of labeled chest radiographs. arXiv\\npreprint arXiv:1901.07042.\\nLau, J. J.; Gayen, S.; Ben Abacha, A.; and Demner-\\nFushman, D. 2018. A dataset of clinically generated visual\\nquestions and answers about radiology images. Scientific\\ndata, 5(1): 1–10.\\nLi, B.; Xue, K.; Liu, B.; and Lai, Y .-K. 2023a. Bbdm: Image-\\nto-image translation with brownian bridge diffusion models.\\nIn Proceedings of the IEEE/CVF conference on computer\\nvision and pattern Recognition, 1952–1961.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024a. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024b. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. Blip-2:\\nBootstrapping language-image pre-training with frozen im-\\nage encoders and large language models. In International\\nconference on machine learning, 19730–19742. PMLR.\\nLin, T.; Liu, J.; Zhang, W.; Li, Z.; Dai, Y .; Li, H.; Yu, Z.; He,\\nW.; Li, J.; Jiang, H.; et al. 2024. Teamlora: Boosting low-\\nrank adaptation with expert collaboration and competition.\\narXiv preprint arXiv:2408.09856.\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_9', embedding=None, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Liu, B.; Zhan, L.-M.; Xu, L.; Ma, L.; Yang, Y .; and Wu,\\nX.-M. 2021. Slake: A semantically-labeled knowledge-\\nenhanced dataset for medical visual question answering. In\\n2021 IEEE 18th International Symposium on Biomedical\\nImaging (ISBI), 1650–1654. IEEE.\\nLiu, D.; Zhao, S.; Zhuo, L.; Lin, W.; Qiao, Y .; Li, H.; and\\nGao, P. 2024a. Lumina-mgpt: Illuminate flexible photore-\\nalistic text-to-image generation with multimodal generative\\npretraining. arXiv preprint arXiv:2408.02657.\\nLiu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2024b. Improved\\nbaselines with visual instruction tuning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 26296–26306.\\nLiu, H.; Li, C.; Li, Y .; Li, B.; Zhang, Y .; Shen, S.; and Lee,\\nY . J. 2024c. LLaV A-NeXT: Improved reasoning, OCR, and\\nworld knowledge. https://llava-vl.github.io/blog/2024-01-\\n30-llava-next/.\\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruc-\\ntion Tuning. In NeurIPS.\\nLiu, Q.; Wu, X.; Zhao, X.; Zhu, Y .; Xu, D.; Tian, F.; and\\nZheng, Y . 2024d. When moe meets llms: Parameter efficient\\nfine-tuning for multi-task medical applications. In Proceed-\\nings of the 47th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval, 1104–\\n1114.\\nLiu, Y .; Tian, Y .; Zhao, Y .; Yu, H.; Xie, L.; Wang, Y .; Ye,\\nQ.; and Liu, Y . 2024e. VMamba: Visual State Space Model.\\narXiv preprint arXiv:2401.10166.\\nLu, J.; Clark, C.; Lee, S.; Zhang, Z.; Khosla, S.; Marten, R.;\\nHoiem, D.; and Kembhavi, A. 2024. Unified-IO 2: Scaling\\nAutoregressive Multimodal Models with Vision Language\\nAudio and Action. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , 26439–\\n26455.\\nLu, J.; Clark, C.; Zellers, R.; Mottaghi, R.; and Kembhavi,\\nA. 2022. Unified-io: A unified model for vision, language,\\nand multi-modal tasks. In The Eleventh International Con-\\nference on Learning Representations.\\nLuo, T.; Lei, J.; Lei, F.; Liu, W.; He, S.; Zhao, J.; and Liu,\\nK. 2024a. Moelora: Contrastive learning guided mixture of\\nexperts on parameter-efficient fine-tuning for large language\\nmodels. arXiv preprint arXiv:2402.12851.\\nLuo, Y .; Zhang, J.; Fan, S.; Yang, K.; Hong, M.; Wu, Y .;\\nQiao, M.; and Nie, Z. 2024b. Biomedgpt: An open multi-\\nmodal large language model for biomedicine. IEEE Journal\\nof Biomedical and Health Informatics.\\nMasoudnia, S.; and Ebrahimpour, R. 2014. Mixture of ex-\\nperts: a literature survey. Artificial Intelligence Review, 42:\\n275–293.\\nMoor, M.; Huang, Q.; Wu, S.; Yasunaga, M.; Dalmia, Y .;\\nLeskovec, J.; Zakka, C.; Reis, E. P.; and Rajpurkar, P. 2023.\\nMed-flamingo: a multimodal medical few-shot learner. In\\nMachine Learning for Health (ML4H), 353–367. PMLR.\\nNath, V .; Li, W.; Yang, D.; Myronenko, A.; Zheng, M.; Lu,\\nY .; Liu, Z.; Yin, H.; Law, Y . M.; Tang, Y .; et al. 2024. Vila-\\nm3: Enhancing vision-language models with medical expert\\nknowledge. arXiv preprint arXiv:2411.12915.\\nOpenAI. 2023. GPT-4V(ision) System Card. https://cdn.\\nopenai.com/papers/GPTV System Card.pdf.\\nPan, K.; Tang, S.; Li, J.; Fan, Z.; Chow, W.; Yan, S.;\\nChua, T.-S.; Zhuang, Y .; and Zhang, H. 2024. Auto-\\nEncoding Morph-Tokens for Multimodal LLM. arXiv\\npreprint arXiv:2405.01926.\\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\\net al. 2021. Learning transferable visual models from nat-\\nural language supervision. In International conference on\\nmachine learning, 8748–8763. PMLR.\\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\\net al. 2023. Large language models encode clinical knowl-\\nedge. Nature, 620(7972): 172–180.\\nTeam, C. 2024. Chameleon: Mixed-modal early-fusion\\nfoundation models. arXiv preprint arXiv:2405.09818.\\nThawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal,\\nH.; Anwer, R. M.; Khan, S.; Laaksonen, J.; and Khan,\\nF. S. 2023. Xraygpt: Chest radiographs summarization\\nusing medical vision-language models. arXiv preprint\\narXiv:2306.07971.\\nThummerer, A.; van der Bijl, E.; Galapon Jr, A.; Verhoeff,\\nJ. J.; Langendijk, J. A.; Both, S.; van den Berg, C. N. A.;\\nand Maspero, M. 2023. SynthRAD2023 Grand Challenge\\ndataset: Generating synthetic CT for radiotherapy. Medical\\nphysics, 50(7): 4664–4674.\\nTian, D.; Jiang, S.; Zhang, L.; Lu, X.; and Xu, Y . 2023. The\\nrole of large language models in medical image processing:\\na narrative review. Quantitative Imaging in Medicine and\\nSurgery, 14(1): 1108.\\nTong, S.; Fan, D.; Zhu, J.; Xiong, Y .; Chen, X.; Sinha, K.;\\nRabbat, M.; LeCun, Y .; Xie, S.; and Liu, Z. 2024. Meta-\\nMorph: Multimodal Understanding and Generation via In-\\nstruction Tuning. arXiv preprint arXiv:2412.14164.\\nTu, T.; Azizi, S.; Driess, D.; Schaekermann, M.; Amin, M.;\\nChang, P.-C.; Carroll, A.; Lau, C.; Tanno, R.; Ktena, I.; et al.\\n2024. Towards generalist biomedical AI. NEJM AI, 1(3):\\nAIoa2300138.\\nVig, J. 2019. A multiscale visualization of attention in the\\ntransformer model. arXiv preprint arXiv:1906.05714.\\nWang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y .; Wang, J.;\\nZhang, F.; Wang, Y .; Li, Z.; Yu, Q.; et al. 2024a. Emu3:\\nNext-token prediction is all you need. arXiv preprint\\narXiv:2409.18869.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_10', embedding=None, metadata={'page_label': '11', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Wang, Z.; Wu, Z.; Agarwal, D.; and Sun, J. 2022. Medclip:\\nContrastive learning from unpaired medical images and text.\\narXiv preprint arXiv:2210.10163.\\nWang, Z.; Zhang, L.; Wang, L.; and Zhang, Z. 2024b. Soft\\nMasked Mamba Diffusion Model for CT to MRI Conver-\\nsion. arXiv preprint arXiv:2406.15910.\\nWu, C.; Chen, X.; Wu, Z.; Ma, Y .; Liu, X.; Pan, Z.; Liu, W.;\\nXie, Z.; Yu, X.; Ruan, C.; and Luo, P. 2024. Janus: Decou-\\npling Visual Encoding for Unified Multimodal Understand-\\ning and Generation. arXiv:2410.13848.\\nWu, S.; Fei, H.; Qu, L.; Ji, W.; and Chua, T.-S. 2023.\\nNext-gpt: Any-to-any multimodal llm. arXiv preprint\\narXiv:2309.05519.\\nXie, J.; Mao, W.; Bai, Z.; Zhang, D. J.; Wang, W.; Lin, K. Q.;\\nGu, Y .; Chen, Z.; Yang, Z.; and Shou, M. Z. 2024. Show-o:\\nOne single transformer to unify multimodal understanding\\nand generation. arXiv preprint arXiv:2408.12528.\\nYoung, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.; Zhang,\\nG.; Li, H.; Zhu, J.; Chen, J.; Chang, J.; et al. 2024.\\nYi: Open foundation models by 01. ai. arXiv preprint\\narXiv:2403.04652.\\nZhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li,\\nY .; Chen, S. S.; Zhou, P.; Liu, J.; et al. 2023. A survey of\\nlarge language models in medicine: Progress, application,\\nand challenge. arXiv preprint arXiv:2311.05112.\\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\\npaired image-to-image translation using cycle-consistent ad-\\nversarial networks. InProceedings of the IEEE international\\nconference on computer vision, 2223–2232.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_11', embedding=None, metadata={'page_label': '12', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix\\nThis is the Appendix for “HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation\\nvia Heterogeneous Knowledge Adaptation”. This Appendix is organized as follows:\\n• Section A presents the experimental implementation details, the training process of HealthGPT, and the specifics of\\nVL-Health.\\n• Section B systematically provides an analysis of Heterogeneous Low-Rank Adaptation.\\n• Section C shows supplementary experimental results to validate the effectiveness ofHealthGPT.\\nA Implementation Details\\nA.1 Model Details\\nWe employ CLIP-L/14 (Radford et al. 2021) as the visual feature extractor, extracting both shallow and deep features to serve as\\nvisual tokens. The model uses alignment adapters, implemented with two-layer MLPs, to align shallow features, representing\\nconcrete visual granularity, and deep features, representing abstract visual granularity. These visual tokens are concatenated\\nwith text tokens and input into the large language models (LLMs).\\nHealthGPT offers two versions: HealthGPT-M3 and HealthGPT-L14, which are based on Phi-3-mini (Abdin et al.\\n2024) and Phi-4 (Abdin et al. 2024) as the pre-trained LLMs, respectively. In addition, we expand the LLM vocabulary with\\n8192 VQ indices derived from VQGAN-f8-8192 (Esser, Rombach, and Ommer 2021), serving as multi-modal tokens to further\\naugment the model’s capacity for understanding both visual and textual input. Figure 6 shows the details.\\nTable 6: Overview of the Components of HealthGPT.\\nModel ViT Adapter MLP-dims Model dims LLM Params Vocab Size H-LoRA Rank\\nHealthGPT-M3 CLIP-L/14 2-layer MLP 1024 3072 Phi-3-mini 3.8B 40206 16(Comp.), 64(Gen.)\\nHealthGPT-L14 CLIP-L/14 2-layer MLP 1024 5120 Phi-4 14B 108547 8(Comp.), 32(Gen.)\\nA.2 Training Details\\nIn this study, we propose a three-stage learning strategy that is compatible with our innovative heterogeneous low-rank adapta-\\ntion (H-LoRA). We provide a detailed hyperparameter configuration for the model’s three-stage training process. The specific\\nhyperparameter settings used are listed in Table 7. These hyperparameters are crucial for ensuring the model’s learning efficacy\\nand final performance.\\nTable 7: Overview of Hyperparameter Configurations.\\nHealthGPT-M3 HealthGPT-L14\\nStage-1 Stage-2 Stage-3 Stage-1 Stage-2 Stage-3Hyperparameter\\nComp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen.\\nOptimizer AdamW AdamW AdamW AdamW AdamW AdamW\\nAdapter LR 1e-3 2e-5 2e-5 2e-5 1e-3 2e-5 2e-5 2e-5\\nLearning Rate / 2e-4 2e-4 2e-4 / 1e-4 2e-4 2e-4\\nGlobal Batch Size 256 64 32 128 64 256 64 32 128 64\\nWeight Decay 0 0 0 0 0 0\\nDropout Rate 0 0.05 0.05 0.05 0 0.05 0.05 0.05\\nLR Scheduler Warm Up Constant Warm Up Warm Up Constant Warm Up\\nMax Sequence Length 2048 2048 2048 2048 2048 2048\\nIt is worth noting that we sometimes observe instances of loss spikes during the training of medical visual comprehension\\nand generation tasks. Through repeated validation, we discovered that larger model parameters and learning rates tend to lead to\\nthis issue, which is the reason for the slight differences in hyperparameters betweenHealthGPT-M3 and HealthGPT-L14.\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_12', embedding=None, metadata={'page_label': '13', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='（a） （b）\\nFigure 8: VL-Health dataset collection distribution.\\nA.3 VL-Health\\nThe construction of theVL-Health dataset involves two key steps:(i) data collection, (ii) data processing, as detailed below:\\nData Collection: During the collection phase, we carefully considered the diversity of medical images and the complexity of\\nthe tasks, selecting appropriate subsets for comprehension and generation tasks. For comprehension tasks, we selected datasets\\nsuch as VQA-RAD (Lau et al. 2018), SLAKE (Liu et al. 2021), PathVQA (He et al. 2020), and MIMIC-CXR-VQA (Bae et al.\\n2024), which cover various medical imaging modalities like radiology and pathology, and include professional annotations\\nto assist the model in learning tasks such as lesion detection and disease diagnosis. Additionally, large-scale multi-modal\\ndatasets like LLaV A-Med (Li et al. 2024b) and PubMedVision (Chen et al. 2024a) were included to provide broader medical\\nknowledge support and facilitate the training of complex reasoning tasks. For generation tasks, we focused on four mainstream\\ntask categories: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction.\\nThe IXI (Davies et al. 2014) dataset, containing a large number of healthy brain MRI images, is suitable for training super-\\nresolution models; the MIMIC-CHEST-XRAY (Bae et al. 2024) dataset, with X-ray images and their corresponding textual\\nreports, is appropriate for text-to-image generation tasks; the SynthRAD2023 (Thummerer et al. 2023) dataset provides a large\\nnumber of paired CT and MRI images, supporting modality conversion model training; for image reconstruction tasks, we\\nrewrote and adjusted the LLaV A-558k (Liu et al. 2024b) dataset.\\nData Processing: After data collection, we performed filtering and processing of the raw data. For VisualQA tasks, we stan-\\ndardized the data entries into two forms: open-ended questions and single-choice questions, enabling flexible training and\\nevaluation. Additionally, considering that multi-image data has a minimal impact on performance but introduces extra padding\\nand training time, we excluded multi-image data. For the scanned image data in generation tasks, we applied slicing extrac-\\ntion, image registration, data augmentation, and normalization to treat 2D images as visual inputs for model training or used\\nVQGAN-generated indices to supervise the generation tasks.\\nData Statistics This section provides detailed statistical information about the VL-Health dataset to offer a more compre-\\nhensive understanding.\\nData Overview: To ensure a balanced development of the model’s comprehension and generation capabilities, in addition\\nto the LLaV A-558k and PubMedVision-PT datasets used for alignment, the VL-Health dataset ultimately selected 765,802\\nadditional visual question-answering (VQA) training samples (to endow the model with visual comprehension and instruction-\\nfollowing capabilities) and 783,045 generation training samples (to provide the model with reconstruction and visual generation\\ninstruction-following abilities). This contributes to the transfer of knowledge between comprehension and generation tasks, en-\\nhancing the model’s overall performance. For medical image comprehension tasks, images were selected from VQA-RAD (ap-\\nproximately 450 images), SLAKE (approximately 630 images), PathVQA (approximately 2,600 images), MIMIC-CXR-VQA\\n(approximately 52,000 images), LLaV A-Med (approximately 61,000 images), and PubMedVision (approximately 500,000 im-\\nages). Multiple question-answer pairs were retained for each image to enhance the model’s understanding and generalization\\nof the image content. Table 8 shows the data distribution ofVL-Health for three-stage learning strategy, where mixed-47k is\\nbased on the sampling of all data in stage-1.\\nDiversity and Quality Assessment: VL-Health covers 11 modalities, including CT, MRI, X-ray, microscopy, OCT, ultra-\\nsound, and fundus photography, which aids the model in learning features from various modalities. The dataset also encom-\\npasses a wide range of diseases, from common to rare, and from localized lesions to systemic diseases, including pulmonary\\ndiseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides com-\\nprehensive training support to the model, enabling it to learn the characteristics and diagnosis of various diseases.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_13', embedding=None, metadata={'page_label': '14', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8: Data distribution of VL-Health in three-stage learning strategy.\\nMedical Task Stage-1 Stage-2\\nComp. LLaV A-558k, PubMedVision-PT Mixed-47kGen. LLaV A-558k\\nMedical Task Stage-3\\nComp. LLaV A Med, MIMIC CXR VQA, PubMedVision-FT, LLaV A-665k, PathVQA, SLAKE, VQA-RAD\\nGen. IXI, SynthRAD2023, MIMIC-CHEST-XRAY\\nData Format. All data samples are converted into a unified instruction-response format for training and evaluation. Specifi-\\ncally, the VL-Health dataset consists of the following components:\\n• Task Type: Specifies the granularity of visual features output by the visual encoder and selects the corresponding H-\\nLoRA submodule. For generation tasks, the response also includes multi-modal tokens corresponding to VQ indices.\\n• Task Instruction: Guides the model to interpret the image and generate a response, covering various aspects of the\\nimage and specifying the output format.\\n• Response: The textual output generated based on the task instruction and input image, ensuring it meets the question and\\nformatting requirements.\\n• Input Image: Provides the visual signal for the model to process.\\n• Target Image Index: In generation tasks, this is added as a multi-modal token to the response for autoregressive\\ngeneration.\\nB Analysis of Heterogeneous Low-Rank Adaptation\\nWe propose H-LoRA, which utilizes hard routing selection to allocate plugins for knowledge learning and representation across\\ntasks, thereby preventing conflicts arising from heterogeneous knowledge. Furthermore, within each task, we optimized based\\non MoELoRA, enhancing performance while reducing computational overhead. The pseudocode is detailed Algorithm 1.\\nAlgorithm 1: H-LoRA Algorithm\\nInput: concrete-grained visual features FCon, abstract-grained visual featuresFAbs, comprehension-based H-LoRA modules\\n({AComp.\\ni }k\\ni=1, RComp.\\nouter ), generation-based H-LoRA modules({AGen.\\ni }k\\ni=1, RGen.\\nouter), task type T (comprehension or generation),\\nnumber of LoRA experts k, origin linear layer weights W0, text features T , hidden state h\\nOutput: final output O\\n// Select task-specific image features\\nif T = generation task then\\nFimg ← FCon\\nelse if T = comprehension task then\\nFimg ← FAbs\\nend if\\nU ←concat(Fimg, T ) // Concatenate image features and text features\\n{Ai}k\\ni=1, {Bi}k\\ni=1, Router ← {AT\\ni }k\\ni=1, {BT\\ni }k\\ni=1, RT\\nouter // Assign task-specific H-LoRA submodule\\n// Merge LoRA experts’ matrices\\nAmerged ← concat({Ai}k\\ni=1)\\nBmerged ← concat({Bi}k\\ni=1)\\nW ←R(h) // Generate routing weights based on input hidden state x\\nWexpanded ← α × W/r ⊗ 1r // Expand routing weights to match merged matrices\\nOH-LoRA ← (x · Amerged ⊙ Wexpanded) · Bmerged // Compute H-LoRA output using element-wise multiplication\\nO ←x · W0 + OH-LoRA // Add H-LoRA output to pre-trained weights to get final output\\nReturn O\\nWe further analyzed the computational overhead differences between MoELoRA and H-LoRA. Assuming that both methods\\nuse the same number of LoRA experts k, we can compare their time complexity from the perspective of the operational steps\\ninvolved.\\nComputational Overhead of MoELoRA. In MoELoRA, the operations involving the expert matrix mainly include the fol-\\nlowing steps: (i) Expert Multiplication : MoELoRA requires 2k multiplications with the LoRA experts. (ii) Router Multi-\\nplication: One multiplication with the Router is required. (iii) Router Output Expansion : MoELoRA needs to perform k\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_14', embedding=None, metadata={'page_label': '15', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='expansion operations on the Router’s output weights to generate the appropriate shapes that match the dimensions of the in-\\nput and LoRA experts while iterating through the experts. (iv) Dot Product: For each expanded Router weight, a dot product\\nwith the intermediate state of the expert is required, resulting in k multiplications. (v) Addition: Finally, k addition operations\\nare required to accumulate the results from each LoRA expert into the final output. Assuming the time complexity of each\\noperation is the same, the additional time complexity introduced when equipping a fully connected layer with MoELoRA is:\\nO(2k+1+ k+k+k) = O(5k+1). Thus, MoELoRA introduces an additional time overhead ofO(5k+1) during computation.\\nH-LoRA. In contrast to MoELoRA, H-LoRA reduces the computational overhead by concatenating the LoRA expert matrices.\\nSpecifically: (i) Expert Multiplication: H-LoRA merges all LoRA experts by directly creating a larger A and B matrix, instead\\nof performing independent operations for each expert. This process can be implemented through matrix initialization without\\nadditional concatenation operations. Therefore, only 2 multiplications with the LoRA experts are required. (ii) Router Multi-\\nplication: H-LoRA still requires one multiplication with the Router. (iii) Router Output Expansion : H-LoRA only requires\\none expansion operation on the Router’s output weights. (iv) Dot Product: H-LoRA only requires one dot product between\\nthe Router’s output and the expert’s intermediate state. (v) Addition: Finally, H-LoRA only requires one addition operation\\nto accumulate the LoRA expert results into the intermediate state. Therefore, the additional time complexity introduced by\\nH-LoRA is: O(2 + 1 + 1 + 1 + 1) =O(6).\\nComparing the two, we see that MoELoRA introduces a linear increase in additional time complexity with respect to the\\nnumber of experts k, resulting in a complexity of O(5k + 1), while H-LoRA’s additional time complexity is fixed at O(6),\\nindependent of k. We observe that when k is small, the time complexity differences between MoELoRA and H-LoRA are\\nnegligible. However, as k increases, MoELoRA’s computational overhead grows linearly, while H-LoRA’s remains constant.\\nThis makes H-LoRA significantly more computationally efficient than MoELoRA, particularly in large-scale tasks. We will\\nfurther demonstrate the significant advantage of H-LoRA in training time in subsequent experiments, validating its efficiency\\nin practical applications.\\nC Supplemental Experimental Results\\nIn this section, we include additional experiments to demonstrate the superiority of HealthGPT and articulate our design\\nphilosophy.\\nC.1 Results: OmniMedVQA Benchmark\\nOmniMedVQA (Hu et al. 2024) is a novel, large-scale medical visual question answering (VQA) benchmark designed to\\nencompass various modalities and anatomical regions by collecting diverse images from multiple medical datasets. Our exper-\\nimental results are presented in Table 9.\\nTable 9: Performance comparison of OmniMedVQA Benchmark.\\nOmniMedVQA↑Type Model # Params Medical\\nLVLM CT X-ray FDM MiS OCT MRI USS Avg.\\nComp. Only\\nMed-Flamingo 8.3B ✓ 30.1 33.9 25.5 37.0 60.0 27.6 30.4 34.9\\nLLaV A-Med 7B ✓ 28.4 32.8 42.7 31.6 55.3 45.0 53.6 41.3\\nHuatuoGPT-Vision 7B ✓ 35.3 41.5 51.4 62.3 59.3 40.4 60.1 50.0\\nBLIP-2 6.7B ✗ 26.6 29.1 22.3 36.9 29.1 22.7 21.4 26.9\\nLLaV A-v1.5 7B ✗ 28.0 55.7 35.5 42.1 49.2 52.9 49.7 44.7\\nInstructBLIP 7B ✗ 20.1 22.2 34.1 30.6 38.6 31.9 25.5 29.0\\nYi-VL 6B ✗ 51.2 47.1 27.7 62.6 67.6 55.0 40.3 50.2\\nInternVL2 8B ✗ 40.2 57.9 53.2 64.0 59.1 58.1 49.1 54.5\\nLlama-3.2 11B ✗ 37.6 55.2 71.4 82.1 62.5 65.2 68.6 63.2\\nComp. & Gen.\\nShow-o 1.3B ✗ 29.0 50.4 30.9 22.0 30.8 34.2 33.8 33.0\\nUnified-IO 2 7B ✗ 10.8 37.7 12.3 25.3 32.6 30.9 37.7 26.8\\nJanus 1.3B ✗ 24.9 54.8 35.9 62.7 54.2 50.7 36.8 45.7\\nHealthGPT-M3 3.8B ✓ 35.3 81.9 54.6 88.2 89.3 78.5 51.4 68.5\\nHealthGPT-L14 14B ✓ 39.0 86.6 64.1 88.6 99.7 80.9 62.2 74.4\\nThrough our analysis, we make the following observations: (i) HealthGPT-M3 outperforms other models in 4 out of 7\\nsub-tasks, achieving an average score that exceeds cutting-edge medical Large Vision-Language Models (LVLMs) as well as\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_15', embedding=None, metadata={'page_label': '16', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='general LVLMs; (ii) the unified model demonstrates relatively weak performance on OmniMedVQA; however, our approach\\neffectively mitigates performance degradation caused by generation tasks, serving as a unified model; (iii) HealthGPT-L14\\nexcels across all sub-tasks, achieving optimal or near-optimal results with an average score of 74.4, significantly surpassing\\nother models.\\nC.2 Stability Analysis of Number of Experts\\nWe investigated the impact of the number of LoRA experts on model performance within a multi-LoRA architecture, conducting\\nextensive experiments on MoELoRA and H-LoRA with varying numbers of experts. The experimental results are presented\\nin Table 10. As the number of experts increases, the training time for MoELoRA is significantly prolonged. When n = 8 ,\\nthe training time for MoELoRA is twice that of LoRA, whereas H-LoRA incurs no additional training delay and performs\\nbetter. It is estimated that at n = 32, the training time for MoELoRA could reach eight times that of LoRA, preventing it from\\ncompleting training and inference. This result aligns with the analysis in Appendix B, indicating that H-LoRA not only avoids\\nintroducing additional training delays compared to LoRA but also outperforms MoELoRA.\\nTable 10: We explored the performance of MoELoRA and H-LoRA with different numbers of LoRA experts. At n = 32 ,\\nMoELoRA was unable to complete training.\\nn=2 n=4 n=8 n=32Model Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time\\n+MoELoRA 50.3 62.98 1.22 × 50.0 64.33 1.49 × 50.8 63.71 2.09 × / / 5.81 ×HealthGPT w/ +H-LoRA 51.5 63.48 0.99× 52.8 64.71 1.00× 53.6 64.98 0.99× 53.5 64.74 1.01×\\nC.3 Impact of Heterogeneous Knowledge Fusion on Performance\\nTraditional unified models often utilize mixed training methods, which may result in performance degradation due to variations\\nin task modes. To address this, we propose a three-phase learning strategy to support H-LoRA, effectively mitigating inter-task\\nconflicts. Specifically, the second phase (Heterogeneous H-LoRA Plugin Adaptation) integrates LLMs with different H-LoRA\\nplugins into a new unified foundation by mixing the training of the embedding layers and output heads for two tasks. Figure\\n9 illustrates the impact of this phase on the performance of medical comprehension and generation tasks. We observe that the\\nsecond phase effectively unifies the model with minimal impact on overall performance, significantly alleviating the conflict\\nissues arising from mixed training in medical scenarios.\\nC.4 Human Evaluation.\\n65.7 65.4 67.7 67.0\\nFigure 9: Performance changes before and after the\\nstage-2.\\nWe further conduct human evaluation on the VQA-RAD, SLAKE,\\nand PathVQA benchmarks, which contain 1,000 open-ended ques-\\ntions. Specifically, we recruit 5 clinicians to rank the randomly shuf-\\nfled responses from HealthGPT-L14, LLaV A-Med, HuatuoGPT-\\nVision, Llama-3.2, InternVL-2 and Show-o. During the evaluation,\\nquestions were randomly selected, and the model-generated responses\\nwere anonymized and ranked. The results, as shown in Figure 10, in-\\ndicate that HealthGPT was frequently selected as the best answer.\\nThis suggests that HealthGPT has further application potential in\\nmedical care scenarios.\\nC.5 Reconstruction Performance\\nCurrently, unified models that align visual features based on recon-\\nstruction tasks include pre-LVLMs, post-LVLMs, as well as Unified-\\nIO 2 (Lu et al. 2024) and SEED-X (Ge et al. 2024). To investigate the\\ncontrollability of visual generation in rigorous settings such as med-\\nical contexts, we evaluated the performance of these models in med-\\nical image reconstruction in Table 11. Experimental results demon-\\nstrate that HealthGPT exhibits the most stable reconstruction per-\\nformance with a small amount of data.\\n16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_16', embedding=None, metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='C.6 Case Study\\nFigures 11 and 12 illustrate examples of modality transformation and super-resolution reconstruction. In Figure 11, the results\\ngenerated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively\\nguiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure 12\\ndemonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the\\nimage.\\nTable 11: The experimental results for the four reconstruction tasks.\\nCT(Brain) CT(Pelvis) MRI (Brain) MRI(Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\nSEED-X 20.18 27.66 112.11 21.53 28.02 102.87 4.90 27.62 112.86 6.31 27.89 106.21\\nUnified-IO 2 83.93 36.09 17.95 85.36 35.10 25.46 87.50 34.25 25.47 86.31 33.53 29.80\\nHealthGPT-M3 91.73 36.42 15.46 94.26 37.30 12.53 88.76 33.97 27.05 84.40 33.11 32.62\\n34.08\\n15.87\\n9.16\\n5.62\\n13.33\\n21.94\\nHealthGPT\\nLLaVA-Med\\nHuatuoGPT-Vision\\nLlama-3.2\\nInternVL-2\\nShow-o\\nHuman\\nEvaluation\\n(a) (b)\\nFigure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_17', embedding=None, metadata={'page_label': '18', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 11: Case of modality transfer.\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/2502.09838v3.pdf_part_18', embedding=None, metadata={'page_label': '19', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 12: Case of MRI image super-resolution.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_0', embedding=None, metadata={'page_label': '1', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' \\n1     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_1', embedding=None, metadata={'page_label': '2', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only \\nVersion  control     Classification  Level:   Internal   Asset  Owner:   Legal     Asset:     WSO2  LLC  -  Anti-Corruption  Policy     Document  History:  \\nDate  Revision  Author(s)  Description  Reviewed  &  Approved  By  28/07/2022  V1.0  \\nLegal  Team  and  External  Counsel  (Cooley  LLP)   \\nInitial  Version  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n19/10/2023  V1.0  \\nLegal  Team  Reviewed.  No  changes  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n \\n2     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_2', embedding=None, metadata={'page_label': '3', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only  \\nContents  I.   PURPOSE \\nII.   POLICY  STATEMENTS \\nIII.   ANTI-BRIBERY  PROHIBITIONS \\nIV.   ACCOUNTING  REQUIREMENTS \\nV.   FACILITATION  PAYMENTS \\nVI.   INTERMEDIARIES  AND  BUSINESS  PARTNERS \\nVII.   GIFTS  AND  HOSPITALITIES \\nIX.   OTHER  ACTIVITIES \\nX.   VIOLATIONS  AND  CONSEQUENCES \\nXI.   TRAINING  AND  CERTIFICATION \\nXII.   STATUS \\nXIII.   REPORTING/QUESTIONS \\nIX.   ACKNOWLEDGEMENT   \\nATTACHMENT  1:  ANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL \\n●  UNITED  KINGDOM ●  THE  UK  BRIBERY  ACT  2010 ●  SRI  LANKA ●  BRAZIL o  THE  BRAZILIAN  ANTI  CORRUPTION  ACT  2013 o  THE  BRAZILIAN  IMPROBITY  ACT  1992 o  OTHER  POTENTIAL  LIABILITIES  \\n \\n3     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_3', embedding=None, metadata={'page_label': '4', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only WSO2\\n \\nLLC   A\\nNTI\\n-C\\nORRUPTION\\n \\nP\\nOLICY\\n  \\nA\\nPPROVED\\n \\nBY\\n \\nTHE\\n \\nB\\nOARD\\n \\nOF\\n \\nD\\nIRECTORS\\n  JULY  12,  2022   \\n  \\nI.   P\\nURPOSE\\n \\n \\nWSO2  LLC  (together  with  its  worldwide  subsidiaries,  “ WSO2 ”  or  the  “ Company ”)  has  implemented  this  \\nAnti-Corruption\\n \\nPolicy\\n \\n(the\\n \\n“\\nPolicy\\n”)\\n \\nfor\\n \\nthe\\n \\npurpose\\n \\nof\\n \\nensuring\\n \\ncompliance\\n \\nwith\\n \\nthe\\n \\nU.S.\\n \\nForeign\\n \\nCorrupt\\n \\nPractices\\n \\nAct\\n \\nof\\n \\n1977,\\n \\nas\\n \\namended\\n \\n(the\\n \\n“\\nFCPA\\n”),\\n \\nthe\\n \\nU.S.\\n \\nTravel\\n \\nAct,\\n \\nthe\\n \\nU.S.\\n \\nDomestic\\n \\nBribery\\n \\nStatute,\\n \\nthe\\n \\nUK\\n \\nBribery\\n \\nAct\\n \\n2010,\\n \\nthe\\n \\nSri\\n \\nLankan\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments),\\n \\nthe\\n \\nBrazilian\\n \\nAnti-corruption\\n \\nAct\\n \\n(Law\\n \\nNo.\\n \\n12,846/2013),\\n \\nthe\\n \\nBrazilian\\n \\nImprobity\\n \\nAct\\n \\n1992\\n \\n(Law\\n \\nNo.\\n \\n8.429/1992),\\n \\nand\\n  \\nall\\n  \\nother\\n  \\nanti-corruption\\n  \\nlaws\\n  \\nand\\n  \\nregulations\\n  \\napplicable\\n  \\nto\\n  \\nWSO2’s\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(collectively,\\n \\n“\\nAnti-Corruption\\n \\nLaws\\n”).\\n  \\nThis\\n \\nPolicy\\n \\napplies\\n \\nto\\n \\nall\\n \\nworld-wide\\n \\ndirectors,\\n \\nofficers,\\n \\nemployees,\\n \\nand\\n \\nindividuals\\n \\nserving\\n \\nas\\n \\nindependent\\n \\ncontractors\\n \\nof\\n \\nWSO2\\n \\n(collectively,\\n \\n“\\nWSO2\\n \\nPersonnel\\n”)\\n \\nto\\n \\ncomply\\n \\nwith\\n \\nthe\\n \\nprinciples\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy.\\n  \\nPlease\\n \\nreport\\n \\nall\\n \\nquestions\\n \\nor\\n \\nconcerns\\n \\nto\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nwhose\\n \\ncontact\\n \\ninformation\\n \\nappears\\n \\nbelow.\\n \\n \\nII.   P\\nOLICY\\n \\nS\\nTATEMENTS\\n \\n \\nWSO2  Personnel  are  strictly  prohibited  from  promising,  offering,  providing,  or  authorizing  cash  \\npayments\\n \\n(such\\n \\nas\\n \\nbribes\\n \\nor\\n \\nkickbacks)\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nalso\\n \\nstrictly\\n \\nprohibited\\n \\nfrom\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue\\n \\nfrom\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n    \\n \\nWSO2  Personnel  must  comply  with  all  of  the  Company’s  internal  controls,  especially  those  designed  \\nto\\n \\nensure\\n \\naccurate\\n \\nand\\n \\ncomplete\\n \\nbooks\\n \\nand\\n \\nrecords,\\n \\nor\\n \\notherwise\\n \\nprevent\\n \\ncorruption,\\n \\nself-dealing,\\n \\nembezzlement,\\n \\nfraud,\\n \\nmoney\\n \\nlaundering,\\n \\nor\\n \\nother\\n \\nimproper\\n \\nactivities.\\n \\n \\nThere  are  no  exceptions  to  this  Policy,  even  if  our  competitors  engage  in  improper  behavior  or  corruption  \\nis\\n \\nan\\n \\naccepted\\n \\npractice\\n \\nin\\n \\na\\n \\ncountry\\n \\nwhere\\n \\nwe\\n \\noperate.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nrequired\\n \\nto\\n \\nadhere\\n \\nto\\n \\nboth\\n \\nthe\\n \\nspirit\\n \\nand\\n \\nthe\\n \\nletter\\n \\nof\\n \\nthis\\n \\nPolicy\\n \\nwith\\n \\nrespect\\n \\nto\\n \\nour\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld.\\n \\n \\nIII.   A\\nNTI\\n-B\\nRIBERY\\n \\nP\\nROHIBITIONS\\n \\n \\nThe  FCPA  and  other  Anti-Corruption  Laws  prohibit  WSO2  and  WSO2  Personnel  from  corruptly  \\npromising,\\n \\noffering,\\n \\nproviding,\\n \\nor\\n \\nauthorizing\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nof\\n \\nvalue\\n \\ndirectly\\n \\nor\\n \\nindirectly\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nand\\n \\ncertain\\n \\nother\\n \\npersons\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose.\\n \\n“Improper\\n \\npurposes”\\n \\ninclude\\n \\ninfluencing\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\nthe\\n \\nrecipient\\n \\nin\\n \\nhis/her\\n \\nofficial\\n \\ncapacity,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ndo\\n \\nor\\n \\nomit\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nin\\n \\nviolation\\n \\nof\\n \\nhis/her\\n \\nlawful\\n \\nduty,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ninfluence\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\na\\n \\ngovernment\\n \\nor\\n \\ninstrumentality\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nor\\n \\nsecuring\\n \\nany\\n \\nimproper\\n \\nadvantage,\\n \\nin\\n \\norder\\n \\nto\\n \\nobtain,\\n \\nretain,\\n \\nor\\n \\ndirect\\n \\nregulatory\\n \\napprovals,\\n \\ncontracts,\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nbenefits.\\n   \\n \\n4     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_4', embedding=None, metadata={'page_label': '5', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only The  FCPA  prohibits  improper  payments  provided  to  officials  of  governments,  state-affiliated  entities,  and  \\npolitical\\n \\nparties\\n \\noutside\\n \\nthe\\n \\nUnited\\n \\nStates.\\n \\nHowever,\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\ngovernment\\n \\nor\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\nthe\\n \\nUnited\\n \\nStates\\n \\nwill\\n \\nviolate\\n \\nU.S.\\n \\ndomestic\\n \\nbribery\\n \\nstatutes.\\n \\n \\nIn  addition  to  the  United  States,  almost  all  other  countries,  including  the  United  Kingdom,  Brazil,  and  Sri  \\nLanka,\\n \\nhave\\n \\npromulgated\\n \\ntheir\\n \\nown\\n \\nanti-bribery\\n \\nlegislation.\\n \\nMost\\n \\nof\\n \\nthose\\n \\ncountries\\n \\nprohibit\\n \\nmaking\\n \\nimproper\\n \\npayments\\n \\nto\\n \\ngovernment\\n \\nand\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\ntheir\\n \\nborders.\\n \\nHowever,\\n \\nseveral\\n \\ncountries\\n \\nhave\\n \\nalso\\n \\nadopted\\n \\nlegislation\\n \\nsimilar\\n \\nto\\n \\nthe\\n \\nFCPA\\n \\nthat\\n \\nprohibit\\n \\nimproper\\n \\npayments\\n \\noutside\\n \\nthose\\n \\ncountries.\\n  \\nThe\\n \\nexistence\\n \\nof\\n \\nall\\n \\nof\\n \\nthese\\n \\nlaws\\n \\nmeans\\n \\nthat\\n \\nthere\\n \\nis\\n \\npotential\\n \\nfor\\n \\na\\n \\ncompany\\n \\nor\\n \\nan\\n \\nindividual\\n \\nto\\n \\nface\\n \\nliability\\n \\nin\\n \\nseveral\\n \\ncountries\\n \\nfor\\n \\nthe\\n \\nsame\\n \\nsingle\\n \\nact\\n \\nof\\n \\ncorruption.\\n  \\nAttachment\\n \\n1\\n \\ncontains\\n \\nan\\n \\noverview\\n \\nof\\n \\nthe\\n \\nAnti-Corruption\\n \\nLaws\\n \\nof\\n \\nother\\n \\njurisdictions\\n \\nwhich\\n \\nare\\n \\napplicable\\n \\nto\\n \\nWSO2.\\n \\n \\nGiven  the  broad  prohibitions  under  Anti-Corruption  Laws  applicable  to  WSO2,  this  Policy  \\nprohibits\\n \\nbribes,\\n \\nkickbacks,\\n \\nand\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nand\\n \\nadvantages\\n \\nto\\n \\nany\\n \\nperson,\\n \\nentity,\\n \\nor\\n \\norganization,\\n \\nincluding,\\n \\nbut\\n \\nnot\\n \\nlimited\\n \\nto,\\n \\nemployees,\\n \\nofficials,\\n \\nrepresentatives,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\n \\n(i)  government;   \\n(ii)  state-owned  or  affiliated  entity,  including,  but  not  limited  to,  a  state  hospital,  research  \\ninstitution,\\n \\nutility,\\n \\npublic\\n \\nuniversity,\\n \\nor\\n \\nsovereign\\n \\nwealth\\n \\nfund;\\n \\n \\n(iii)  public  international  organization  such  as  the  United  Nations  or  the  World  Bank;    \\n(iv)  political  party,  including  the  party  itself  as  well  as  candidates  for  public  office;    \\n(v)  non-governmental  organization;  or   (vi)   private-sector  company.      \\nOne  may  be  asked  by  certain  parties  to  provide  a  bribe  or  other  improper  benefit  in  exchange  for  the  \\naward\\n \\nof\\n \\na\\n \\ncontract,\\n \\nsponsorship\\n \\nopportunity,\\n \\nor\\n \\nother\\n \\nbusiness;\\n \\nthe\\n \\nissuance\\n \\nor\\n \\nrenewal\\n \\nof\\n \\na\\n \\nconcession,\\n \\nlicense,\\n \\nor\\n \\nbusiness,\\n \\nconstruction,\\n \\nor\\n \\nother\\n \\npermit\\n \\nor\\n \\nregistration;\\n \\nthe\\n \\nsuccessful\\n \\nfiling\\n \\nof\\n \\na\\n \\npatent\\n \\nor\\n \\ntrademark\\n \\napplication;\\n \\nan\\n \\nimpermissible\\n \\nreduction\\n \\nin\\n \\nduties\\n \\nor\\n \\nother\\n \\ntaxes;\\n \\nobtaining\\n \\na\\n \\nfavorable\\n \\ninspection\\n \\nresult\\n \\nor\\n \\ncourt\\n \\ndecision,\\n \\neven\\n \\nif\\n \\nthe\\n \\nfacts\\n \\nor\\n \\ncircumstances\\n \\ndo\\n \\nnot\\n \\nsupport\\n \\nsuch\\n \\na\\n \\nresult;\\n \\nor\\n \\nthe\\n \\ngrant\\n \\nof\\n \\nsome\\n \\nother\\n \\nimproper\\n \\nadvantage.\\n  \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nWSO2\\n \\nPersonnel\\n \\nfrom\\n \\nproviding\\n \\nbribes\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nany\\n \\nof\\n \\nthe\\n \\nabove\\n \\npurposes.\\n \\n \\nA  violation  of  this  Policy  can  occur  even  if  the  bribe  fails  to  achieve  the  purpose  for  which  it  was  \\nintended.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\na\\n \\nperson\\n \\ncan\\n \\nviolate\\n \\nthis\\n \\nPolicy\\n \\nif\\n \\nthat\\n \\nperson\\n \\nprovides\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nbenefit\\n \\nto\\n \\na\\n \\nrecipient\\n \\nand\\n \\nthe\\n \\nrecipient\\n \\ndoes\\n \\nnot\\n \\ngrant\\n \\nany\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nin\\n \\nreturn.\\n   \\nIn\\n \\naddition,\\n \\nthe\\n \\nmere\\n \\noffer\\n \\nor\\n \\npromise\\n \\nof\\n \\na\\n \\nbribe\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefit\\n \\nis\\n \\nsufficient\\n \\nto\\n \\ncause\\n \\na\\n \\nviolation.\\n  \\nAll\\n \\nof\\n \\nthe\\n \\nanti-bribery\\n \\nprohibitions\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy\\n \\napply\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\none\\n \\nuses\\n \\nWSO2\\n \\nfunds\\n \\nor\\n \\npersonal\\n \\nfunds\\n \\nto\\n \\nfinance\\n \\nimproper\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits.\\n \\n \\nThis  Policy  also  prohibits  WSO2  Personnel  from  soliciting  or  accepting  bribes,  kickbacks,  or  other  \\nimproper\\n \\npayments/benefits\\n \\nfrom\\n \\nthe\\n \\nCompany’s\\n \\nvendors\\n \\nor\\n \\nother\\n \\npersons\\n \\nin\\n \\nrelation\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\nFor\\n \\n5     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_5', embedding=None, metadata={'page_label': '6', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only instance,  a  violation  of  this  Policy  will  occur  if  you  cause  WSO2  to  overpay  a  vendor  and  that  vendor  then  \\nshares\\n \\nall\\n \\nor\\n \\na\\n \\nportion\\n \\nof\\n \\nthat\\n \\noverpayment\\n \\nwith\\n \\nyou.\\n   \\n \\nThis  Policy  requires  WSO2  Personnel  to  adhere  to  high  ethical  standards  and  to  comply  with  all  \\napplicable\\n \\nlaws\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nthe\\n \\nCompany.\\n  \\nAnti-corruption\\n \\nviolations\\n \\ntypically\\n \\ninvolve\\n \\ncircumstances\\n \\nthat\\n \\nalso\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws,\\n \\nincluding\\n \\nthose\\n \\nthat\\n \\naddress\\n \\nmoney\\n \\nlaundering,\\n \\nembezzlement,\\n \\nfraud,\\n \\nexport\\n \\ncontrols,\\n \\nand\\n \\nsanctions/embargoes.\\n \\nGuilty\\n \\npersons\\n \\ncan\\n \\nface\\n \\nmultiple\\n \\ncharges\\n \\nbased\\n \\non\\n \\nthe\\n \\nsame\\n \\nset\\n \\nof\\n \\nfacts.\\n \\n \\nIV.   A\\nCCOUNTING\\n \\nR\\nEQUIREMENTS\\n \\n \\nWSO2  must  maintain  books,  records,  and  accounts,  which,  in  reasonable  detail,  accurately  and  fairly  \\nreflect\\n \\nthe\\n \\nCompany’s\\n \\ntransactions,\\n \\nexpenses,\\n \\nand\\n \\nasset\\n \\ndispositions.\\n \\nWSO2\\n \\nis\\n \\nalso\\n \\ncommitted\\n \\nto\\n \\nmaintaining\\n \\na\\n  \\nsystem\\n \\nof\\n \\ninternal\\n \\naccounting\\n \\ncontrols\\n \\nto\\n \\nprovide\\n \\nreasonable\\n \\nassurances\\n \\nthat\\n \\ntransactions\\n \\nare\\n \\nproperly\\n \\nauthorized\\n \\nby\\n \\nmanagement,\\n \\nexecuted,\\n \\nand\\n \\nrecorded.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\ncomply\\n \\nwith\\n  \\nour\\n \\ninternal\\n \\ncontrols\\n \\nand\\n \\navoid\\n \\nunauthorized\\n \\nactivities\\n \\nor\\n \\nexpenses.\\n  \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\nalso\\n \\ncooperate\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nperiodic\\n \\naudits\\n \\nand\\n \\nother\\n \\nefforts\\n \\nto\\n \\nensure\\n \\nthat\\n \\nour\\n \\ninternal\\n \\ncontrols\\n \\nare\\n \\nbeing\\n \\nobserved.\\n \\n \\nViolations  of  the  above  accounting  standards  can  occur  if  one  conceals  bribes  or  falsifies  other  \\ntransactions\\n \\nor\\n \\nexpenses,\\n \\neven\\n \\nif\\n \\nthey\\n \\nare\\n \\nnot\\n \\nrelated\\n \\nto\\n \\na\\n \\nbribe,\\n \\nin\\n \\nWSO2’s\\n \\nledgers\\n \\nor\\n \\nother\\n \\nrecords.\\n  \\nAlso,\\n \\nthere\\n \\nis\\n \\nno\\n \\nmateriality\\n \\nstandard.\\n \\nThis\\n \\nmeans\\n \\nthat\\n \\neven\\n \\nsmall\\n \\nmisreported\\n \\namounts\\n \\nmay\\n \\nresult\\n \\nin\\n \\nviolations.\\n   \\n \\nV.   F\\nACILITATION\\n \\nP\\nAYMENTS\\n \\n \\n \\nThis  Policy  prohibits  all  corrupt  payments  or  benefits,  including  so-called  grease  or  facilitation  payments  \\nprovided\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\nto\\n \\nexpedite\\n \\nor\\n \\nsecure\\n \\nroutine\\n \\ngovernment\\n \\nactions\\n \\n(collectively,\\n \\n“\\nFacilitation\\n \\nPayments\\n”).\\n  \\nFacilitation\\n \\nPayments\\n \\ninclude\\n \\npayments\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nto\\n \\nexpedite\\n \\nroutine\\n \\nand\\n \\nnondiscretionary\\n \\nactivities,\\n \\nsuch\\n \\nas\\n \\nprocessing\\n \\npermit\\n \\nand\\n \\nlicense\\n \\napplications,\\n \\nscheduling\\n \\ninspections,\\n \\nand/or\\n \\nproviding\\n \\ninfrastructure\\n \\nservices\\n \\n(\\ne.g.\\n,\\n \\nwater,\\n \\nelectricity\\n \\nmail).\\n  \\nWSO2\\n \\nstrictly\\n \\nprohibits\\n \\nthe\\n \\noffer,\\n \\npromise,\\n \\nor\\n \\nprovision\\n \\nof\\n \\nFacilitation\\n \\nPayments\\n \\nto\\n \\nany\\n \\ndomestic\\n \\nor\\n \\nforeign\\n \\nlocal\\n \\nor\\n \\nfederal\\n \\ngovernment\\n \\nofficial,\\n \\nas\\n \\nthey\\n \\ncan\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws\\n \\nand\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndescribed\\n \\nabove.\\n \\n \\nPlease  note  that  in  some  cases,  government  agencies  may  impose  official  fees  that  may  be  paid  directly  in  \\nthe\\n \\nname\\n \\nof\\n \\na\\n \\ngovernmental\\n \\nentity\\n \\nor\\n \\nenterprise\\n \\nitself,\\n \\nas\\n \\nset\\n \\nout\\n \\nin\\n \\npublished\\n \\nfee\\n \\nschedules\\n \\nor\\n \\nother\\n \\nofficial\\n \\ndocuments.\\n  \\nThese\\n \\nofficial\\n \\ngovernment\\n \\nfees\\n \\ncan\\n \\nbe\\n \\npaid\\n \\nto\\n \\nexpedite\\n \\npassports,\\n \\nlicenses,\\n \\nor\\n \\nother\\n \\nservices,\\n \\nprovided\\n \\nthat\\n \\nthey\\n \\nare\\n \\ndeposited\\n \\nin\\n \\nthe\\n \\ntreasury\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nan\\n \\nofficial\\n \\ngovernment\\n \\nreceipt\\n \\nis\\n \\ncollected,\\n \\nand\\n \\nthe\\n \\nexpense\\n \\nis\\n \\naccurately\\n \\nrecorded\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n  \\nHowever,\\n \\nFacilitation\\n \\nPayments\\n \\nprovided\\n \\nfor\\n \\nthe\\n \\nbenefit\\n \\nof\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\n(\\ni.e.\\n,\\n \\nare\\n \\nnot\\n \\ndeposited\\n \\nin\\n \\nan\\n \\nofficial\\n \\ntreasury\\n \\naccount\\n \\nbelonging\\n \\nto\\n \\na\\n \\ngovernment)\\n \\nwill\\n \\nviolate\\n \\nthis\\n \\nPolicy.\\n     \\n \\n \\nVI.   I\\nNTERMEDIARIES\\n \\nAND\\n \\nB\\nUSINESS\\n \\nP\\nARTNERS\\n \\n \\nThis  Policy  prohibits  WSO2  Personnel  from  providing  bribes  or  other  improper  benefits  directly  as  well  \\nas\\n \\nindirectly\\n \\nthrough\\n \\nthird\\n \\nparties.\\n \\nThis\\n \\nrisk\\n \\ncan\\n \\narise\\n \\nin\\n \\ncases\\n \\nwhere\\n \\nthe\\n \\nCompany\\n \\nworks\\n \\nwith\\n \\nagents,\\n 6     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_6', embedding=None, metadata={'page_label': '7', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only consultants,  representatives,  lobbyists,  suppliers/vendors,  resellers,  distributors,  customs  or  other  brokers,  \\ncontractors,\\n \\nadvisors,\\n \\nother\\n \\nbusiness\\n \\npartners,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nthat\\n \\nperforms\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\nWSO2\\n \\n(collectively\\n \\n“\\nIntermediaries\\n”).\\n   \\n \\nIn  certain  cases,  WSO2  and  WSO2  Personnel  can  be  held  liable  under  the  FCPA  and  other  laws  even  if  \\none\\n \\ndoes\\n \\nnot\\n \\nexpressly\\n \\nauthorize\\n \\nan\\n \\nIntermediary\\n \\nto\\n \\nengage\\n \\nin\\n \\ncorruption,\\n \\nbut\\n \\nthey\\n \\ndo\\n \\nso\\n \\nanyway.\\n \\nThis\\n \\ncan\\n \\noccur\\n \\nif\\n \\none\\n \\n(i)\\n \\nhas\\n \\nactual\\n \\nknowledge\\n \\nor\\n \\na\\n \\nfirm\\n \\nbelief\\n \\nthat\\n \\na\\n \\nperson\\n \\nwill\\n \\nengage\\n \\nin\\n \\ncorruption\\n \\nor\\n \\n(ii)\\n \\nconsciously\\n \\ndisregards,\\n \\ndeliberately\\n \\nignores,\\n \\nor\\n \\nis\\n \\nwillfully\\n \\nblind\\n \\nto\\n \\nthe\\n \\nIntermediary’s\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\npractices.\\n  \\n \\nGiven  these  risks,  this  Policy  requires  that  (i)  appropriate,  risk-based  anti-corruption  due  diligence  is  \\nperformed\\n \\non\\n \\nIntermediaries\\n \\nto\\n \\nconfirm\\n \\nthat\\n \\nsuch\\n \\nIntermediary\\n \\ndoes\\n \\nnot\\n \\nhave\\n \\na\\n \\nhistory\\n \\nor\\n \\nreputation\\n \\nfor\\n \\ncorruption\\n \\nor\\n \\nsimilar\\n \\nwrong\\n \\ndoing,\\n \\nand\\n \\n(ii)\\n \\nthe\\n \\nIntermediary\\n \\nhas\\n \\nexecuted\\n \\na\\n \\nwritten\\n \\nagreement\\n \\ncontaining\\n \\nanti-corruption\\n \\ncompliance\\n \\nclauses.\\n \\nPlease\\n \\nconsult\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nfor\\n \\ninformation\\n \\nregarding\\n \\nWSO2’s\\n \\nIntermediary\\n \\ndue\\n \\ndiligence\\n \\nprocedures.\\n \\n \\nThroughout  any  relationship  with  an  Intermediary,  WSO2  Personnel  must  monitor  their  performance  to  \\nensure\\n \\nthat\\n \\nthey\\n \\ndo\\n \\nnot\\n \\nengage\\n \\nin\\n \\nactivities\\n \\nthat\\n \\nraise\\n \\ncorruption\\n \\nconcerns.\\n  \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nprovide\\n \\nguidance\\n \\non\\n \\nthe\\n \\ntypes\\n \\nof\\n \\nred\\n \\nflags\\n \\nthat\\n \\none\\n \\nshould\\n \\nmonitor\\n \\nbefore\\n \\nand\\n \\nafter\\n \\nengaging\\n \\nan\\n \\nIntermediary.\\n \\n \\nThis  Policy  requires  WSO2  Personnel  to  notify  the  Compliance  Officer  if  they  learn  of  any  Company  \\nIntermediary\\n \\nthat\\n \\nengages\\n \\nin\\n \\ncorrupt\\n \\nor\\n \\nother\\n \\nimproper\\n \\npractices.\\n \\nAlso,\\n \\nall\\n \\npayments\\n \\nto\\n \\nIntermediaries\\n \\nmust\\n \\nbe\\n \\naccurately\\n \\nreported\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks\\n \\nand\\n \\nrecords\\n \\nin\\n \\naccordance\\n \\nwith\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndiscussed\\n \\nabove.\\n \\n \\nVII.   G\\nIFTS\\n \\nAND\\n \\nH\\nOSPITALITIES\\n \\n \\nAnti-Corruption  Laws  prohibit  the  provision  or  acceptance  of  money  or  things  of  value  for  corrupt  or  \\nimproper\\n \\npurposes.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthis\\n \\nprohibition\\n \\nis\\n \\nlikely\\n \\nin\\n \\ninstances\\n \\nwhere\\n \\npersonal\\n \\nbenefits\\n \\nare\\n \\ngiven\\n \\nor\\n \\naccepted\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nnegotiation\\n \\nor\\n \\ntender\\n \\nbid.\\n \\nHowever,\\n \\nreasonably\\n \\npriced\\n \\ngifts,\\n \\nmeals,\\n \\nentertainment,\\n \\ntravel,\\n \\nand\\n \\nother\\n \\nbenefits\\n \\nprovided\\n \\nfor\\n \\nnon-corrupt\\n \\nbusiness\\n \\npromotion\\n \\nor\\n \\ngoodwill\\n \\npurposes\\n \\nmay\\n \\nbe\\n \\npermissible\\n \\nunder\\n \\nAnti-Corruption\\n \\nLaws\\n \\nin\\n \\ncertain\\n \\ncases.\\n  \\nFor\\n \\ninstance,\\n \\na\\n \\nplastic\\n \\npen,\\n \\na\\n \\nt-shirt,\\n \\na\\n \\ncoffee\\n \\nmug,\\n \\na\\n \\npaper\\n \\nweight,\\n \\nor\\n \\na\\n \\ncap\\n \\nof\\n \\nmoderate\\n \\nvalue\\n \\nand\\n \\nembossed\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nlogo\\n \\nwill\\n \\ngenerally\\n \\nnot\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws.\\n \\nHowever,\\n \\na\\n \\nfur\\n \\ncoat,\\n \\na\\n \\ncar,\\n \\nor\\n \\na\\n \\nvacation\\n \\nwill\\n \\nraise\\n \\nanticorruption\\n \\nconcerns,\\n \\nespecially\\n \\nif\\n \\nsuch\\n \\nbenefits\\n \\nare\\n \\nprovided\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nother\\n \\nperson\\n \\nwho\\n \\nis\\n \\nresponsible\\n \\nfor\\n \\nmaking\\n \\ndecisions\\n \\nin\\n \\nrelation\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n   \\n \\nWSO2  Personnel  must  also  ensure  that  the  provision  of  a  gift  or  other  benefit  does  not  violate  local  laws  \\nor\\n \\npolicies\\n \\nthat\\n \\napply\\n \\nin\\n \\nthe\\n \\ncountry\\n \\nwhere\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nthe\\n \\nbenefit\\n \\nis\\n \\nlocated.\\n  \\nSome\\n \\ncountries\\n \\nimpose\\n \\nexpress\\n \\nlimits\\n \\non\\n \\nthe\\n \\nvalue\\n \\nof\\n \\ngifts/benefits\\n \\nthat\\n \\na\\n \\nrecipient\\n \\ncan\\n \\naccept;\\n \\nother\\n \\ncountries\\n \\nban\\n \\nsuch\\n \\ngifts/benefits\\n \\naltogether\\n \\neven\\n \\nif\\n \\ngiven\\n \\nwith\\n \\nno\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\nintention.\\n \\n \\nWSO2  Personnel  must  obtain  the  approval  of  the  Compliance  Officer  prior  to  providing  gifts,  meals,  \\ntravel\\n \\nbenefits,\\n \\nand\\n \\nother\\n \\nhospitalities\\n \\nto\\n \\nemployees,\\n \\nofficials,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\ngovernment,\\n \\npolitical\\n \\nparty,\\n \\nstateowned\\n \\nentity,\\n \\nor\\n \\npublic\\n \\ninternational\\n \\norganization.\\n \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nhelp\\n \\ndetermine\\n \\n7     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_7', embedding=None, metadata={'page_label': '8', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only whether  the  provision  of  the  benefit  is  permissible  under  applicable  Anti-Corruption  Laws.   If  the  expense  \\nis\\n \\napproved,\\n \\nits\\n \\nvalue\\n \\nand\\n \\nbusiness\\n \\npurpose\\n \\nmust\\n \\nbe\\n \\nrecorded\\n \\naccurately\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nCompany\\n \\npersonnel\\n \\nfrom\\n \\nproviding\\n \\ncash\\n \\nor\\n \\ngift\\n \\ncards\\n \\nor\\n \\ngift\\n \\ncertificates\\n \\nthat\\n \\ncan\\n \\neasily\\n \\nbe\\n \\nconverted\\n \\ninto\\n \\ncash.\\n \\n \\nIX.   O\\nTHER\\n \\nA\\nCTIVITIES\\n \\n \\nCorruption  concerns  can  arise  in  a  number  of  other  cases  including,  but  not  limited  to  (i)  joint  ventures  or  \\nteaming\\n \\narrangements\\n \\nwith\\n \\npublic\\n \\nor\\n \\nprivate-sector\\n \\npartners;\\n \\n(ii)\\n \\nmergers\\n \\nand\\n \\nacquisitions,\\n \\nespecially\\n \\nif\\n \\nthe\\n \\ntarget\\n \\nbusiness\\n \\nhas\\n \\nsignificant\\n \\ngovernment\\n \\ninteractions\\n \\nor\\n \\nan\\n \\ninternational\\n \\nprofile;\\n \\nand\\n \\n(iii)\\n \\ncharitable\\n \\nand\\n \\npolitical\\n \\ndonations.\\n \\nPlease\\n \\nconfer\\n \\nwith\\n \\nthe\\n \\nCompliance\\n \\nOfficer\\n \\nbefore\\n \\nengaging\\n \\nin\\n \\nthese\\n \\ntypes\\n \\nof\\n \\nactivities\\n \\nto\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nanti-corruption\\n \\ncompliance\\n \\nmeasures\\n \\nare\\n \\nobserved.\\n \\n \\n \\nX.   V\\nIOLATIONS\\n \\nAND\\n \\nC\\nONSEQUENCES\\n \\n \\nA  violation  of  this  Policy  will  result  in  appropriate  disciplinary  action,  including  demotion,  reassignment,  \\nadditional\\n \\ntraining,\\n \\nprobation,\\n \\nsuspension,\\n \\nor\\n \\neven\\n \\ntermination.\\n \\n \\nBoth  the  Company  and  Company  Personnel  may  be  subject  to  substantial  fines  and  penalties  for  violating  \\nAnti-Corruption\\n \\nLaws.\\n  \\nIn\\n \\nserious\\n \\ncases,\\n \\nindividuals\\n \\nmay\\n \\nface\\n \\nimprisonment,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\nassessment\\n \\nof\\n \\nmonetary\\n \\nfines\\n \\nand\\n \\npenalties.\\n  \\nIn\\n \\naddition,\\n \\nthe\\n \\nCompany\\n \\nmay\\n \\nface\\n \\nsuspension\\n \\nor\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts,\\n \\nthe\\n \\nloss\\n \\nof\\n \\nU.S.\\n \\nexport\\n \\nprivileges,\\n \\nand\\n \\ncertain\\n \\nother\\n \\nconsequences.\\n \\nThese\\n \\nresults\\n \\ncan\\n \\nbe\\n \\ndevastating\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\n \\nXI.   T\\nRAINING\\n \\nAND\\n \\nC\\nERTIFICATION\\n \\n \\n \\nAll  designated  personnel  must  undergo  anti-corruption  training  provided  by  WSO2.   The  nature,  content,  \\nand\\n \\nfrequency\\n \\nof\\n \\nthat\\n \\ntraining\\n \\nwill\\n \\nbe\\n \\ndetermined\\n \\nby\\n \\nWSO2\\n \\nbased\\n \\non\\n \\nrisk\\n \\nprofile.\\n  \\n \\nWSO2  may  require  certain  WSO2  Personnel  to  certify  compliance  with  this  Policy  on  a  periodic  basis.   \\nXII.   S\\nTATUS\\n \\n \\n \\nThe  Compliance  Officer  and/or  outside  counsel  will  review  this  Policy  on  a  periodic  basis  and  update  it,  \\nas\\n \\nappropriate,\\n \\nto\\n \\nreflect\\n \\nany\\n \\nchanges.\\n   \\n \\nThis  Policy  does  not  form  part  of  any  employment  contract  with  you  and  may  be  amended  at  any  time.   \\nThis\\n \\nPolicy\\n \\nshould\\n \\nbe\\n \\nread\\n \\nin\\n \\nconjunction\\n \\nwith\\n \\nWSO2’s\\n \\nother\\n \\npolicies.\\n \\n \\n \\n8     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_8', embedding=None, metadata={'page_label': '9', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only XIII.   R\\nEPORTING\\n/Q\\nUESTIONS\\n  \\n \\n \\nWSO2  Personnel  have  an  affirmative  obligation  to  report  all  violations  of  this  Policy  to  the  Compliance  \\nOfficer\\n \\nas\\n \\nfollows:\\n \\n \\nPuny  Navaratne  legal-compliance@wso2.com   \\nReports  may  also  be  submitted  anonymously  by  using  the  Company’s  hotline  number  \\n(800)\\n \\n461-9330\\n \\nor\\n \\nonline\\n \\nat\\n \\nwhistleblower.wso2.com.\\n    \\nHowever,\\n \\nwe\\n \\nencourage\\n \\nyou\\n \\nto\\n \\nconsider\\n \\nrevealing\\n \\nyour\\n \\nidentity\\n \\nso\\n \\nthat\\n \\nwe\\n \\ncan\\n \\nproperly\\n \\nfollow\\n \\nup\\n \\nand\\n \\ninvestigate\\n \\nalleged\\n \\nviolations.\\n \\nThe\\n \\nCompany\\n \\nwill\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nconfidentiality\\n \\nmeasures\\n \\nare\\n \\ntaken\\n \\nand\\n \\nwill\\n \\nnot\\n \\nretaliate\\n \\nagainst\\n \\nany\\n \\nindividual\\n \\nfor\\n \\nreporting\\n \\nviolations\\n \\nin\\n \\ngood\\n \\nfaith.\\n \\n \\nWSO2  Personnel  must  also  notify  the  Compliance  Officer  of  any  corrupt,  improper,  illegal,  or  other  \\nunusual\\n \\nrequests\\n \\nfor\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits\\n \\nmade\\n \\nby\\n \\ncustomers,\\n \\nIntermediaries,\\n \\nvendors,\\n \\nbusiness\\n \\npartners,\\n \\ngovernment\\n \\nofficials,\\n \\nor\\n \\nCompany\\n \\nemployees.\\n   \\nBy\\n \\nreporting\\n \\nsuch\\n \\nmatters,\\n \\nyou\\n \\nwill\\n \\nenable\\n \\nus\\n \\nto\\n \\nexplore\\n \\noptions\\n \\nto\\n \\nachieve\\n \\nour\\n \\nbusiness\\n \\ngoals\\n \\nwithout\\n \\nhaving\\n \\nto\\n \\ninteract\\n \\nwith\\n \\nsuch\\n \\npersons\\n \\nor\\n \\nprovide\\n \\nimproper\\n \\nbenefits.\\n \\n \\nIX.   ACKNOWLEDGEMENT\\n  \\n \\n \\nPlease  click here to  certify  and  acknowledge  that  you  have  read  and  understood  the  contents  of  this  Policy.  \\n9     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_9', embedding=None, metadata={'page_label': '10', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only ATTACHMENT  1   \\nANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL   \\n  \\nUNITED  KINGDOM   \\nT\\nHE\\n \\nUK\\n \\nB\\nRIBERY\\n \\nA\\nCT\\n \\n2010  \\n \\n  Among  various  matters,  the  UK  Bribery  Act  2010  (the  “ UKBA ”)  prohibits  individuals  and  entities  from  \\noffering,\\n \\npromising,\\n \\nor\\n \\ngiving\\n \\n(directly\\n \\nor\\n \\nindirectly\\n \\nthrough\\n \\na\\n \\nthird\\n \\nparty)\\n \\na\\n \\nfinancial\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nto\\n \\na\\n \\nrecipient\\n \\nwith\\n \\n(i)\\n \\nthe\\n \\nintention\\n \\nthat\\n \\nthe\\n \\nadvantage\\n \\ninduce\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\nperform\\n \\nimproperly\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity\\n \\nor\\n \\nto\\n \\nreward\\n \\na\\n \\nperson\\n \\nfor\\n \\nthe\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\nsuch\\n \\nfunction\\n \\nor\\n \\nactivity,\\n \\nor\\n \\n(ii)\\n \\nthe\\n \\nknowledge\\n \\nor\\n \\nbelief\\n \\nthat\\n \\nthe\\n \\nacceptance\\n \\nof\\n \\nthe\\n \\nadvantage\\n \\nwould\\n \\nitself\\n \\nconstitute\\n \\nan\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nUKBA\\n \\nwill\\n \\noccur\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nadvantage\\n \\nis\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nan\\n \\nemployee\\n \\nof\\n \\na\\n \\nprivatesector\\n \\nentity.\\n \\n   The  UKBA  contains  four  principal  offenses  as  follows:  (i)  offering,  promising,  or  giving  of  a  bribe  to  \\nanother\\n \\nperson\\n \\n(Section\\n \\n1);\\n \\n(ii)\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\na\\n \\nbribe\\n \\n(Section\\n \\n2);\\n \\n(iii)\\n \\nbribery\\n \\nof\\n \\na\\n \\nforeign\\n \\n(non-UK)\\n \\npublic\\n \\nofficial\\n \\n(Section\\n \\n6);\\n \\nand\\n \\n(iv)\\n \\nfailure\\n \\nby\\n \\ncertain\\n \\ncommercial\\n \\norganizations\\n \\nto\\n \\nprevent\\n \\nSection\\n \\n1\\n \\nor\\n \\n6\\n \\nbribery\\n \\noffenses\\n \\nby\\n \\ntheir\\n \\nassociated\\n \\npersons\\n \\n(including\\n \\nemployees,\\n \\ncontractors,\\n \\nIntermediaries,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\na\\n \\ncompany)\\n \\nof\\n \\nany\\n \\nnationality\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(Section\\n \\n7).\\n  \\nThe\\n \\nUKBA\\n \\nprovides\\n \\na\\n \\nstatutory\\n \\ndefense\\n \\nto\\n \\na\\n \\nSection\\n \\n7\\n \\nviolation\\n \\nfor\\n \\ncompanies\\n \\nthat\\n \\ncan\\n \\ndemonstrate\\n \\nthat\\n \\nthey\\n \\nhad\\n \\nin\\n \\nplace\\n \\nadequate\\n \\nsystems\\n \\nand\\n \\ncontrols\\n \\ndesigned\\n \\nto\\n \\nprevent\\n \\noffenses\\n \\nunder\\n \\nUKBA.\\n \\nThis\\n \\nPolicy\\n \\nis\\n \\npart\\n \\nof\\n \\nthe\\n \\nCompany’s\\n \\noverall\\n \\neffort\\n \\nto\\n \\nestablish\\n \\nsuch\\n \\nsystems\\n \\nand\\n \\ncontrols.\\n  \\n   Courts  in  the  United  Kingdom  exercise  broad  jurisdiction  over  UK  as  well  as  non-UK  persons  who  \\ncommit\\n \\nUKBA\\n \\noffenses.\\n  \\nThe\\n \\nCompany\\n \\nmaintains\\n \\na\\n \\nUK\\n \\nsubsidiary.\\n \\nIt\\n \\nis\\n \\nclear\\n \\nthat\\n \\nboth\\n \\nthis\\n \\nUK\\n \\nsubsidiary\\n \\nand\\n \\nmost\\n \\nof\\n \\nits\\n \\nemployees\\n \\nwill\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nthe\\n \\nUKBA.\\n  \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-UK\\n \\nentities\\n \\nand\\n \\nemployees\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nUKBA\\n \\njurisdiction.\\n \\n    Under  the  UKBA,  individuals  guilty  of  bribery  may  be  subject  to  imprisonment  for  up  to  10  years  and/or  \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount.\\n \\nCommercial\\n \\norganizations\\n \\nguilty\\n \\nof\\n \\nbribery\\n \\nor\\n \\nfailure\\n \\nto\\n \\nprevent\\n \\nbribery\\n \\nmay\\n \\nalso\\n \\nbe\\n \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount\\n \\nas\\n \\nwell\\n \\nas\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts.\\n  \\nIn\\n \\naddition,\\n \\nUKBA\\n \\noffenses\\n \\ncould\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws\\n \\nsuch\\n \\nas\\n \\nthe\\n \\nUK\\n \\nProceeds\\n \\nof\\n \\nCrime\\n \\nAct\\n \\n2002,\\n \\nwhich\\n \\ncontains\\n \\nthe\\n \\nUK’s\\n \\nprincipal\\n \\nmoney\\n \\nlaundering\\n \\noffenses.\\n \\n    *   *   *   *   *     SRI  LANKA   \\nThe  legal  framework  for  the  prevention,  investigation  and  punishment  of  corruption  is  primarily  reflected  \\nin\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments).\\n  \\n \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_10', embedding=None, metadata={'page_label': '11', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only   The  law  prohibits  persons  from  offering  gratifications  and  rewards  to  certain  categories  of  persons  as  \\ninducements\\n \\nand\\n \\nrewards\\n \\nfor\\n \\nthe\\n \\nperformance\\n \\nor\\n \\nnonperformance\\n \\nof\\n \\nspecified\\n \\nactivities.\\n \\nThey\\n \\nare\\n \\n(a)\\n \\njudicial\\n \\nofficers\\n \\nand\\n \\nMembers\\n \\nof\\n \\nParliament\\n \\nin\\n \\nrespect\\n \\nof\\n \\ntheir\\n \\nofficial\\n \\nduties;\\n \\n(b)\\n \\npolice\\n \\nofficers,\\n \\npeace\\n \\nofficers\\n \\nor\\n \\nother\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ninterfering\\n \\nwith\\n \\nthe\\n \\ndue\\n \\nadministration\\n \\nof\\n \\njustice,\\n \\nor\\n \\nprocuring\\n \\nor\\n \\nfacilitating\\n \\nthe\\n \\ncommission\\n \\nof\\n \\nany\\n \\noffence,\\n \\nor\\n \\nprotecting\\n \\noffenders\\n \\nfrom\\n \\ndetection\\n \\nor\\n \\npunishment,\\n \\nor\\n \\nabusing\\n \\nofficial\\n \\npowers\\n \\nto\\n \\nthe\\n \\ninjury\\n \\nor\\n \\ndetriment\\n \\nof\\n \\nany\\n \\nperson;\\n \\n(c)\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ntheir\\n \\nassistance\\n \\nor\\n \\ninfluence\\n \\nin\\n \\npromoting\\n \\nthe\\n \\nprocurement\\n \\nof\\n \\nany\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nfor\\n \\nany\\n \\nwork,\\n \\nservice\\n \\nor\\n \\nthe\\n \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial\\n \\nor\\n \\nsubstance,\\n \\nor\\n \\nin\\n \\nthe\\n \\nexecution\\n \\nof\\n \\nany\\n \\ncontract,\\n \\nor\\n \\nin\\n \\nthe\\n \\npayment\\n \\nof\\n \\nthe\\n \\nprice\\n \\nor\\n \\nconsideration\\n \\nor\\n \\nof\\n \\nany\\n \\nsubsidy\\n \\nin\\n \\nrespect\\n \\nthereof;\\n \\n(d)\\n  \\na\\n \\ntenderer\\n \\nfor\\n \\na\\n \\ncontract\\n \\nto\\n \\nwithdraw\\n \\nthe\\n \\ntender,\\n \\nor\\n \\nfor\\n \\nwithdrawing\\n \\na\\n \\ntender\\n \\nmade\\n \\nfor\\n \\na\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nintent\\n \\nof\\n \\nobtaining\\n \\nsuch\\n \\ncontract\\n \\nfor\\n \\nwork,\\n \\nservice\\n \\nor\\n  \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial,\\n \\nor\\n \\nsubstance;\\n \\n(e)\\n  \\npublic\\n \\nofficers\\n \\nto\\n \\nperform,\\n \\nabstain\\n \\nfrom\\n \\nperforming,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact\\n \\nwhether\\n \\nby\\n \\nthat\\n \\npublic\\n \\nofficer\\n \\nor\\n \\nby\\n \\nany\\n \\nother\\n \\npublic\\n \\nofficer,\\n \\nor\\n \\nassisting,\\n \\nfavoring,\\n \\nhindering\\n \\nor\\n \\ndelaying\\n \\nany\\n \\nperson\\n \\nin\\n \\nthe\\n \\ntransaction\\n \\nof\\n \\nany\\n \\nbusiness\\n \\nwith\\n \\nthe\\n \\nGovernment;\\n \\n(f)\\n \\npersons\\n \\nto\\n \\nprocure\\n \\nthe\\n \\nGovernment\\n \\nto\\n \\npay\\n \\nany\\n \\nclaim,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nprevent\\n \\nappointment\\n \\nto\\n \\nany\\n \\noffice,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nsecure\\n \\nany\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nother\\n \\nbenefit\\n \\nfrom\\n \\nthe\\n \\nGovernment,\\n \\nor\\n \\nprevent\\n \\nthe\\n \\nsecuring\\n \\nof\\n \\nany\\n \\nsuch\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nbenefit\\n \\nby\\n \\nsuch\\n \\nother\\n \\nperson;\\n \\n(g)\\n \\npublic\\n \\nofficer\\n \\nemployed\\n \\nin\\n \\na\\n \\ngovernment\\n \\ndepartment,\\n \\noffice\\n \\nor\\n \\nestablishment\\n \\nwhile\\n \\nhaving\\n \\ndealings\\n \\nof\\n \\nany\\n \\nkind\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nthrough\\n \\nsuch\\n \\nan\\n \\nentity,\\n \\nor\\n \\nwithin\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nsuch\\n \\ndealings\\n \\n(provided\\n \\nthat\\n \\nif\\n \\nsuch\\n \\ngratification\\n \\nwas\\n \\npaid\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nthe\\n \\ndealings\\n \\nit\\n \\nshall\\n \\nnot\\n \\nbe\\n \\nconsidered\\n \\nan\\n \\noffence\\n \\nif\\n \\nit\\n \\ncan\\n \\nbe\\n \\nproved\\n \\nthat\\n \\nit\\n \\nwas\\n \\noffered\\n \\nin\\n \\ngood\\n \\nfaith\\n \\nfor\\n \\na\\n \\npurpose\\n \\nnot\\n \\nconnected\\n \\nwith\\n \\nor\\n \\nunrelated\\n \\nto\\n \\nsuch\\n \\ndealings,\\n \\nand\\n \\nwhen\\n \\nit\\n \\nwas\\n \\noffered,\\n \\nthere\\n \\nwas\\n \\nno\\n \\nhope\\n \\nor\\n \\nexpectation\\n \\nof\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nit\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nfor\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nthe\\n \\ngratification\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nor\\n \\nreward\\n \\nfor\\n \\nthe\\n \\npublic\\n \\nofficer\\n \\ndoing\\n \\nor\\n \\nforbearing\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nconnected\\n \\nwith\\n \\nsuch\\n \\ndealings);\\n \\n(h)\\n \\nmembers,\\n \\nofficers\\n \\nor\\n \\nemployees\\n \\nof\\n \\nlocal\\n \\nauthorities\\n \\nor\\n \\nscheduled\\n \\ninstitutions\\n \\nfor\\n \\nvoting\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nvoting\\n \\nat\\n \\nmeetings\\n \\nof\\n \\nsuch\\n \\nbodies\\n \\nfor\\n \\nor\\n \\nagainst\\n \\nmatters\\n \\narising\\n \\nbefore\\n \\nthem,\\n \\nor\\n \\ntheir\\n \\nperformance,\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nperforming,\\n \\nor\\n \\naiding\\n \\nin\\n \\nprocuring,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact,\\n \\nor\\n \\naiding,\\n \\nprocuring,\\n \\nor\\n \\npreventing\\n \\nthe\\n \\npassing\\n \\nof\\n \\nany\\n \\nvote\\n \\nor\\n \\ngranting\\n \\nof\\n \\nany\\n \\ncontract\\n \\nor\\n \\nadvantage\\n \\nin\\n \\nfavor\\n \\nof\\n \\nany\\n \\nperson.\\n  \\n \\nEach  of  the  above  provisions  have  corresponding  offence  in  respect  of  the  receipt  of  gratifications.      The  Bribery  Act  provides  for  imprisonment  of  up  to  seven  years  and  fines  of  up  to  five  thousand  Sri  \\nLankan\\n \\nrupees\\n \\nfor\\n \\nthe\\n \\ncommission\\n \\nof\\n \\noffences.\\n  \\nSri\\n \\nLankan\\n \\ncourts\\n \\nmay\\n \\nalso\\n \\nimpose\\n \\npenalties\\n \\namounting\\n \\nto\\n \\nthe\\n \\nvalue\\n \\nof\\n \\nthe\\n \\ngratification\\n \\nif\\n \\nthe\\n \\nconviction\\n \\nis\\n \\nentered\\n \\nby\\n \\na\\n \\nHigh\\n \\nCourt.\\n   \\n   Similar  offences  have  been  created  in  respect  of  members  of  public  authorities  by  the  Public  Bodies  \\n(Prevention\\n \\nof\\n \\nCorruption)\\n \\nAct\\n \\nNo\\n \\n13\\n \\nof\\n \\n1950.\\n \\n \\nThe  Bribery  Act  is  enforced  through  the  Commission  to  Investigate  Allegations  of  Bribery  or  Corruption  \\nAct\\n \\nNo.\\n \\n19\\n \\nof\\n \\n1994.\\n  \\nThe\\n \\nCommission\\n \\nhas\\n \\nwide\\n \\npowers\\n \\nof\\n \\ninvestigation\\n \\nincluding\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nas\\n \\nwell\\n \\nas\\n \\nof\\n \\nrequiring\\n \\ndeclaration\\n \\nof\\n \\nassets\\n \\nand\\n \\nliabilities\\n \\nby\\n \\nMembers\\n \\nof\\n \\nParliament,\\n \\njudges,\\n \\npublic\\n \\nofficials\\n \\nof\\n \\nGovernment\\n \\ndepartments,\\n \\nministries,\\n \\nand\\n \\nlocal\\n \\nauthorities,\\n \\nchairpersons\\n \\nand\\n \\nstaff\\n \\nof\\n \\npublic\\n \\ncorporations,\\n \\ncandidates\\n \\nfor\\n \\nelected\\n \\npublic\\n \\noffice\\n \\nand\\n \\nelected\\n \\nofficials\\n \\nunder\\n \\nthe\\n \\nDeclaration\\n \\nof\\n \\nAssets\\n \\nand\\n \\nLiabilities\\n \\nLaw,\\n \\nNo.\\n \\n1\\n \\nof\\n \\n1975.\\n  \\n \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_11', embedding=None, metadata={'page_label': '12', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only Sri  Lanka  also  has  a  domestic  legal  regime  against  money-laundering  which  includes  the  Prevention  of  \\nMoney-\\n \\nLaundering\\n \\nAct,\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct\\n \\nand\\n \\nthe\\n \\nConvention\\n \\non\\n \\nthe\\n \\nSuppression\\n \\nof\\n \\nTerrorist\\n \\nFinancing\\n \\nAct.\\n  \\nOffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nand\\n \\nother\\n \\ncorruption-related\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nPenal\\n \\nCode\\n \\nare\\n \\nconsidered\\n \\npredicate\\n \\noffences\\n \\nfor\\n \\nthe\\n \\npurposes\\n \\nof\\n \\nthe\\n \\nPrevention\\n \\nof\\n \\nMoney-Laundering\\n \\nAct\\n \\nand\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct.\\n  \\n \\n     *   *   *   *   *     B\\nRAZIL\\n \\n \\nTHE  BRAZILIAN  ANTICORRUPTION  ACT  2013   \\nLaw  No.  12,846/2013  (the  Brazilian  Anticorruption  Act  or  Lei  Anticorrupção  –  “ LAC ”)  provides  for  \\nstrict\\n \\nliability\\n \\nto\\n \\ncompanies,\\n1\\n \\nin\\n \\nthe\\n \\nadministrative\\n \\nand\\n \\ncivil\\n \\nspheres,\\n \\nfor\\n \\nwrongful\\n \\nacts\\n \\ncarried\\n \\nout\\n \\nin\\n \\ntheir\\n \\ninterest\\n \\nor\\n \\nfor\\n \\ntheir\\n \\nbenefit.\\n \\n   The  LAC  is  applicable  to  activities  after  January  2014  and  the  main  offenses  are:  (i)  promising,  offering  \\nor\\n \\ngiving,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nundue\\n \\nadvantage\\n \\nto\\n \\na\\n \\npublic\\n \\nagent\\n \\nor\\n \\nthird\\n \\nperson\\n \\nrelated\\n \\nto\\n \\nit;\\n \\n(ii)\\n \\nfinancing\\n \\nor\\n \\nin\\n \\nany\\n \\nway\\n \\nsponsoring\\n \\nthe\\n \\npractice\\n \\nof\\n \\nwrongdoings\\n \\ndescribed\\n \\nin\\n \\nthe\\n \\nAct;\\n \\n(iii)\\n \\nusing\\n \\na\\n \\nthird\\n \\nparty\\n \\nto\\n \\nconceal\\n \\nor\\n \\nsimulate\\n \\nits\\n \\nactual\\n \\ninterests\\n \\nor\\n \\nthe\\n \\nidentity\\n \\nof\\n \\nthe\\n \\nbeneficiaries\\n \\nof\\n \\nthe\\n \\nillegal\\n \\nacts\\n \\nagainst\\n \\nthe\\n \\npublic\\n \\nadministration;\\n \\n(iv)\\n \\nengaging\\n \\nin\\n \\nfraudulent\\n \\nacts\\n \\nin\\n \\npublic\\n \\ntenders,\\n \\nsuch\\n \\nas\\n \\nparticipating\\n \\nin\\n \\nbid\\n \\nrigging\\n \\nor\\n \\ndisturbing\\n \\nany\\n \\nstep\\n \\nof\\n \\nthe\\n \\npublic\\n \\ntender;\\n \\nand\\n \\n(v)\\n \\nobstructing\\n \\nor\\n \\nhampering\\n \\nthe\\n \\nsurveillance\\n \\nor\\n \\ninvestigations\\n \\nof\\n \\npublic\\n \\nentities.\\n \\n   In  the  administrative  sphere,  legal  entities  that  are  found  guilty  of  breaching  the  LAC  are  subject  to  a  fine  \\nof\\n \\n0.1%\\n \\nto\\n \\n20%\\n \\nof\\n \\nthe\\n \\ngross\\n \\nrevenue,\\n \\nless\\n \\ntaxes,\\n \\nregistered\\n \\nin\\n \\nthe\\n \\nyear\\n \\nprior\\n \\nto\\n \\nthe\\n \\ninitiation\\n \\nof\\n \\nthe\\n \\nadministrative\\n \\nproceedings,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\npublication\\n \\nof\\n \\nthe\\n \\ndecision.\\n \\nMoreover,\\n \\nother\\n \\npenalties\\n \\nmay\\n \\nbe\\n \\nenforced\\n \\nin\\n \\nthe\\n \\ncivil\\n \\nsphere\\n \\nby\\n \\ncourts,\\n \\nsuch\\n \\nas:\\n \\n(i)\\n \\nseizure\\n \\nof\\n \\nassets\\n \\nobtained\\n \\nthrough\\n \\nillegal\\n \\npractice;\\n \\n(ii)\\n \\nsuspension\\n \\nor\\n \\npartial\\n \\nshutdown\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity’s\\n \\nactivities;\\n \\n(iii)\\n \\ncompulsory\\n \\ntermination\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity;\\n \\nand\\n \\n(iv)\\n \\nprohibition\\n \\nof\\n \\nreceiving\\n \\nincentives,\\n \\nsubsidies,\\n \\ngrants,\\n \\ndonations\\n \\nor\\n \\nloans\\n \\nfrom\\n \\npublic\\n \\nbodies\\n \\nor\\n \\nentities\\n \\nor\\n \\nfrom\\n \\npublic\\n \\nfinancial\\n \\ninstitutions\\n \\nor\\n \\npublicly-controlled\\n \\nfinancial\\n \\ninstitutions,\\n \\nfrom\\n \\n1\\n \\nto\\n \\n5\\n \\nyears.\\n \\n   The  existence  of  an  effective  compliance  program  may  be  considered  a  mitigating  factor  in  the  evaluation  \\nof\\n \\nthe\\n \\napplicable\\n \\nfine\\n \\nto\\n \\nbe\\n \\napplied\\n \\nto\\n \\nthe\\n \\nlegal\\n \\nentity.\\n \\n   The  Company  maintains  a  Brazilian  subsidiary.  It  is  clear  that  the  Brazilian  subsidiary  may  be  held  liable  \\nunder\\n \\nthe\\n \\nLAC.\\n \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-Brazilian\\n \\nentities\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nLAC\\n \\njurisdiction.\\n \\n   \\n1\\n  In  this  sense,  the  authorities  are  not  required  to  show  intent  or  fault  of  the  legal  entity.  The  mere  fact  that  there  is  \\nmateriality\\n \\nas\\n \\nto\\n \\nthe\\n \\nviolation\\n \\nand\\n \\nthat\\n \\nthe\\n \\nviolation\\n \\nhappened\\n \\nin\\n \\nthe\\n \\ninterest\\n \\nor\\n \\nbenefit\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity\\n \\nis\\n \\nsufficient\\n \\nto\\n \\nconsider\\n \\nthat\\n \\nthere\\n \\nis\\n \\na\\n \\nbreach\\n \\nto\\n \\nthe\\n \\nAct.\\n \\n  \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
              " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_12', embedding=None, metadata={'page_label': '13', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only T\\nHE\\n \\nB\\nRAZILIAN\\n \\nI\\nMPROBITY\\n \\nA\\nCT\\n \\n1992  \\n \\n  In  a  broad  sense,  Law  No.  8,429/1992  (the  Brazilian  Improbity  Act  or  Lei  de  Improbidade  Administrativa   –  “ LIA ”)  is  applicable  to  (i)  facts  that  also  constitute  a  violation  to  the  LAC  but  happened  before  January   \\n  2014;  and  (ii)  facts  that  happened  after  January  2014  and  do  not  constitute  a  breach  to  the  LAC,  but  are  a  \\nbreach\\n \\nof\\n \\nthe\\n \\nLIA.\\n \\n   There  are  three  broad  types  of  misconduct  provided  for  in  the  LIA:  (i)  unjust  enrichment;  (ii)  damage  to  \\nthe\\n \\npublic\\n \\ntreasury;\\n \\nand\\n \\n(iii)\\n \\nacts\\n \\nin\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nprinciples\\n \\nthat\\n \\ngovern\\n \\nthe\\n \\nPublic\\n \\nAdministration.\\n  \\nThe\\n \\nLIA\\n \\nprovides\\n \\nfor\\n \\nsanctions\\n \\non\\n \\npublic\\n \\nagents,\\n \\nas\\n \\nwell\\n \\nas\\n \\non\\n \\nprivate\\n \\nentities\\n \\nand\\n \\nindividuals\\n \\nthat\\n \\nwillfully\\n \\naided\\n \\nor\\n \\nparticipated\\n \\nin\\n \\nimprobity\\n \\nacts.\\n  \\nIts\\n \\nmain\\n \\nsanctions\\n \\nare\\n \\n(i)\\n \\nforfeiture\\n \\nof\\n \\nassets\\n \\nor\\n \\nvalues\\n \\nunlawfully\\n \\nobtained;\\n \\n(ii)\\n \\ndismissal\\n \\nfrom\\n \\npublic\\n \\noffice;\\n \\n(iii)\\n \\npolitical\\n \\nblacklisting;\\n \\n(iv)\\n \\npayment\\n \\nof\\n \\nfines\\n \\nequivalent\\n \\nto\\n \\nthe\\n \\nunlawfully\\n \\nobtained\\n \\namounts;\\n \\n(v)\\n \\nprohibition\\n \\nagainst\\n \\ncontracting\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nor\\n \\nreceiving\\n \\ntax\\n \\nor\\n \\ncredit\\n \\nincentives,\\n \\ndirectly\\n \\nor\\n \\nindirectly;\\n \\nand\\n \\n(vi)\\n \\npayment\\n \\nof\\n \\nfines.\\n  \\nIn\\n \\norder\\n \\nfor\\n \\na\\n \\nsanction\\n \\nto\\n \\nbe\\n \\napplied,\\n \\nit\\n \\nis\\n \\nnecessary\\n \\nto\\n \\nshow\\n \\nintent\\n \\nof\\n \\nthe\\n \\nwrongdoer.\\n \\n   O\\nTHER\\n \\nP\\nOTENTIAL\\n \\nL\\nIABILITIES\\n \\n \\nIn  addition  to  the  LAC  and  the  LIA,  private  and  public  entities  can  request  compensation  for  collective  or  \\nmoral\\n \\ndamages\\n \\nresulting\\n \\nfrom\\n \\ncorruption\\n \\ncases,\\n \\nas\\n \\nprovided\\n \\nby\\n \\nBrazilian\\n \\nClass\\n \\nAction\\n \\nLaw,\\n \\nand\\n \\nentities\\n \\ndeemed\\n \\nto\\n \\nbe\\n \\nharmed/damaged\\n \\nby\\n \\nthe\\n \\nwrongdoing\\n \\ncan\\n \\nfile\\n \\na\\n \\nlawsuit\\n \\nclaiming\\n \\ncompensation\\n \\nfor\\n \\ndamages.\\n \\nAlso,\\n \\nthe\\n \\nBrazilian\\n \\nFederal\\n \\nCourt\\n \\nof\\n \\nAccounts\\n \\ncan\\n \\nimpose\\n \\nsanctions\\n \\nif\\n \\nthey\\n \\nfind\\n \\ncontract\\n \\nfraud\\n \\nwhile\\n \\nauditing\\n \\npublic\\n \\nentities.\\n \\n   Finally,  the  Brazilian  Criminal  Code  sets  forth  that  corruption  may  result  in  imprisonment  for  up  to  16  \\nyears\\n \\nfor\\n \\nindividuals.\\n \\n        \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_nam_as_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VXJOc1mPQPV",
        "outputId": "bfc065f8-0bdd-446f-c8f5-db1a13eb5469"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdQk5JnXPQR4",
        "outputId": "db657948-8b2d-409a-c1be-2ab2cb73b61a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(id_='487b9093-1ad3-4685-ab72-3428a9365856', embedding=None, metadata={'page_label': '1', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='HealthGPT: A Medical Large Vision-Language Model for Unifying\\nComprehension and Generation via Heterogeneous Knowledge Adaptation\\nTianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\\nMengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\\n1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\\n4The Hong Kong University of Science and Technology,5National University of Singapore\\nProject Page\\n Code\\n1. X-Ray \\nComprehension\\n2. CT\\nComprehension\\n3. MRI\\nComprehension\\nComp. Perf.\\n7 Medical Multi-Modal Comprehension Tasks\\nGen. \\nPerformance\\n5 Medical Multi-Modal Generation Tasks\\nList all anatomical locations showing \\npulmonary edema, hazy opacity, or \\nmediastinal displacement.\\nLeft hilar structures, left lung,\\nright hilar structures, right lung.\\nWhich abdominal organ shows any \\nindication of a lesion or abnormality \\nin the CT image?\\nNo abdominal organs show\\nany clear indications of lesions\\nor abnormalities.\\nCould you explain what this mass\\nin the MRI means for my health? \\nIs it very serious?\\nCertainly, the MRI shows a\\ndefined mass in your left nasal\\ncavity. There is no sign of the ...\\n4. Microsopy Comprehension\\n What is the purpose of the different \\nmembrane treatments used in this \\nstudy?\\nThe purpose of the different\\nmembrane treatments used…\\n5. OCT Comprehension\\n What is the purpose of comparing \\nthe OCT structure image and OCTA \\nimage with H&E histology?\\nTo confirm the histological position\\nof the obtained OCT brain images.\\n6. Fundus\\nComprehension\\n What specific findings or pathological \\nchanges can be observed in this \\nfundus image?\\nThe fundus image appears normal with\\nno noticeable signs of pathology…\\n7. Ultrasound\\nComprehension\\n What type of imaging technique \\nis used in this image?\\nThe image is a sagittal gray-\\nscale ultrasonographic…\\n1. CT2MRI\\nGeneration\\nI need a version of this CT representation \\nin MRI.\\nThe image has\\nbeen transformed\\ninto MRI.\\n2. MRI2CT\\nGeneration\\nTransform the MRI display into a \\nCT image.\\nHere is the CT\\nversion of the\\nMRI image.\\n3. Image Reconstruction\\nReconstruct the following \\nmedical images.\\nHere is the reconstructed\\nmedical image you need.\\n4. Super Resolution\\nCould you improve the quality\\nof this MRI image?\\nHere is the image with\\nimproved resolution.\\n5. Report-to-CXR\\nThe X-ray shows no \\npleural effusion or \\npneumothorax.\\nHere is the\\nchest X-ray\\nimage for\\nyou.\\nGen. Perf.\\nFigure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\\nunified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\\nplex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\\nAbstract\\nWe present HealthGPT, a powerful Medical Large Vision-\\nLanguage Model (Med-LVLM) that integrates medical vi-\\nsual comprehension and generation capabilities within a uni-\\nfied autoregressive paradigm. Our bootstrapping philosophy\\nis to progressively adapt heterogeneous comprehension and\\ngeneration knowledge to pre-trained large language mod-\\nels (LLMs). This is achieved through a novel heterogeneous\\nlow-rank adaptation (H-LoRA) technique, which is com-\\nplemented by a tailored hierarchical visual perception ap-\\nproach and a three-stage learning strategy. To effectively\\nlearn the HealthGPT, we devise a comprehensive medi-\\ncal domain-specific comprehension and generation dataset\\ncalled VL-Health. Experimental results demonstrate ex-\\nceptional performance and scalability of HealthGPT in\\nmedical visual unified tasks. Our project can be accessed at\\nhttps://github.com/DCDmllm/HealthGPT.\\n1 Introduction\\nLarge Vision-Language Models (LVLMs) (Liu et al. 2023;\\nOpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\\ndemonstrated outstanding open-world visual comprehension\\nand reasoning abilities through language-based interactive\\ndialogue over the past years, simultaneously opening up\\nnew opportunities for applications in specialized domains.\\n1\\narXiv:2502.09838v3  [cs.CV]  21 Feb 2025', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='9582fe07-8d83-4ad4-a92e-e9058d05b328', embedding=None, metadata={'page_label': '2', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Specifically, recent studies (Li et al. 2024a; Tu et al. 2024)\\nhave utilized pre-trained large language models (LLMs) and\\nvisual instruction data to build interactive diagnostic tools\\nand treatment planning systems, revealing the immense po-\\ntential of LVLMs in medical scenarios. However, these stud-\\nies primarily concentrate on visual comprehension tasks that\\nproduce text-based outputs, such as medical visual ques-\\ntion answering (Li et al. 2024a) or report generation (Nath\\net al. 2024), and deficient the “drawing” capability needed\\nfor medical visual generation. In practice, integrating visual\\ncomprehension and generation can significantly enhance the\\nmultifunctionality of medical LVLMs.\\nRecent studies have increasingly focused on developing\\nunified LVLMs capable of comprehending and generating\\ncontent across diverse visual modalities. Earlier approaches\\npredominantly utilized continuous visual tokens fed into\\nLLMs, using the LLMs themselves as conditional genera-\\ntors for external generative models (Ge et al. 2024; Wu et al.\\n2023; Dong et al. 2023). More recent research has explored\\nthe use of discrete visual tokens for image representation and\\ngeneration within a fully autoregressive framework (Team\\n2024; Wang et al. 2024a; Xie et al. 2024). These meth-\\nods not only enhance controllability but also demonstrate\\nearly success in open-world, any-to-any tasks, highlighting\\nthe preliminary potential of a unified autoregressive learning\\nparadigm in multi-modal tasks.\\nWhile unified LVLMs have achieved initial success in\\ngeneral scenarios, such a unified framework remains under-\\nexplored in the medical domain. Adapting the aforemen-\\ntioned general unified model paradigm to the medical do-\\nmain presents two major challenges: (i) High-scale and\\n-quality Data Limitations . Open-world models necessi-\\ntate extensive pre-training on billions or even more diverse,\\nmulti-modal data samples for comprehension and genera-\\ntion tasks (Lu et al. 2024; Team 2024). However, the ac-\\ncessible medical data significantly lacks in scale and qual-\\nity compared to natural multi-modal datasets. Its special-\\nized and domain-specific characteristics make it challenging\\nto develop a unified medical model from scratch. (ii) Con-\\nflicts between Comprehension and Generation . Compre-\\nhension tasks often strip away visual details to focus on\\nabstraction, while generation tasks require detailed preser-\\nvation, making tokens sensitive to all visual alterations. As\\nshown in Figure 2, which features experiments conducted on\\nmedical images, the performance in comprehension (or gen-\\neration) tasks steadily decreases as the proportion of genera-\\ntion (or comprehension) data increases, and vice versa. This\\nhighlights a dilemma in autoregressive multi-modal training,\\nstemming from the need to maintain consistency between\\npre- and post-LVLMs. While some methods have explored\\nmutual enhancement between comprehension and genera-\\ntion (Pan et al. 2024; Tong et al. 2024), improvements still\\nexhibit diminishing returns, with performance degradation\\nremaining a significant issue.\\n(a) (b)\\nFigure 2: With a fixed amount of comprehension (genera-\\ntion) data, increasing the proportion of the other type leads\\nto significant performance degradation.\\nTo tackle the aforementioned challenges, we propose\\nHealthGPT (see Figure 1) , which progressively adapts a\\npre-trained LLM as an unified medical multi-modal model\\nwith a small amount of visual instruction data. We de-\\nvise innovative Parameter-Efficient Fine-Tuning (PEFT) ap-\\nproach (Ding et al. 2023), calledHeterogeneous Low-Rank\\nAdaptation (H-LoRA), which decouples the learning pro-\\ncess of LVLMs for comprehension and generation tasks. In-\\nspired by the plug-and-play nature of LoRA (Hu et al. 2021),\\nH-LoRA enables the model to store heterogeneous compre-\\nhension and generation knowledge in independent “plug-\\nins”, thus avoiding joint optimization issues caused by con-\\nflicts between comprehension and generation tasks. In addi-\\ntion, we also consider the variety of sub-tasks among com-\\nprehension or generation tasks. Qualitative research high-\\nlights the limitations of a single LoRA in handling multi-\\ndimensional task scenarios, mainly due to catastrophic for-\\ngetting and interference (Liu et al. 2024d; Lin et al. 2024).\\nTo address this, we draw on the concept of Mixture of Ex-\\nperts (MoE) (Masoudnia and Ebrahimpour 2014) and in-\\ntroduce LoRA experts. The aim is to dynamically transfer\\ntask-shared knowledge to adapt to downstream tasks. Unlike\\nMoELoRA (Luo et al. 2024a), H-LoRA employs reversible\\nmatrix block multiplication to combine LoRA experts, sig-\\nnificantly reducing the overhead of multiple matrix multi-\\nplications. Notably, when using four experts, it requires\\nonly 67% of the MoELoRA training time.\\nTo effectively leverage H-LoRA inHealthGPT, we fur-\\nther introduce a Hierarchical Visual Perception (HVP)\\nand devise a correspondingThree-stage Learning Strategy\\n(TLS). HVP: we separate visual details learning from Vi-\\nsion transformer (ViT) for comprehension and generation.\\nAs is widely recognized, the ViT encodes visual concepts\\nwith increasing abstraction, generally, becoming finer as we\\nprogress over levels (Vig 2019). Thus, we maintain the vi-\\nsual features of the anterior and posterior layers to accom-\\nmodate the differing requirements for visual granularity in\\ncomprehension and generation tasks while preventing po-\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='009972f7-4fc4-436e-82c5-cbb05e68c6fa', embedding=None, metadata={'page_label': '3', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tential task interference. TLS: In the first and second stages,\\ngiven the heterogeneity between comprehension and gener-\\nation tasks, we first train H-LoRA plugins for HealthGPT\\nto incorporate both medical comprehension and generation\\nknowledge, thus endowing the LLMs with capabilities for\\nvision-language alignment and vision-to-vision reconstruc-\\ntion. Additionally, through minimal mixed-task training, we\\nbuilt fusion embedding layers and output heads that merge\\ntext and visual tokens, establishing a unified LVLM founda-\\ntion for visual instruction fine-tuning. In the third stage, by\\nonly training the H-LoRA plugins, HealthGPT is able to\\nrapidly adapt to a wide range of downstream medical tasks,\\ncovering various types of medical comprehension and gen-\\neration tasks.\\nTo effectively implement our approach, we have cu-\\nrated a dataset for training unified medical LVLMs, called\\nVL-Health, including seven comprehension tasks and five\\ngeneration tasks (Figure 1). Through quantitative analysis\\nand validation on multi-modal tasks, the results demonstrate\\nthat HealthGPT is capable of unifying medical multi-\\nmodal abilities in data-constrained scenarios, achieving per-\\nformance comparable to or better than existing state-of-the-\\nart (SOTA) models across multiple metrics. Overall, the\\nmain contributions of this paper are summarized as follows:\\n• Unified Med-LVLM. We introduce HealthGPT,\\nwhich, to the best of our knowledge, is the first unified\\nframework for multi-modal comprehension and genera-\\ntion in complex medical scenarios.\\n• Effective Learning Paradigm. We present H-LoRA, an\\noptimized multi-LoRA PEFT architecture based on task-\\ngated decoupling, is designed to effectively mitigate data\\nconflict issues.\\n• Holistic Training Dataset. We curated VL-Health, a\\ncomprehensive dataset designed for both comprehension\\nand generation tasks.\\n• Superior Downstream Improvements : Extensive ex-\\nperiments are conducted and the results confirm\\nHealthGPT’s effectiveness in medical vision-language\\ncomprehension and generation.\\n2 Related Work\\nMedical Vision Large Language Models. Recently, medi-\\ncal vision large language models (Med-VLLMs) have made\\nsignificant progress, demonstrating excellent performance\\nin understanding medical images and responding to human\\nqueries based on these images (Zhou et al. 2023; Tian et al.\\n2023). XrayGPT (Thawkar et al. 2023) combines a med-\\nical visual encoder (MedClip) (Wang et al. 2022) with a\\nfine-tuned LLM , using a simple linear transformation layer\\nto achieve alignment between visual and textual informa-\\ntion, significantly enhancing the understanding of medical\\nimages. On this basis, LLaV A-Med (Li et al. 2024b) fur-\\nther enhances visual-text alignment in medical contexts by\\nselecting high-quality image-text pairs from PubMed pa-\\npers and synthesized VQA datasets. BiomedGPT (Luo et al.\\n2024b) employs a BERT-style encoder and GPT-style de-\\ncoder architecture, pre-trained on interdisciplinary datasets.\\nCompared to commercial models like Med-PaLM (Singhal\\net al. 2023), BiomedGPT significantly reduces model size\\nwhile maintaining superior performance. However, issues\\nof language adaptability and dataset specificity still remain.\\nTo address these, HuatuoGPT-Vision (Chen et al. 2024a)\\nintroduces the PubMedVision dataset, which contains 1.3\\nmillion high-quality medical samples, significantly improv-\\ning the model’s adaptability across diverse medical applica-\\ntions. However, current Med-VLLMs mainly focus on med-\\nical comprehension and lack the capability for the medical\\nvision-language generation.\\nUnified Visual Comprehension and Generation Mod-\\nels. Recent research has increasingly concentrated on cre-\\nating unified LVLMs that are adept at understanding and\\nproducing content across various visual modalities. NExT-\\nGPT (Wu et al. 2023) achieves perception and generation for\\narbitrary combinations of multi-modal inputs and outputs by\\naligning LLMs. Similarly, SEED (Ge et al. 2023), SEED-\\nX (Ge et al. 2024), and DreamLLM (Dong et al. 2023) em-\\nploy learnable queries and leverage next-token prediction to\\ngenerate visual tokens, providing conditional inputs to exter-\\nnal generation modules. Unlike these methods, which func-\\ntion as external conditioners, Unified-IO (Lu et al. 2022),\\nUnified-IO 2 (Lu et al. 2024), and Chameleon (Team 2024)\\ninternalize multi-modal generation tasks within a unified\\nTransformer architecture by extending multi-modal vocab-\\nularies, enabling direct generation based on next-token pre-\\ndiction. Building on this concept, Lumina-mGPT (Liu et al.\\n2024a) and ANOLE (Chern et al. 2024) further enhance the\\ngeneration capabilities of unified models using high-quality\\ndata, particularly improving the quality and flexibility of im-\\nage generation.\\n3 Preliminaries\\nLarge Vision-Language Models.The input to a LVLM typ-\\nically consists of an image ximg and a discrete text sequence\\nxtxt. The visual encoder Eimg converts the input image ximg\\ninto a sequence of visual tokens V = [ vi]Nv\\ni=1, while the\\ntext sequence xtxt is mapped into a sequence of text to-\\nkens T = [ ti]Nt\\ni=1 using an embedding function Etxt. The\\nLLM MLLM(·|θ) models the joint probability of the token\\nsequence U = {V, T }, which is expressed as:\\nPθ(R|U) =\\nNrY\\ni=1\\nPθ(ri|{U, r<i}), (1)\\nwhere R = [ri]Nr\\ni=1 is the text response sequence. The LVLM\\niteratively generates the next token ri based on r<i. The op-\\ntimization objective is to minimize the cross-entropy loss of\\nthe response R. It is worth noting that most LVLMs adopt\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='9d24e4e3-4794-421f-87d8-932b3548a4a9', embedding=None, metadata={'page_label': '4', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3: The HealthGPT architecture integrates hierarchical visual perception and H-LoRA, employing a task-specific hard\\nrouter to select visual features and H-LoRA plugins, ultimately generating outputs with an autoregressive manner.\\na design paradigm based on ViT, alignment adapters, and\\npre-trained LLMs(Liu et al. 2023, 2024b), enabling quick\\nadaptation to downstream tasks.\\nVQGAN. VQGAN (Esser, Rombach, and Ommer 2021)\\nemploys latent space compression and indexing mechanisms\\nto effectively learn a complete discrete representation of im-\\nages. VQGAN first maps the input imageximg to a latent rep-\\nresentation z = E(x) through a encoder E. Then, the latent\\nrepresentation is quantized using a codebookZ = {zk}K\\nk=1,\\ngenerating a discrete index sequence I = [im]N\\nm=1, where\\nim ∈ Zrepresents the quantized code index:\\nI = Quantize(z|Z) = arg min\\nzk∈Z\\n∥z − zk∥2. (2)\\nIn our approach, the discrete index sequence I serves as\\na supervisory signal for the generation task, enabling the\\nmodel to predict the index sequence ˆI from input conditions\\nsuch as text or other modality signals. Finally, the predicted\\nindex sequence ˆI is upsampled by the VQGAN decoder G,\\ngenerating the high-quality image ˆximg = G(ˆI).\\nLow Rank Adaptation. LoRA(Hu et al. 2021) effectively\\ncaptures the characteristics of downstream tasks by intro-\\nducing low-rank adapters. The core idea is to decompose\\nthe bypass weight matrix ∆W ∈ Rdin×dout\\ninto two low-\\nrank matrices {A ∈ Rdin×r, B ∈ Rr×dout\\n}, where r ≪\\nmin{din, dout}, significantly reducing learnable parameters.\\nThe output with the LoRA adapter for the input x is then\\ngiven by:\\nh = xW0 + αx∆W/r = xW0 + αxAB/r, (3)\\nwhere matrix A is initialized with a Gaussian distribution,\\nwhile the matrixB is initialized as a zero matrix. The scaling\\nfactor α/r controls the impact of ∆W on the model.\\n4 HealthGPT\\n4.1 Unified Autoregressive Generation.\\nHealthGPT (Figure 3) utilizes a discrete token representa-\\ntion that covers both text and visual outputs, unifying visual\\ncomprehension and generation as an autoregressive task. For\\ncomprehension, Mllm receives the input joint sequence U\\nand outputs a series of text token R = [ r1, r2, . . . , rNr ],\\nwhere ri ∈ Vtxt, and Vtxt represents the LLM’s vocabulary:\\nPθ(R | U) =\\nNrY\\ni=1\\nPθ(ri | U, r<i). (4)\\nFor generation, Mllm first receives a special start token\\n⟨START IMG⟩, then generates a series of tokens corre-\\nsponding to the VQGAN indices I = [ i1, i2, . . . , iNi ],\\nwhere ij ∈ Vvq, and Vvq represents the index range of VQ-\\nGAN. Upon completion of generation, the LLM outputs an\\nend token ⟨END IMG⟩:\\nPθ(I | U) =\\nNiY\\nj=1\\nPθ(ij | U, i<j). (5)\\nFinally, the generated index sequence I is fed into the de-\\ncoder G, which reconstructs the target image ˆximg = G(I).\\n4.2 Hierarchical Visual Perception\\nGiven the differences in visual perception between compre-\\nhension and generation tasks—where the former focuses on\\nabstract semantics and the latter emphasizes complete se-\\nmantics—we employ ViT to compress the image into dis-\\ncrete visual tokens at multiple hierarchical levels. Specif-\\nically, the image is converted into a series of features\\n{f1, f2, . . . , fL} as it passes through L ViT blocks.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='b8f18ed1-d6d6-45a9-b396-528ddbe7acea', embedding=None, metadata={'page_label': '5', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='To address the needs of various tasks, the hidden states\\nare divided into two types: (i) Concrete-grained features\\nFCon = {f1, f2, . . . , fk}, k < L, derived from the shal-\\nlower layers of ViT, containing sufficient global features,\\nsuitable for generation tasks; (ii) Abstract-grained features\\nFAbs = {fk+1, fk+2, . . . , fL}, derived from the deeper\\nlayers of ViT, which contain abstract semantic information\\ncloser to the text space, suitable for comprehension tasks.\\nThe task type T (comprehension or generation) deter-\\nmines which set of features is selected as the input for the\\ndownstream large language model:\\nFimg\\nT =\\n(\\nFCon, if T = generation task\\nFAbs, if T = comprehension task (6)\\nWe integrate the image featuresFimg\\nT and text featuresT into\\na joint sequence through simple concatenation, which is then\\nfed into the LLM Mllm for autoregressive generation.\\n4.3 Heterogeneous Knowledge Adaptation\\nWe devise H-LoRA, which stores heterogeneous knowledge\\nfrom comprehension and generation tasks in separate mod-\\nules and dynamically routes to extract task-relevant knowl-\\nedge from these modules. At the task level, for each task type\\nT, we dynamically assign a dedicated H-LoRA submodule\\nθT , which is expressed as:\\nR = MLLM(U|θ, θT ), θ T = {AT , BT , RT\\nouter}. (7)\\nAt the feature level for a single task, H-LoRA integrates the\\nidea of Mixture of Experts (MoE) (Masoudnia and Ebrahim-\\npour 2014) and designs an efficient matrix merging and rout-\\ning weight allocation mechanism, thus avoiding the signif-\\nicant computational delay introduced by matrix splitting in\\nexisting MoELoRA (Luo et al. 2024a). Specifically, we first\\nmerge the low-rank matrices (rank = r) of k LoRA experts\\ninto a unified matrix:\\nAmerged, Bmerged = Concat({Ai}k\\n1 ), Concat({Bi}k\\n1 ), (8)\\nwhere Amerged ∈ Rdin×rk and Bmerged ∈ Rrk×dout\\n. The\\nk-dimension routing layer generates expert weights W ∈\\nRtoken num×k based on the input hidden state x, and these are\\nexpanded to Rtoken num×rk as follows:\\nWexpanded = αkW/r ⊗ 1r, (9)\\nwhere ⊗ denotes the replication operation. The overall out-\\nput of H-LoRA is computed as:\\nOH-LoRA = (xAmerged ⊙ Wexpanded)Bmerged, (10)\\nwhere ⊙ represents element-wise multiplication. Finally, the\\noutput of H-LoRA is added to the frozen pre-trained weights\\nto produce the final output:\\nO = xW0 + OH-LoRA. (11)\\n900\\n800\\n700\\n600\\n500\\n400\\n300\\n200\\n100\\n0\\nComp. Gen.\\n(a) (b)\\n783K765K\\n（K）\\nFigure 4: Data statistics of VL-Health.\\n4.4 Training Pipeline\\n1st Stage: Multi-modal Alignment. In the first stage, we\\ndesign separate visual adapters and H-LoRA submodules for\\nmedical unified tasks. For the medical comprehension task,\\nwe train abstract-grained visual adapters using high-quality\\nimage-text pairs to align visual embeddings with textual\\nembeddings, thereby enabling the model to accurately de-\\nscribe medical visual content. During this process, the pre-\\ntrained LLM and its corresponding H-LoRA submodules\\nremain frozen. In contrast, the medical generation task re-\\nquires training concrete-grained adapters and H-LoRA sub-\\nmodules while keeping the LLM frozen. Meanwhile, we ex-\\ntend the textual vocabulary to include multimodal tokens,\\nenabling the support of additional VQGAN vector quanti-\\nzation indices. The model trains on image-VQ pairs, en-\\ndowing the pre-trained LLM with the capability for image\\nreconstruction. This design ensures pixel-level consistency\\nof pre- and post-LVLM. The processes establish the initial\\nalignment between the LLM’s outputs and the visual inputs.\\n2nd Stage: Heterogeneous H-LoRA Plugin Adaptation.\\nThe submodules of H-LoRA share the word embedding\\nlayer and output head but may encounter issues such as\\nbias and scale inconsistencies during training across dif-\\nferent tasks. To ensure that the multiple H-LoRA plugins\\nseamlessly interface with the LLMs and form a unified base,\\nwe fine-tune the word embedding layer and output head us-\\ning a small amount of mixed data to maintain consistency\\nin the model weights. Specifically, during this stage, all H-\\nLoRA submodules for different tasks are kept frozen, with\\nonly the word embedding layer and output head being op-\\ntimized. Through this stage, the model accumulates foun-\\ndational knowledge for unified tasks by adapting H-LoRA\\nplugins.\\n3rd Stage: Visual Instruction Fine-Tuning. In the third\\nstage, we introduce additional task-specific data to fur-\\nther optimize the model and enhance its adaptability to\\ndownstream tasks such as medical visual comprehension\\n(e.g., medical QA, medical dialogues, and report generation)\\nor generation tasks (e.g., super-resolution, denoising, and\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='6d5a0570-973b-42f7-806f-dd3a7431788c', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Comparison of HealthGPT with other LVLMs and unified multi-modal models on medical visual comprehension\\ntasks. Bold and underlined text indicates the best performance and second-best performance, respectively.\\nVQA-RAD↑ SLAKE↑ PathVQA↑Type Model # Params Medical\\nLVLM close all close all close all\\nMMMU\\n-Med ↑ OMVQA↑ Avg.↑\\nComp. Only\\nMed-Flamingo 8.3B ✓ 58.6 43.0 47.0 25.5 61.9 31.3 28.7 34.9 41.4\\nLLaV A-Med 7B ✓ 60.2 48.1 58.4 44.8 62.3 35.7 30.0 41.3 47.6\\nHuatuoGPT-Vision 7B ✓ 66.9 53.0 59.8 49.1 52.9 32.0 42.0 50.0 50.7\\nBLIP-2 6.7B ✗ 43.4 36.8 41.6 35.3 48.5 28.8 27.3 26.9 36.1\\nLLaV A-v1.5 7B ✗ 51.8 42.8 37.1 37.7 53.5 31.4 32.7 44.7 41.5\\nInstructBLIP 7B ✗ 61.0 44.8 66.8 43.3 56.0 32.3 25.3 29.0 44.8\\nYi-VL 6B ✗ 52.6 42.1 52.4 38.4 54.9 30.9 38.0 50.2 44.9\\nInternVL2 8B ✗ 64.9 49.0 66.6 50.1 60.0 31.9 43.3 54.5 52.5\\nLlama-3.2 11B ✗ 68.9 45.5 72.4 52.1 62.8 33.6 39.3 63.2 54.7\\nComp. & Gen.\\nShow-o 1.3B ✗ 50.6 33.9 31.5 17.9 52.9 28.2 22.7 45.7 42.6\\nUnified-IO 2 7B ✗ 46.2 32.6 35.9 21.9 52.5 27.0 25.3 33.0 33.8\\nJanus 1.3B ✗ 70.9 52.8 34.7 26.9 51.9 27.9 30.0 26.8 33.5\\nHealthGPT-M3 3.8B ✓ 73.7 55.9 74.6 56.4 78.7 39.7 43.3 68.5 61.3\\nHealthGPT-L14 14B ✓ 77.7 58.3 76.4 64.5 85.9 44.4 49.2 74.4 66.4\\nTable 2: The experimental results for the four modality conversion tasks.\\nCT to MRI (Brain) CT to MRI (Pelvis) MRI to CT (Brain) MRI to CT (Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\npix2pix 71.09 32.65 36.85 59.17 31.02 51.91 78.79 33.85 28.33 72.31 32.98 36.19\\nCycleGAN 54.76 32.23 40.56 54.54 30.77 55.00 63.75 31.02 52.78 50.54 29.89 67.78\\nBBDM 71.69 32.91 34.44 57.37 31.37 48.06 86.40 34.12 26.61 79.26 33.15 33.60\\nVmanba 69.54 32.67 36.42 63.01 31.47 46.99 79.63 34.12 26.49 77.45 33.53 31.85\\nDiffMa 71.47 32.74 35.77 62.56 31.43 47.38 79.00 34.13 26.45 78.53 33.68 30.51\\nHealthGPT-M3 79.38 33.03 33.48 71.81 31.83 43.45 85.06 34.40 25.49 84.23 34.29 27.99\\nHealthGPT-L14 79.73 33.10 32.96 71.92 31.87 43.09 85.31 34.29 26.20 84.96 34.14 28.13\\nmodality conversion). Notably, by this stage, the word em-\\nbedding layer and output head have been fine-tuned, only the\\nH-LoRA modules and adapter modules need to be trained.\\nThis strategy significantly improves the model’s adaptability\\nand flexibility across different tasks.\\n5 Experiments\\n5.1 Data and Experimental Setup\\nData Details. We curate VL-Health dataset (see Fig-\\nure 4). For medical visual comprehension, we leverage\\nmultiple medical-specific datasets, including PubMedVi-\\nsion (Chen et al. 2024a), LLaV A-Med (Li et al. 2024b),\\nPathVQA (He et al. 2020), MIMIC-CXR-VQA (Bae et al.\\n2024), SLAKE (Liu et al. 2021), and VQA-RAD (Lau\\net al. 2018). Additionally, we incorporate high-quality open-\\nworld data from LLaV A-1.5 (Liu et al. 2024b) to preserve\\nthe model’s general knowledge and instruction-following\\ncapabilities. For generation tasks, we construct a recon-\\nstruction dataset based on LLaV A-558k (Liu et al. 2024b),\\nand also explore two key tasks in personalized medical\\nimage enhancement—super-resolution and modality con-\\nversion—using the IXI (Davies et al. 2014) and Syn-\\nthRAD2023 (Thummerer et al. 2023) datasets. Detailed data\\nselection and instruction templates are in the Appendix.\\nModel Details. We select CLIP-L/14 (Radford et al. 2021)\\nas the visual encoder and used the hidden states of its\\nsecond and penultimate layers as concrete-grained and\\nabstract-grained features for model’s dynamic hierarchical\\nvisual perception. Drawing on the successful experiences of\\nLLaV A, we employ a MLP to align the multi-modal fea-\\nture embeddings. We choose the parameter-efficient phi-3-\\nmini (Abdin et al. 2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al. 2023), LLaV A-Med (Li et al.\\n2024b), HuatuoGPT-Vision (Chen et al. 2024a)) as well\\nas recent open-world LVLMs (e.g., BLIP-2 (Li et al.\\n2023b), LLaV A-v1.5 (Liu et al. 2024b), InstructBLIP (Dai\\net al. 2023), Yi-VL (Young et al. 2024), InternVL2 (Chen\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='de4841da-cca7-4622-af87-3588becbeae1', embedding=None, metadata={'page_label': '7', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 3: Comparison results of super-resolution task.\\nModel SSIM↑ PSNR↑ MSE↓ LPIPS↓\\nSRGAN 71.34 32.01 41.27 24.50\\nDASR 71.57 32.34 38.25 19.17\\nReal-ESRGAN 67.30 31.87 42.57 20.64\\nLIIF 73.27 32.13 40.14 22.93\\nBSRGAN 69.97 31.97 41.52 28.72\\nHealthGPT-M3 78.19 32.76 34.47 12.02\\nHealthGPT-L14 77.94 32.71 35.19 12.43\\nFigure 5: Performance comparison of LoRA, MoELoRA,\\nand H-LoRA under different rank settings.\\net al. 2024b), Llama-3.2 (Dubey et al. 2024)). Addition-\\nally, we test several SOTA unified visual comprehension\\nand generation models, including Show-o (Xie et al. 2024),\\nUnified-IO 2 (Lu et al. 2024), and Janus (Wu et al.\\n2024). The experimental results are shown in Table 1, with\\nthe following key observations: (i) SOTA Results Com-\\npared with LVLMs: In medical visual comprehension\\ntasks, HealthGPT demonstrates superior performance,\\nsignificantly outperforming both medical-specific models\\n(e.g., HuatuoGPT-Vision) and general-purpose models (e.g.,\\nLlama-3.2). (ii) Surpassing Current Unified LVLMs: De-\\nspite being trained on billions of data points, unified mod-\\nels still exhibit poor generalization performance in medi-\\ncal visual comprehension. For instance, Unified-IO 2 scored\\nonly 33.8. In contrast,HealthGPT-M3, with only 3.8B pa-\\nrameters, scored 61.3 on the medical multi-modal unified\\ntask, significantly outperforming existing unified models in\\nmedical downstream scenarios. (iii) Stable Improvement\\nwith Large Base Model: Our method demonstrates excel-\\nlent scalability, with HealthGPT-L14 achieving a score\\nof 66.4 in the larger model configuration. This result signif-\\nicantly outperforms all other models, highlighting the effec-\\ntiveness of scaling up the base model for enhanced perfor-\\nmance in medical tasks.\\nGeneration. We study three key tasks in medical imag-\\ning. (i) Modality Conversion: In this task, we focus on\\nthe conversion between CT and MRI modalities for the\\nbrain and pelvic regions, designing four specific sub-tasks.\\nAll comparative models (Pix2Pix (Isola et al. 2017), Cy-\\ncleGAN (Zhu et al. 2017), BBDM (Li et al. 2023a),\\n(%)\\n(%)\\n（a） （b）\\nFigure 6: The loss visualization (a) and performance com-\\nparison (b) with respect to different visual perceptions.\\nVmamba (Liu et al. 2024e), and DiffMa (Wang et al.\\n2024b)) trained a separate model for each sub-task, while\\nHealthGPT unify all tasks into a single training process.\\nThe experimental results, shown in Table 11, demonstrate\\nthat our approach outperforms other methods across multi-\\nple evaluation metrics. For instance, in the CT2MRI-Brain\\ntask, HealthGPT-M3 achieves an SSIM of 79.38, signif-\\nicantly surpassing traditional methods like Pix2Pix (71.09)\\nand the recent DiffMa (71.47). (ii) Super-Resolution: We\\nconduct 4× super-resolution experiments on the IXI dataset,\\nwith the results presented in Table 3. Notably, most exist-\\ning methods fail to fully leverage the prior knowledge of key\\nstructures in medical images, resulting in significant short-\\ncomings in detail recovery. In contrast, our method signif-\\nicantly mitigates this issue. Specifically, HealthGPT-M3\\nexcels in key metrics such as SSIM, PSNR, and ISE, achiev-\\ning scores of 78.19, 32.76, and 34.47, respectively. Ad-\\nditionally, HealthGPT-M3 achieves the lowest score of\\n12.34, further validating its exceptional performance in hu-\\nman visual perception. (iii) Reconstruction: We compare\\nHealthGPT-M3 with unified models with reconstruction\\ncapabilities, such as Unified-IO 2 and SEED-X. The results\\nshow that our approach performs better controllability for vi-\\nsual reconstruction. We also trainHealthGPT-L14 with a\\nsimilar number of trainable parameters to the M3 version.\\nHence, the similar performance between the two models\\nmeets our expectations. Details are in the Appendix.\\n5.3 In-Depth Study\\nEffect of Heterogeneous Low-Rank Adaptation.H-LoRA\\nprovides an optimized multi-LoRA architecture for multi-\\ntask learning. We conduct extensive validation of this struc-\\nture, with results presented in Table 4, comparing the per-\\nformance of LoRA, MoELoRA, and H-LoRA in medical\\nunified comprehension and generation tasks. In the majority\\nof comprehension tasks and all generation tasks, H-LoRA\\ndemonstrates superior performance, particularly in the Om-\\nniMedVQA benchmark, where it improved from 64.90 to\\n68.50. Notably, despite some applications of MoELoRA in\\ncertain scenarios, it do not show advantages in this task and\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='f3f31dff-a830-4267-9c9a-cf4c96410395', embedding=None, metadata={'page_label': '8', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 4: We present the performance and speed differences of LoRA, MoELoRA (n=4), and H-LoRA (n=4) on medical visual\\ncomprehension and generation tasks.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQAModel\\nclose all close all close all\\nMMMU\\n-Med OMVQA RECOM MTRANS SR\\nTraining\\nTime\\nHealthGPT w/\\n+LoRA 71.3 57.2 70.0 53.4 76.4 38.6 41.30 65.10 62.67 59.99 65.88 1.00×\\n+MoELoRA 72.5 57.2 66.4 52.4 73.2 36.0 39.30 64.90 67.31 59.76 65.91 1.49×\\n+H-LoRA 73.7 55.9 74.6 56.4 78.7 39.7 43.30 68.50 67.69 60.30 66.14 1.00×\\nTable 5: Comparison between the H-LoRA-based Three-Stage Learning Strategy and the mixed-training approach.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQA CT MRITraining Strategy\\nclose all close all close all\\nMMMU\\n-Med OMVQA Brain Pelvis Brain Pelvis\\nMixed-Training 56.6 37.9 45.0 32.9 65.7 33.6 44.0 48.9 65.64 62.75 56.61 50.77HealthGPT w/ 3-stage-Training 72.5 55.2 77.9 59.6 79.7 49.0 42.7 68.5 70.84 72.99 65.26 61.33\\nFigure 7: Case study of report-to-CXR under different instructions. (a) shows a normal CXR image for comparison. (b) and (c)\\nillustrate generated cases with varying severity and affected regions. The graffiti areas indicate abnormal conditions.\\nhad a training time approximately 50% longer than LoRA.\\nFigure 5 illustrates the performance of the three PEFT meth-\\nods in medical visual comprehension and generation tasks\\nacross different ranks, with H-LoRA consistently outper-\\nforming the other methods in all scenarios, demonstrating\\nsignificant advantages in handling diverse tasks.\\nDifferent Learning Strategy. We propose a three-stage\\nlearning strategy for H-LoRA that decouples comprehension\\nand generation tasks. Unlike methods that train both tasks\\nsimultaneously, our approach reduces performance degra-\\ndation from task conflicts (see Table 5). In the medical vi-\\nsual comprehension task, mixed training causes catastrophic\\nforgetting and degrades visual reconstruction, whereas our\\nstrategy effectively uses the medical embedding knowledge\\nin pre-trained LLMs to mitigate these conflicts. Meanwhile,\\nwe examine how fusing heterogeneous H-LoRA plugins in\\nthe second training stage results in minimal performance\\ndegradation. Detailed results are in the Appendix.\\nHierarchical Visual Perception Analysis. We conduct an\\nablation analysis on visual perceptual inputs for comprehen-\\nsion and generation tasks. Figure 6 shows that comprehen-\\nsion tasks converge more efficiently with abstract-grained\\ninputs, while generation tasks perform better with concrete-\\ngrained inputs. This highlights the importance of the hier-\\narchical visual perception we propose, suggesting that tai-\\nloring visual inputs for specific tasks at different hierarchies\\ncan significantly improve efficiency.\\nReport-to-CXR Task. We further explore the medical im-\\nage generation task without reference images, using a small\\namount of MIMIC-CXR data (Johnson et al. 2019) for in-\\nstruction fine-tuning. Figure 7 annotates images with vary-\\ning injury degrees and locations, comparing them to healthy\\nCXR images. We observe thatHealthGPT effectively gen-\\nerates CXR images based on the instructions, showcasing its\\npotential in healthcare education and auxiliary diagnosis.\\n6 Conclusion\\nIn this paper, we introduceHealthGPT, a Med-LVLM that\\nunifies medical vision-language comprehension and gen-\\neration through a novel heterogeneous knowledge adap-\\ntation approach. Experimental results demonstrate that\\nHealthGPT achieves significant performance improve-\\nments across multiple medical comprehension and genera-\\ntion tasks, showcasing its potential for healthcare applica-\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='d62a7ebd-46e5-4e42-b68a-23024ada525f', embedding=None, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tions.\\nReferences\\nAbdin, M.; Aneja, J.; Behl, H.; Bubeck, S.; Eldan, R.; Gu-\\nnasekar, S.; Harrison, M.; Hewett, R. J.; Javaheripi, M.;\\nKauffmann, P.; et al. 2024. Phi-4 technical report. arXiv\\npreprint arXiv:2412.08905.\\nBae, S.; Kyung, D.; Ryu, J.; Cho, E.; Lee, G.; Kweon, S.;\\nOh, J.; JI, L.; Chang, E.; Kim, T.; et al. 2024. MIMIC-Ext-\\nMIMIC-CXR-VQA: A Complex, Diverse, And Large-Scale\\nVisual Question Answering Dataset for Chest X-ray Images.\\nChen, J.; Gui, C.; Ouyang, R.; Gao, A.; Chen, S.; Chen,\\nG. H.; Wang, X.; Zhang, R.; Cai, Z.; Ji, K.; et al.\\n2024a. Huatuogpt-vision, towards injecting medical visual\\nknowledge into multimodal llms at scale. arXiv preprint\\narXiv:2406.19280.\\nChen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong,\\nW.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024b. How far are we to\\ngpt-4v? closing the gap to commercial multimodal models\\nwith open-source suites. arXiv preprint arXiv:2404.16821.\\nChern, E.; Su, J.; Ma, Y .; and Liu, P. 2024. Anole:\\nAn open, autoregressive, native large multimodal mod-\\nels for interleaved image-text generation. arXiv preprint\\narXiv:2407.06135.\\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\\nGeneral-purpose Vision-Language Models with Instruction\\nTuning. arXiv:2305.06500.\\nDavies, R. L.; Royston, P. A.; Leung, M. S.; Haider, M. E.\\nA. M. J.; Barkhof, S. G. A. L.; and B., P. E. T. M. 2014. The\\nIXI Dataset. Accessed: 2025-01-30.\\nDing, N.; Qin, Y .; Yang, G.; Wei, F.; Yang, Z.; Su, Y .;\\nHu, S.; Chen, Y .; Chan, C.-M.; Chen, W.; et al. 2023.\\nParameter-efficient fine-tuning of large-scale pre-trained\\nlanguage models. Nature Machine Intelligence, 5(3): 220–\\n235.\\nDong, R.; Han, C.; Peng, Y .; Qi, Z.; Ge, Z.; Yang, J.; Zhao,\\nL.; Sun, J.; Zhou, H.; Wei, H.; et al. 2023. Dreamllm:\\nSynergistic multimodal comprehension and creation. arXiv\\npreprint arXiv:2309.11499.\\nDubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.;\\nLetman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;\\net al. 2024. The llama 3 herd of models. arXiv preprint\\narXiv:2407.21783.\\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming trans-\\nformers for high-resolution image synthesis. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern\\nrecognition, 12873–12883.\\nGe, Y .; Ge, Y .; Zeng, Z.; Wang, X.; and Shan, Y . 2023. Plant-\\ning a seed of vision in large language model. arXiv preprint\\narXiv:2307.08041.\\nGe, Y .; Zhao, S.; Zhu, J.; Ge, Y .; Yi, K.; Song, L.; Li, C.;\\nDing, X.; and Shan, Y . 2024. Seed-x: Multimodal models\\nwith unified multi-granularity comprehension and genera-\\ntion. arXiv preprint arXiv:2404.14396.\\nHe, X.; Zhang, Y .; Mou, L.; Xing, E.; and Xie, P. 2020.\\nPathvqa: 30000+ questions for medical visual question an-\\nswering. arXiv preprint arXiv:2003.10286.\\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\\nof large language models. arXiv preprint arXiv:2106.09685.\\nHu, Y .; Li, T.; Lu, Q.; Shao, W.; He, J.; Qiao, Y .; and Luo,\\nP. 2024. Omnimedvqa: A new large-scale comprehensive\\nevaluation benchmark for medical lvlm. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 22170–22183.\\nIsola, P.; Zhu, J.-Y .; Zhou, T.; and Efros, A. A. 2017. Image-\\nto-image translation with conditional adversarial networks.\\nIn Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 1125–1134.\\nJohnson, A. E.; Pollard, T. J.; Greenbaum, N. R.; Lungren,\\nM. P.; Deng, C.-y.; Peng, Y .; Lu, Z.; Mark, R. G.; Berkowitz,\\nS. J.; and Horng, S. 2019. MIMIC-CXR-JPG, a large pub-\\nlicly available database of labeled chest radiographs. arXiv\\npreprint arXiv:1901.07042.\\nLau, J. J.; Gayen, S.; Ben Abacha, A.; and Demner-\\nFushman, D. 2018. A dataset of clinically generated visual\\nquestions and answers about radiology images. Scientific\\ndata, 5(1): 1–10.\\nLi, B.; Xue, K.; Liu, B.; and Lai, Y .-K. 2023a. Bbdm: Image-\\nto-image translation with brownian bridge diffusion models.\\nIn Proceedings of the IEEE/CVF conference on computer\\nvision and pattern Recognition, 1952–1961.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024a. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024b. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. Blip-2:\\nBootstrapping language-image pre-training with frozen im-\\nage encoders and large language models. In International\\nconference on machine learning, 19730–19742. PMLR.\\nLin, T.; Liu, J.; Zhang, W.; Li, Z.; Dai, Y .; Li, H.; Yu, Z.; He,\\nW.; Li, J.; Jiang, H.; et al. 2024. Teamlora: Boosting low-\\nrank adaptation with expert collaboration and competition.\\narXiv preprint arXiv:2408.09856.\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='f8441ba4-0487-4eb3-a2e0-19131d9ace1b', embedding=None, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Liu, B.; Zhan, L.-M.; Xu, L.; Ma, L.; Yang, Y .; and Wu,\\nX.-M. 2021. Slake: A semantically-labeled knowledge-\\nenhanced dataset for medical visual question answering. In\\n2021 IEEE 18th International Symposium on Biomedical\\nImaging (ISBI), 1650–1654. IEEE.\\nLiu, D.; Zhao, S.; Zhuo, L.; Lin, W.; Qiao, Y .; Li, H.; and\\nGao, P. 2024a. Lumina-mgpt: Illuminate flexible photore-\\nalistic text-to-image generation with multimodal generative\\npretraining. arXiv preprint arXiv:2408.02657.\\nLiu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2024b. Improved\\nbaselines with visual instruction tuning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 26296–26306.\\nLiu, H.; Li, C.; Li, Y .; Li, B.; Zhang, Y .; Shen, S.; and Lee,\\nY . J. 2024c. LLaV A-NeXT: Improved reasoning, OCR, and\\nworld knowledge. https://llava-vl.github.io/blog/2024-01-\\n30-llava-next/.\\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruc-\\ntion Tuning. In NeurIPS.\\nLiu, Q.; Wu, X.; Zhao, X.; Zhu, Y .; Xu, D.; Tian, F.; and\\nZheng, Y . 2024d. When moe meets llms: Parameter efficient\\nfine-tuning for multi-task medical applications. In Proceed-\\nings of the 47th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval, 1104–\\n1114.\\nLiu, Y .; Tian, Y .; Zhao, Y .; Yu, H.; Xie, L.; Wang, Y .; Ye,\\nQ.; and Liu, Y . 2024e. VMamba: Visual State Space Model.\\narXiv preprint arXiv:2401.10166.\\nLu, J.; Clark, C.; Lee, S.; Zhang, Z.; Khosla, S.; Marten, R.;\\nHoiem, D.; and Kembhavi, A. 2024. Unified-IO 2: Scaling\\nAutoregressive Multimodal Models with Vision Language\\nAudio and Action. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , 26439–\\n26455.\\nLu, J.; Clark, C.; Zellers, R.; Mottaghi, R.; and Kembhavi,\\nA. 2022. Unified-io: A unified model for vision, language,\\nand multi-modal tasks. In The Eleventh International Con-\\nference on Learning Representations.\\nLuo, T.; Lei, J.; Lei, F.; Liu, W.; He, S.; Zhao, J.; and Liu,\\nK. 2024a. Moelora: Contrastive learning guided mixture of\\nexperts on parameter-efficient fine-tuning for large language\\nmodels. arXiv preprint arXiv:2402.12851.\\nLuo, Y .; Zhang, J.; Fan, S.; Yang, K.; Hong, M.; Wu, Y .;\\nQiao, M.; and Nie, Z. 2024b. Biomedgpt: An open multi-\\nmodal large language model for biomedicine. IEEE Journal\\nof Biomedical and Health Informatics.\\nMasoudnia, S.; and Ebrahimpour, R. 2014. Mixture of ex-\\nperts: a literature survey. Artificial Intelligence Review, 42:\\n275–293.\\nMoor, M.; Huang, Q.; Wu, S.; Yasunaga, M.; Dalmia, Y .;\\nLeskovec, J.; Zakka, C.; Reis, E. P.; and Rajpurkar, P. 2023.\\nMed-flamingo: a multimodal medical few-shot learner. In\\nMachine Learning for Health (ML4H), 353–367. PMLR.\\nNath, V .; Li, W.; Yang, D.; Myronenko, A.; Zheng, M.; Lu,\\nY .; Liu, Z.; Yin, H.; Law, Y . M.; Tang, Y .; et al. 2024. Vila-\\nm3: Enhancing vision-language models with medical expert\\nknowledge. arXiv preprint arXiv:2411.12915.\\nOpenAI. 2023. GPT-4V(ision) System Card. https://cdn.\\nopenai.com/papers/GPTV System Card.pdf.\\nPan, K.; Tang, S.; Li, J.; Fan, Z.; Chow, W.; Yan, S.;\\nChua, T.-S.; Zhuang, Y .; and Zhang, H. 2024. Auto-\\nEncoding Morph-Tokens for Multimodal LLM. arXiv\\npreprint arXiv:2405.01926.\\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\\net al. 2021. Learning transferable visual models from nat-\\nural language supervision. In International conference on\\nmachine learning, 8748–8763. PMLR.\\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\\net al. 2023. Large language models encode clinical knowl-\\nedge. Nature, 620(7972): 172–180.\\nTeam, C. 2024. Chameleon: Mixed-modal early-fusion\\nfoundation models. arXiv preprint arXiv:2405.09818.\\nThawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal,\\nH.; Anwer, R. M.; Khan, S.; Laaksonen, J.; and Khan,\\nF. S. 2023. Xraygpt: Chest radiographs summarization\\nusing medical vision-language models. arXiv preprint\\narXiv:2306.07971.\\nThummerer, A.; van der Bijl, E.; Galapon Jr, A.; Verhoeff,\\nJ. J.; Langendijk, J. A.; Both, S.; van den Berg, C. N. A.;\\nand Maspero, M. 2023. SynthRAD2023 Grand Challenge\\ndataset: Generating synthetic CT for radiotherapy. Medical\\nphysics, 50(7): 4664–4674.\\nTian, D.; Jiang, S.; Zhang, L.; Lu, X.; and Xu, Y . 2023. The\\nrole of large language models in medical image processing:\\na narrative review. Quantitative Imaging in Medicine and\\nSurgery, 14(1): 1108.\\nTong, S.; Fan, D.; Zhu, J.; Xiong, Y .; Chen, X.; Sinha, K.;\\nRabbat, M.; LeCun, Y .; Xie, S.; and Liu, Z. 2024. Meta-\\nMorph: Multimodal Understanding and Generation via In-\\nstruction Tuning. arXiv preprint arXiv:2412.14164.\\nTu, T.; Azizi, S.; Driess, D.; Schaekermann, M.; Amin, M.;\\nChang, P.-C.; Carroll, A.; Lau, C.; Tanno, R.; Ktena, I.; et al.\\n2024. Towards generalist biomedical AI. NEJM AI, 1(3):\\nAIoa2300138.\\nVig, J. 2019. A multiscale visualization of attention in the\\ntransformer model. arXiv preprint arXiv:1906.05714.\\nWang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y .; Wang, J.;\\nZhang, F.; Wang, Y .; Li, Z.; Yu, Q.; et al. 2024a. Emu3:\\nNext-token prediction is all you need. arXiv preprint\\narXiv:2409.18869.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='eafae00c-fd06-4349-839e-280d6fccf61c', embedding=None, metadata={'page_label': '11', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Wang, Z.; Wu, Z.; Agarwal, D.; and Sun, J. 2022. Medclip:\\nContrastive learning from unpaired medical images and text.\\narXiv preprint arXiv:2210.10163.\\nWang, Z.; Zhang, L.; Wang, L.; and Zhang, Z. 2024b. Soft\\nMasked Mamba Diffusion Model for CT to MRI Conver-\\nsion. arXiv preprint arXiv:2406.15910.\\nWu, C.; Chen, X.; Wu, Z.; Ma, Y .; Liu, X.; Pan, Z.; Liu, W.;\\nXie, Z.; Yu, X.; Ruan, C.; and Luo, P. 2024. Janus: Decou-\\npling Visual Encoding for Unified Multimodal Understand-\\ning and Generation. arXiv:2410.13848.\\nWu, S.; Fei, H.; Qu, L.; Ji, W.; and Chua, T.-S. 2023.\\nNext-gpt: Any-to-any multimodal llm. arXiv preprint\\narXiv:2309.05519.\\nXie, J.; Mao, W.; Bai, Z.; Zhang, D. J.; Wang, W.; Lin, K. Q.;\\nGu, Y .; Chen, Z.; Yang, Z.; and Shou, M. Z. 2024. Show-o:\\nOne single transformer to unify multimodal understanding\\nand generation. arXiv preprint arXiv:2408.12528.\\nYoung, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.; Zhang,\\nG.; Li, H.; Zhu, J.; Chen, J.; Chang, J.; et al. 2024.\\nYi: Open foundation models by 01. ai. arXiv preprint\\narXiv:2403.04652.\\nZhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li,\\nY .; Chen, S. S.; Zhou, P.; Liu, J.; et al. 2023. A survey of\\nlarge language models in medicine: Progress, application,\\nand challenge. arXiv preprint arXiv:2311.05112.\\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\\npaired image-to-image translation using cycle-consistent ad-\\nversarial networks. InProceedings of the IEEE international\\nconference on computer vision, 2223–2232.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='a271bd41-2149-4376-808e-f5dc9e5c8ed4', embedding=None, metadata={'page_label': '12', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix\\nThis is the Appendix for “HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation\\nvia Heterogeneous Knowledge Adaptation”. This Appendix is organized as follows:\\n• Section A presents the experimental implementation details, the training process of HealthGPT, and the specifics of\\nVL-Health.\\n• Section B systematically provides an analysis of Heterogeneous Low-Rank Adaptation.\\n• Section C shows supplementary experimental results to validate the effectiveness ofHealthGPT.\\nA Implementation Details\\nA.1 Model Details\\nWe employ CLIP-L/14 (Radford et al. 2021) as the visual feature extractor, extracting both shallow and deep features to serve as\\nvisual tokens. The model uses alignment adapters, implemented with two-layer MLPs, to align shallow features, representing\\nconcrete visual granularity, and deep features, representing abstract visual granularity. These visual tokens are concatenated\\nwith text tokens and input into the large language models (LLMs).\\nHealthGPT offers two versions: HealthGPT-M3 and HealthGPT-L14, which are based on Phi-3-mini (Abdin et al.\\n2024) and Phi-4 (Abdin et al. 2024) as the pre-trained LLMs, respectively. In addition, we expand the LLM vocabulary with\\n8192 VQ indices derived from VQGAN-f8-8192 (Esser, Rombach, and Ommer 2021), serving as multi-modal tokens to further\\naugment the model’s capacity for understanding both visual and textual input. Figure 6 shows the details.\\nTable 6: Overview of the Components of HealthGPT.\\nModel ViT Adapter MLP-dims Model dims LLM Params Vocab Size H-LoRA Rank\\nHealthGPT-M3 CLIP-L/14 2-layer MLP 1024 3072 Phi-3-mini 3.8B 40206 16(Comp.), 64(Gen.)\\nHealthGPT-L14 CLIP-L/14 2-layer MLP 1024 5120 Phi-4 14B 108547 8(Comp.), 32(Gen.)\\nA.2 Training Details\\nIn this study, we propose a three-stage learning strategy that is compatible with our innovative heterogeneous low-rank adapta-\\ntion (H-LoRA). We provide a detailed hyperparameter configuration for the model’s three-stage training process. The specific\\nhyperparameter settings used are listed in Table 7. These hyperparameters are crucial for ensuring the model’s learning efficacy\\nand final performance.\\nTable 7: Overview of Hyperparameter Configurations.\\nHealthGPT-M3 HealthGPT-L14\\nStage-1 Stage-2 Stage-3 Stage-1 Stage-2 Stage-3Hyperparameter\\nComp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen.\\nOptimizer AdamW AdamW AdamW AdamW AdamW AdamW\\nAdapter LR 1e-3 2e-5 2e-5 2e-5 1e-3 2e-5 2e-5 2e-5\\nLearning Rate / 2e-4 2e-4 2e-4 / 1e-4 2e-4 2e-4\\nGlobal Batch Size 256 64 32 128 64 256 64 32 128 64\\nWeight Decay 0 0 0 0 0 0\\nDropout Rate 0 0.05 0.05 0.05 0 0.05 0.05 0.05\\nLR Scheduler Warm Up Constant Warm Up Warm Up Constant Warm Up\\nMax Sequence Length 2048 2048 2048 2048 2048 2048\\nIt is worth noting that we sometimes observe instances of loss spikes during the training of medical visual comprehension\\nand generation tasks. Through repeated validation, we discovered that larger model parameters and learning rates tend to lead to\\nthis issue, which is the reason for the slight differences in hyperparameters betweenHealthGPT-M3 and HealthGPT-L14.\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='ba538698-7cc4-4ecb-afcb-6813a2391de6', embedding=None, metadata={'page_label': '13', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='（a） （b）\\nFigure 8: VL-Health dataset collection distribution.\\nA.3 VL-Health\\nThe construction of theVL-Health dataset involves two key steps:(i) data collection, (ii) data processing, as detailed below:\\nData Collection: During the collection phase, we carefully considered the diversity of medical images and the complexity of\\nthe tasks, selecting appropriate subsets for comprehension and generation tasks. For comprehension tasks, we selected datasets\\nsuch as VQA-RAD (Lau et al. 2018), SLAKE (Liu et al. 2021), PathVQA (He et al. 2020), and MIMIC-CXR-VQA (Bae et al.\\n2024), which cover various medical imaging modalities like radiology and pathology, and include professional annotations\\nto assist the model in learning tasks such as lesion detection and disease diagnosis. Additionally, large-scale multi-modal\\ndatasets like LLaV A-Med (Li et al. 2024b) and PubMedVision (Chen et al. 2024a) were included to provide broader medical\\nknowledge support and facilitate the training of complex reasoning tasks. For generation tasks, we focused on four mainstream\\ntask categories: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction.\\nThe IXI (Davies et al. 2014) dataset, containing a large number of healthy brain MRI images, is suitable for training super-\\nresolution models; the MIMIC-CHEST-XRAY (Bae et al. 2024) dataset, with X-ray images and their corresponding textual\\nreports, is appropriate for text-to-image generation tasks; the SynthRAD2023 (Thummerer et al. 2023) dataset provides a large\\nnumber of paired CT and MRI images, supporting modality conversion model training; for image reconstruction tasks, we\\nrewrote and adjusted the LLaV A-558k (Liu et al. 2024b) dataset.\\nData Processing: After data collection, we performed filtering and processing of the raw data. For VisualQA tasks, we stan-\\ndardized the data entries into two forms: open-ended questions and single-choice questions, enabling flexible training and\\nevaluation. Additionally, considering that multi-image data has a minimal impact on performance but introduces extra padding\\nand training time, we excluded multi-image data. For the scanned image data in generation tasks, we applied slicing extrac-\\ntion, image registration, data augmentation, and normalization to treat 2D images as visual inputs for model training or used\\nVQGAN-generated indices to supervise the generation tasks.\\nData Statistics This section provides detailed statistical information about the VL-Health dataset to offer a more compre-\\nhensive understanding.\\nData Overview: To ensure a balanced development of the model’s comprehension and generation capabilities, in addition\\nto the LLaV A-558k and PubMedVision-PT datasets used for alignment, the VL-Health dataset ultimately selected 765,802\\nadditional visual question-answering (VQA) training samples (to endow the model with visual comprehension and instruction-\\nfollowing capabilities) and 783,045 generation training samples (to provide the model with reconstruction and visual generation\\ninstruction-following abilities). This contributes to the transfer of knowledge between comprehension and generation tasks, en-\\nhancing the model’s overall performance. For medical image comprehension tasks, images were selected from VQA-RAD (ap-\\nproximately 450 images), SLAKE (approximately 630 images), PathVQA (approximately 2,600 images), MIMIC-CXR-VQA\\n(approximately 52,000 images), LLaV A-Med (approximately 61,000 images), and PubMedVision (approximately 500,000 im-\\nages). Multiple question-answer pairs were retained for each image to enhance the model’s understanding and generalization\\nof the image content. Table 8 shows the data distribution ofVL-Health for three-stage learning strategy, where mixed-47k is\\nbased on the sampling of all data in stage-1.\\nDiversity and Quality Assessment: VL-Health covers 11 modalities, including CT, MRI, X-ray, microscopy, OCT, ultra-\\nsound, and fundus photography, which aids the model in learning features from various modalities. The dataset also encom-\\npasses a wide range of diseases, from common to rare, and from localized lesions to systemic diseases, including pulmonary\\ndiseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides com-\\nprehensive training support to the model, enabling it to learn the characteristics and diagnosis of various diseases.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='1687014f-48f1-44a0-81b3-fdc19b576877', embedding=None, metadata={'page_label': '14', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8: Data distribution of VL-Health in three-stage learning strategy.\\nMedical Task Stage-1 Stage-2\\nComp. LLaV A-558k, PubMedVision-PT Mixed-47kGen. LLaV A-558k\\nMedical Task Stage-3\\nComp. LLaV A Med, MIMIC CXR VQA, PubMedVision-FT, LLaV A-665k, PathVQA, SLAKE, VQA-RAD\\nGen. IXI, SynthRAD2023, MIMIC-CHEST-XRAY\\nData Format. All data samples are converted into a unified instruction-response format for training and evaluation. Specifi-\\ncally, the VL-Health dataset consists of the following components:\\n• Task Type: Specifies the granularity of visual features output by the visual encoder and selects the corresponding H-\\nLoRA submodule. For generation tasks, the response also includes multi-modal tokens corresponding to VQ indices.\\n• Task Instruction: Guides the model to interpret the image and generate a response, covering various aspects of the\\nimage and specifying the output format.\\n• Response: The textual output generated based on the task instruction and input image, ensuring it meets the question and\\nformatting requirements.\\n• Input Image: Provides the visual signal for the model to process.\\n• Target Image Index: In generation tasks, this is added as a multi-modal token to the response for autoregressive\\ngeneration.\\nB Analysis of Heterogeneous Low-Rank Adaptation\\nWe propose H-LoRA, which utilizes hard routing selection to allocate plugins for knowledge learning and representation across\\ntasks, thereby preventing conflicts arising from heterogeneous knowledge. Furthermore, within each task, we optimized based\\non MoELoRA, enhancing performance while reducing computational overhead. The pseudocode is detailed Algorithm 1.\\nAlgorithm 1: H-LoRA Algorithm\\nInput: concrete-grained visual features FCon, abstract-grained visual featuresFAbs, comprehension-based H-LoRA modules\\n({AComp.\\ni }k\\ni=1, RComp.\\nouter ), generation-based H-LoRA modules({AGen.\\ni }k\\ni=1, RGen.\\nouter), task type T (comprehension or generation),\\nnumber of LoRA experts k, origin linear layer weights W0, text features T , hidden state h\\nOutput: final output O\\n// Select task-specific image features\\nif T = generation task then\\nFimg ← FCon\\nelse if T = comprehension task then\\nFimg ← FAbs\\nend if\\nU ←concat(Fimg, T ) // Concatenate image features and text features\\n{Ai}k\\ni=1, {Bi}k\\ni=1, Router ← {AT\\ni }k\\ni=1, {BT\\ni }k\\ni=1, RT\\nouter // Assign task-specific H-LoRA submodule\\n// Merge LoRA experts’ matrices\\nAmerged ← concat({Ai}k\\ni=1)\\nBmerged ← concat({Bi}k\\ni=1)\\nW ←R(h) // Generate routing weights based on input hidden state x\\nWexpanded ← α × W/r ⊗ 1r // Expand routing weights to match merged matrices\\nOH-LoRA ← (x · Amerged ⊙ Wexpanded) · Bmerged // Compute H-LoRA output using element-wise multiplication\\nO ←x · W0 + OH-LoRA // Add H-LoRA output to pre-trained weights to get final output\\nReturn O\\nWe further analyzed the computational overhead differences between MoELoRA and H-LoRA. Assuming that both methods\\nuse the same number of LoRA experts k, we can compare their time complexity from the perspective of the operational steps\\ninvolved.\\nComputational Overhead of MoELoRA. In MoELoRA, the operations involving the expert matrix mainly include the fol-\\nlowing steps: (i) Expert Multiplication : MoELoRA requires 2k multiplications with the LoRA experts. (ii) Router Multi-\\nplication: One multiplication with the Router is required. (iii) Router Output Expansion : MoELoRA needs to perform k\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='2176f658-0034-4503-9b51-710dc7a262dc', embedding=None, metadata={'page_label': '15', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='expansion operations on the Router’s output weights to generate the appropriate shapes that match the dimensions of the in-\\nput and LoRA experts while iterating through the experts. (iv) Dot Product: For each expanded Router weight, a dot product\\nwith the intermediate state of the expert is required, resulting in k multiplications. (v) Addition: Finally, k addition operations\\nare required to accumulate the results from each LoRA expert into the final output. Assuming the time complexity of each\\noperation is the same, the additional time complexity introduced when equipping a fully connected layer with MoELoRA is:\\nO(2k+1+ k+k+k) = O(5k+1). Thus, MoELoRA introduces an additional time overhead ofO(5k+1) during computation.\\nH-LoRA. In contrast to MoELoRA, H-LoRA reduces the computational overhead by concatenating the LoRA expert matrices.\\nSpecifically: (i) Expert Multiplication: H-LoRA merges all LoRA experts by directly creating a larger A and B matrix, instead\\nof performing independent operations for each expert. This process can be implemented through matrix initialization without\\nadditional concatenation operations. Therefore, only 2 multiplications with the LoRA experts are required. (ii) Router Multi-\\nplication: H-LoRA still requires one multiplication with the Router. (iii) Router Output Expansion : H-LoRA only requires\\none expansion operation on the Router’s output weights. (iv) Dot Product: H-LoRA only requires one dot product between\\nthe Router’s output and the expert’s intermediate state. (v) Addition: Finally, H-LoRA only requires one addition operation\\nto accumulate the LoRA expert results into the intermediate state. Therefore, the additional time complexity introduced by\\nH-LoRA is: O(2 + 1 + 1 + 1 + 1) =O(6).\\nComparing the two, we see that MoELoRA introduces a linear increase in additional time complexity with respect to the\\nnumber of experts k, resulting in a complexity of O(5k + 1), while H-LoRA’s additional time complexity is fixed at O(6),\\nindependent of k. We observe that when k is small, the time complexity differences between MoELoRA and H-LoRA are\\nnegligible. However, as k increases, MoELoRA’s computational overhead grows linearly, while H-LoRA’s remains constant.\\nThis makes H-LoRA significantly more computationally efficient than MoELoRA, particularly in large-scale tasks. We will\\nfurther demonstrate the significant advantage of H-LoRA in training time in subsequent experiments, validating its efficiency\\nin practical applications.\\nC Supplemental Experimental Results\\nIn this section, we include additional experiments to demonstrate the superiority of HealthGPT and articulate our design\\nphilosophy.\\nC.1 Results: OmniMedVQA Benchmark\\nOmniMedVQA (Hu et al. 2024) is a novel, large-scale medical visual question answering (VQA) benchmark designed to\\nencompass various modalities and anatomical regions by collecting diverse images from multiple medical datasets. Our exper-\\nimental results are presented in Table 9.\\nTable 9: Performance comparison of OmniMedVQA Benchmark.\\nOmniMedVQA↑Type Model # Params Medical\\nLVLM CT X-ray FDM MiS OCT MRI USS Avg.\\nComp. Only\\nMed-Flamingo 8.3B ✓ 30.1 33.9 25.5 37.0 60.0 27.6 30.4 34.9\\nLLaV A-Med 7B ✓ 28.4 32.8 42.7 31.6 55.3 45.0 53.6 41.3\\nHuatuoGPT-Vision 7B ✓ 35.3 41.5 51.4 62.3 59.3 40.4 60.1 50.0\\nBLIP-2 6.7B ✗ 26.6 29.1 22.3 36.9 29.1 22.7 21.4 26.9\\nLLaV A-v1.5 7B ✗ 28.0 55.7 35.5 42.1 49.2 52.9 49.7 44.7\\nInstructBLIP 7B ✗ 20.1 22.2 34.1 30.6 38.6 31.9 25.5 29.0\\nYi-VL 6B ✗ 51.2 47.1 27.7 62.6 67.6 55.0 40.3 50.2\\nInternVL2 8B ✗ 40.2 57.9 53.2 64.0 59.1 58.1 49.1 54.5\\nLlama-3.2 11B ✗ 37.6 55.2 71.4 82.1 62.5 65.2 68.6 63.2\\nComp. & Gen.\\nShow-o 1.3B ✗ 29.0 50.4 30.9 22.0 30.8 34.2 33.8 33.0\\nUnified-IO 2 7B ✗ 10.8 37.7 12.3 25.3 32.6 30.9 37.7 26.8\\nJanus 1.3B ✗ 24.9 54.8 35.9 62.7 54.2 50.7 36.8 45.7\\nHealthGPT-M3 3.8B ✓ 35.3 81.9 54.6 88.2 89.3 78.5 51.4 68.5\\nHealthGPT-L14 14B ✓ 39.0 86.6 64.1 88.6 99.7 80.9 62.2 74.4\\nThrough our analysis, we make the following observations: (i) HealthGPT-M3 outperforms other models in 4 out of 7\\nsub-tasks, achieving an average score that exceeds cutting-edge medical Large Vision-Language Models (LVLMs) as well as\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='28be5ce0-dc80-4d50-b31a-6d36ce4d47d8', embedding=None, metadata={'page_label': '16', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='general LVLMs; (ii) the unified model demonstrates relatively weak performance on OmniMedVQA; however, our approach\\neffectively mitigates performance degradation caused by generation tasks, serving as a unified model; (iii) HealthGPT-L14\\nexcels across all sub-tasks, achieving optimal or near-optimal results with an average score of 74.4, significantly surpassing\\nother models.\\nC.2 Stability Analysis of Number of Experts\\nWe investigated the impact of the number of LoRA experts on model performance within a multi-LoRA architecture, conducting\\nextensive experiments on MoELoRA and H-LoRA with varying numbers of experts. The experimental results are presented\\nin Table 10. As the number of experts increases, the training time for MoELoRA is significantly prolonged. When n = 8 ,\\nthe training time for MoELoRA is twice that of LoRA, whereas H-LoRA incurs no additional training delay and performs\\nbetter. It is estimated that at n = 32, the training time for MoELoRA could reach eight times that of LoRA, preventing it from\\ncompleting training and inference. This result aligns with the analysis in Appendix B, indicating that H-LoRA not only avoids\\nintroducing additional training delays compared to LoRA but also outperforms MoELoRA.\\nTable 10: We explored the performance of MoELoRA and H-LoRA with different numbers of LoRA experts. At n = 32 ,\\nMoELoRA was unable to complete training.\\nn=2 n=4 n=8 n=32Model Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time\\n+MoELoRA 50.3 62.98 1.22 × 50.0 64.33 1.49 × 50.8 63.71 2.09 × / / 5.81 ×HealthGPT w/ +H-LoRA 51.5 63.48 0.99× 52.8 64.71 1.00× 53.6 64.98 0.99× 53.5 64.74 1.01×\\nC.3 Impact of Heterogeneous Knowledge Fusion on Performance\\nTraditional unified models often utilize mixed training methods, which may result in performance degradation due to variations\\nin task modes. To address this, we propose a three-phase learning strategy to support H-LoRA, effectively mitigating inter-task\\nconflicts. Specifically, the second phase (Heterogeneous H-LoRA Plugin Adaptation) integrates LLMs with different H-LoRA\\nplugins into a new unified foundation by mixing the training of the embedding layers and output heads for two tasks. Figure\\n9 illustrates the impact of this phase on the performance of medical comprehension and generation tasks. We observe that the\\nsecond phase effectively unifies the model with minimal impact on overall performance, significantly alleviating the conflict\\nissues arising from mixed training in medical scenarios.\\nC.4 Human Evaluation.\\n65.7 65.4 67.7 67.0\\nFigure 9: Performance changes before and after the\\nstage-2.\\nWe further conduct human evaluation on the VQA-RAD, SLAKE,\\nand PathVQA benchmarks, which contain 1,000 open-ended ques-\\ntions. Specifically, we recruit 5 clinicians to rank the randomly shuf-\\nfled responses from HealthGPT-L14, LLaV A-Med, HuatuoGPT-\\nVision, Llama-3.2, InternVL-2 and Show-o. During the evaluation,\\nquestions were randomly selected, and the model-generated responses\\nwere anonymized and ranked. The results, as shown in Figure 10, in-\\ndicate that HealthGPT was frequently selected as the best answer.\\nThis suggests that HealthGPT has further application potential in\\nmedical care scenarios.\\nC.5 Reconstruction Performance\\nCurrently, unified models that align visual features based on recon-\\nstruction tasks include pre-LVLMs, post-LVLMs, as well as Unified-\\nIO 2 (Lu et al. 2024) and SEED-X (Ge et al. 2024). To investigate the\\ncontrollability of visual generation in rigorous settings such as med-\\nical contexts, we evaluated the performance of these models in med-\\nical image reconstruction in Table 11. Experimental results demon-\\nstrate that HealthGPT exhibits the most stable reconstruction per-\\nformance with a small amount of data.\\n16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='5544de1c-168b-4191-8aa5-cd5eb4cdd619', embedding=None, metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='C.6 Case Study\\nFigures 11 and 12 illustrate examples of modality transformation and super-resolution reconstruction. In Figure 11, the results\\ngenerated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively\\nguiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure 12\\ndemonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the\\nimage.\\nTable 11: The experimental results for the four reconstruction tasks.\\nCT(Brain) CT(Pelvis) MRI (Brain) MRI(Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\nSEED-X 20.18 27.66 112.11 21.53 28.02 102.87 4.90 27.62 112.86 6.31 27.89 106.21\\nUnified-IO 2 83.93 36.09 17.95 85.36 35.10 25.46 87.50 34.25 25.47 86.31 33.53 29.80\\nHealthGPT-M3 91.73 36.42 15.46 94.26 37.30 12.53 88.76 33.97 27.05 84.40 33.11 32.62\\n34.08\\n15.87\\n9.16\\n5.62\\n13.33\\n21.94\\nHealthGPT\\nLLaVA-Med\\nHuatuoGPT-Vision\\nLlama-3.2\\nInternVL-2\\nShow-o\\nHuman\\nEvaluation\\n(a) (b)\\nFigure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='0f6b5fc1-8392-44dd-84e8-b46c060254fc', embedding=None, metadata={'page_label': '18', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 11: Case of modality transfer.\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='c9b83d92-ae76-40fe-8cd8-4fe94bd0b932', embedding=None, metadata={'page_label': '19', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 12: Case of MRI image super-resolution.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='a34239c3-9cb7-4c92-91a7-112e1434a421', embedding=None, metadata={'page_label': '1', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' \\n1     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='d0b1409e-b4ad-40a9-9198-000eecc769f9', embedding=None, metadata={'page_label': '2', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only \\nVersion  control     Classification  Level:   Internal   Asset  Owner:   Legal     Asset:     WSO2  LLC  -  Anti-Corruption  Policy     Document  History:  \\nDate  Revision  Author(s)  Description  Reviewed  &  Approved  By  28/07/2022  V1.0  \\nLegal  Team  and  External  Counsel  (Cooley  LLP)   \\nInitial  Version  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n19/10/2023  V1.0  \\nLegal  Team  Reviewed.  No  changes  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n \\n2     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='eb2ae63c-c316-4974-bb5f-9b7b9ca8097b', embedding=None, metadata={'page_label': '3', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only  \\nContents  I.   PURPOSE \\nII.   POLICY  STATEMENTS \\nIII.   ANTI-BRIBERY  PROHIBITIONS \\nIV.   ACCOUNTING  REQUIREMENTS \\nV.   FACILITATION  PAYMENTS \\nVI.   INTERMEDIARIES  AND  BUSINESS  PARTNERS \\nVII.   GIFTS  AND  HOSPITALITIES \\nIX.   OTHER  ACTIVITIES \\nX.   VIOLATIONS  AND  CONSEQUENCES \\nXI.   TRAINING  AND  CERTIFICATION \\nXII.   STATUS \\nXIII.   REPORTING/QUESTIONS \\nIX.   ACKNOWLEDGEMENT   \\nATTACHMENT  1:  ANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL \\n●  UNITED  KINGDOM ●  THE  UK  BRIBERY  ACT  2010 ●  SRI  LANKA ●  BRAZIL o  THE  BRAZILIAN  ANTI  CORRUPTION  ACT  2013 o  THE  BRAZILIAN  IMPROBITY  ACT  1992 o  OTHER  POTENTIAL  LIABILITIES  \\n \\n3     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='9aa353a0-cc99-4018-99e9-5d5d27bce189', embedding=None, metadata={'page_label': '4', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only WSO2\\n \\nLLC   A\\nNTI\\n-C\\nORRUPTION\\n \\nP\\nOLICY\\n  \\nA\\nPPROVED\\n \\nBY\\n \\nTHE\\n \\nB\\nOARD\\n \\nOF\\n \\nD\\nIRECTORS\\n  JULY  12,  2022   \\n  \\nI.   P\\nURPOSE\\n \\n \\nWSO2  LLC  (together  with  its  worldwide  subsidiaries,  “ WSO2 ”  or  the  “ Company ”)  has  implemented  this  \\nAnti-Corruption\\n \\nPolicy\\n \\n(the\\n \\n“\\nPolicy\\n”)\\n \\nfor\\n \\nthe\\n \\npurpose\\n \\nof\\n \\nensuring\\n \\ncompliance\\n \\nwith\\n \\nthe\\n \\nU.S.\\n \\nForeign\\n \\nCorrupt\\n \\nPractices\\n \\nAct\\n \\nof\\n \\n1977,\\n \\nas\\n \\namended\\n \\n(the\\n \\n“\\nFCPA\\n”),\\n \\nthe\\n \\nU.S.\\n \\nTravel\\n \\nAct,\\n \\nthe\\n \\nU.S.\\n \\nDomestic\\n \\nBribery\\n \\nStatute,\\n \\nthe\\n \\nUK\\n \\nBribery\\n \\nAct\\n \\n2010,\\n \\nthe\\n \\nSri\\n \\nLankan\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments),\\n \\nthe\\n \\nBrazilian\\n \\nAnti-corruption\\n \\nAct\\n \\n(Law\\n \\nNo.\\n \\n12,846/2013),\\n \\nthe\\n \\nBrazilian\\n \\nImprobity\\n \\nAct\\n \\n1992\\n \\n(Law\\n \\nNo.\\n \\n8.429/1992),\\n \\nand\\n  \\nall\\n  \\nother\\n  \\nanti-corruption\\n  \\nlaws\\n  \\nand\\n  \\nregulations\\n  \\napplicable\\n  \\nto\\n  \\nWSO2’s\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(collectively,\\n \\n“\\nAnti-Corruption\\n \\nLaws\\n”).\\n  \\nThis\\n \\nPolicy\\n \\napplies\\n \\nto\\n \\nall\\n \\nworld-wide\\n \\ndirectors,\\n \\nofficers,\\n \\nemployees,\\n \\nand\\n \\nindividuals\\n \\nserving\\n \\nas\\n \\nindependent\\n \\ncontractors\\n \\nof\\n \\nWSO2\\n \\n(collectively,\\n \\n“\\nWSO2\\n \\nPersonnel\\n”)\\n \\nto\\n \\ncomply\\n \\nwith\\n \\nthe\\n \\nprinciples\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy.\\n  \\nPlease\\n \\nreport\\n \\nall\\n \\nquestions\\n \\nor\\n \\nconcerns\\n \\nto\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nwhose\\n \\ncontact\\n \\ninformation\\n \\nappears\\n \\nbelow.\\n \\n \\nII.   P\\nOLICY\\n \\nS\\nTATEMENTS\\n \\n \\nWSO2  Personnel  are  strictly  prohibited  from  promising,  offering,  providing,  or  authorizing  cash  \\npayments\\n \\n(such\\n \\nas\\n \\nbribes\\n \\nor\\n \\nkickbacks)\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nalso\\n \\nstrictly\\n \\nprohibited\\n \\nfrom\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue\\n \\nfrom\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n    \\n \\nWSO2  Personnel  must  comply  with  all  of  the  Company’s  internal  controls,  especially  those  designed  \\nto\\n \\nensure\\n \\naccurate\\n \\nand\\n \\ncomplete\\n \\nbooks\\n \\nand\\n \\nrecords,\\n \\nor\\n \\notherwise\\n \\nprevent\\n \\ncorruption,\\n \\nself-dealing,\\n \\nembezzlement,\\n \\nfraud,\\n \\nmoney\\n \\nlaundering,\\n \\nor\\n \\nother\\n \\nimproper\\n \\nactivities.\\n \\n \\nThere  are  no  exceptions  to  this  Policy,  even  if  our  competitors  engage  in  improper  behavior  or  corruption  \\nis\\n \\nan\\n \\naccepted\\n \\npractice\\n \\nin\\n \\na\\n \\ncountry\\n \\nwhere\\n \\nwe\\n \\noperate.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nrequired\\n \\nto\\n \\nadhere\\n \\nto\\n \\nboth\\n \\nthe\\n \\nspirit\\n \\nand\\n \\nthe\\n \\nletter\\n \\nof\\n \\nthis\\n \\nPolicy\\n \\nwith\\n \\nrespect\\n \\nto\\n \\nour\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld.\\n \\n \\nIII.   A\\nNTI\\n-B\\nRIBERY\\n \\nP\\nROHIBITIONS\\n \\n \\nThe  FCPA  and  other  Anti-Corruption  Laws  prohibit  WSO2  and  WSO2  Personnel  from  corruptly  \\npromising,\\n \\noffering,\\n \\nproviding,\\n \\nor\\n \\nauthorizing\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nof\\n \\nvalue\\n \\ndirectly\\n \\nor\\n \\nindirectly\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nand\\n \\ncertain\\n \\nother\\n \\npersons\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose.\\n \\n“Improper\\n \\npurposes”\\n \\ninclude\\n \\ninfluencing\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\nthe\\n \\nrecipient\\n \\nin\\n \\nhis/her\\n \\nofficial\\n \\ncapacity,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ndo\\n \\nor\\n \\nomit\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nin\\n \\nviolation\\n \\nof\\n \\nhis/her\\n \\nlawful\\n \\nduty,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ninfluence\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\na\\n \\ngovernment\\n \\nor\\n \\ninstrumentality\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nor\\n \\nsecuring\\n \\nany\\n \\nimproper\\n \\nadvantage,\\n \\nin\\n \\norder\\n \\nto\\n \\nobtain,\\n \\nretain,\\n \\nor\\n \\ndirect\\n \\nregulatory\\n \\napprovals,\\n \\ncontracts,\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nbenefits.\\n   \\n \\n4     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='6540bb39-89af-46cd-9aa5-8ece52852f2f', embedding=None, metadata={'page_label': '5', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only The  FCPA  prohibits  improper  payments  provided  to  officials  of  governments,  state-affiliated  entities,  and  \\npolitical\\n \\nparties\\n \\noutside\\n \\nthe\\n \\nUnited\\n \\nStates.\\n \\nHowever,\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\ngovernment\\n \\nor\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\nthe\\n \\nUnited\\n \\nStates\\n \\nwill\\n \\nviolate\\n \\nU.S.\\n \\ndomestic\\n \\nbribery\\n \\nstatutes.\\n \\n \\nIn  addition  to  the  United  States,  almost  all  other  countries,  including  the  United  Kingdom,  Brazil,  and  Sri  \\nLanka,\\n \\nhave\\n \\npromulgated\\n \\ntheir\\n \\nown\\n \\nanti-bribery\\n \\nlegislation.\\n \\nMost\\n \\nof\\n \\nthose\\n \\ncountries\\n \\nprohibit\\n \\nmaking\\n \\nimproper\\n \\npayments\\n \\nto\\n \\ngovernment\\n \\nand\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\ntheir\\n \\nborders.\\n \\nHowever,\\n \\nseveral\\n \\ncountries\\n \\nhave\\n \\nalso\\n \\nadopted\\n \\nlegislation\\n \\nsimilar\\n \\nto\\n \\nthe\\n \\nFCPA\\n \\nthat\\n \\nprohibit\\n \\nimproper\\n \\npayments\\n \\noutside\\n \\nthose\\n \\ncountries.\\n  \\nThe\\n \\nexistence\\n \\nof\\n \\nall\\n \\nof\\n \\nthese\\n \\nlaws\\n \\nmeans\\n \\nthat\\n \\nthere\\n \\nis\\n \\npotential\\n \\nfor\\n \\na\\n \\ncompany\\n \\nor\\n \\nan\\n \\nindividual\\n \\nto\\n \\nface\\n \\nliability\\n \\nin\\n \\nseveral\\n \\ncountries\\n \\nfor\\n \\nthe\\n \\nsame\\n \\nsingle\\n \\nact\\n \\nof\\n \\ncorruption.\\n  \\nAttachment\\n \\n1\\n \\ncontains\\n \\nan\\n \\noverview\\n \\nof\\n \\nthe\\n \\nAnti-Corruption\\n \\nLaws\\n \\nof\\n \\nother\\n \\njurisdictions\\n \\nwhich\\n \\nare\\n \\napplicable\\n \\nto\\n \\nWSO2.\\n \\n \\nGiven  the  broad  prohibitions  under  Anti-Corruption  Laws  applicable  to  WSO2,  this  Policy  \\nprohibits\\n \\nbribes,\\n \\nkickbacks,\\n \\nand\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nand\\n \\nadvantages\\n \\nto\\n \\nany\\n \\nperson,\\n \\nentity,\\n \\nor\\n \\norganization,\\n \\nincluding,\\n \\nbut\\n \\nnot\\n \\nlimited\\n \\nto,\\n \\nemployees,\\n \\nofficials,\\n \\nrepresentatives,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\n \\n(i)  government;   \\n(ii)  state-owned  or  affiliated  entity,  including,  but  not  limited  to,  a  state  hospital,  research  \\ninstitution,\\n \\nutility,\\n \\npublic\\n \\nuniversity,\\n \\nor\\n \\nsovereign\\n \\nwealth\\n \\nfund;\\n \\n \\n(iii)  public  international  organization  such  as  the  United  Nations  or  the  World  Bank;    \\n(iv)  political  party,  including  the  party  itself  as  well  as  candidates  for  public  office;    \\n(v)  non-governmental  organization;  or   (vi)   private-sector  company.      \\nOne  may  be  asked  by  certain  parties  to  provide  a  bribe  or  other  improper  benefit  in  exchange  for  the  \\naward\\n \\nof\\n \\na\\n \\ncontract,\\n \\nsponsorship\\n \\nopportunity,\\n \\nor\\n \\nother\\n \\nbusiness;\\n \\nthe\\n \\nissuance\\n \\nor\\n \\nrenewal\\n \\nof\\n \\na\\n \\nconcession,\\n \\nlicense,\\n \\nor\\n \\nbusiness,\\n \\nconstruction,\\n \\nor\\n \\nother\\n \\npermit\\n \\nor\\n \\nregistration;\\n \\nthe\\n \\nsuccessful\\n \\nfiling\\n \\nof\\n \\na\\n \\npatent\\n \\nor\\n \\ntrademark\\n \\napplication;\\n \\nan\\n \\nimpermissible\\n \\nreduction\\n \\nin\\n \\nduties\\n \\nor\\n \\nother\\n \\ntaxes;\\n \\nobtaining\\n \\na\\n \\nfavorable\\n \\ninspection\\n \\nresult\\n \\nor\\n \\ncourt\\n \\ndecision,\\n \\neven\\n \\nif\\n \\nthe\\n \\nfacts\\n \\nor\\n \\ncircumstances\\n \\ndo\\n \\nnot\\n \\nsupport\\n \\nsuch\\n \\na\\n \\nresult;\\n \\nor\\n \\nthe\\n \\ngrant\\n \\nof\\n \\nsome\\n \\nother\\n \\nimproper\\n \\nadvantage.\\n  \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nWSO2\\n \\nPersonnel\\n \\nfrom\\n \\nproviding\\n \\nbribes\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nany\\n \\nof\\n \\nthe\\n \\nabove\\n \\npurposes.\\n \\n \\nA  violation  of  this  Policy  can  occur  even  if  the  bribe  fails  to  achieve  the  purpose  for  which  it  was  \\nintended.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\na\\n \\nperson\\n \\ncan\\n \\nviolate\\n \\nthis\\n \\nPolicy\\n \\nif\\n \\nthat\\n \\nperson\\n \\nprovides\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nbenefit\\n \\nto\\n \\na\\n \\nrecipient\\n \\nand\\n \\nthe\\n \\nrecipient\\n \\ndoes\\n \\nnot\\n \\ngrant\\n \\nany\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nin\\n \\nreturn.\\n   \\nIn\\n \\naddition,\\n \\nthe\\n \\nmere\\n \\noffer\\n \\nor\\n \\npromise\\n \\nof\\n \\na\\n \\nbribe\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefit\\n \\nis\\n \\nsufficient\\n \\nto\\n \\ncause\\n \\na\\n \\nviolation.\\n  \\nAll\\n \\nof\\n \\nthe\\n \\nanti-bribery\\n \\nprohibitions\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy\\n \\napply\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\none\\n \\nuses\\n \\nWSO2\\n \\nfunds\\n \\nor\\n \\npersonal\\n \\nfunds\\n \\nto\\n \\nfinance\\n \\nimproper\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits.\\n \\n \\nThis  Policy  also  prohibits  WSO2  Personnel  from  soliciting  or  accepting  bribes,  kickbacks,  or  other  \\nimproper\\n \\npayments/benefits\\n \\nfrom\\n \\nthe\\n \\nCompany’s\\n \\nvendors\\n \\nor\\n \\nother\\n \\npersons\\n \\nin\\n \\nrelation\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\nFor\\n \\n5     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='5131d538-492d-4d7b-876a-4c7b00d3d5b8', embedding=None, metadata={'page_label': '6', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only instance,  a  violation  of  this  Policy  will  occur  if  you  cause  WSO2  to  overpay  a  vendor  and  that  vendor  then  \\nshares\\n \\nall\\n \\nor\\n \\na\\n \\nportion\\n \\nof\\n \\nthat\\n \\noverpayment\\n \\nwith\\n \\nyou.\\n   \\n \\nThis  Policy  requires  WSO2  Personnel  to  adhere  to  high  ethical  standards  and  to  comply  with  all  \\napplicable\\n \\nlaws\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nthe\\n \\nCompany.\\n  \\nAnti-corruption\\n \\nviolations\\n \\ntypically\\n \\ninvolve\\n \\ncircumstances\\n \\nthat\\n \\nalso\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws,\\n \\nincluding\\n \\nthose\\n \\nthat\\n \\naddress\\n \\nmoney\\n \\nlaundering,\\n \\nembezzlement,\\n \\nfraud,\\n \\nexport\\n \\ncontrols,\\n \\nand\\n \\nsanctions/embargoes.\\n \\nGuilty\\n \\npersons\\n \\ncan\\n \\nface\\n \\nmultiple\\n \\ncharges\\n \\nbased\\n \\non\\n \\nthe\\n \\nsame\\n \\nset\\n \\nof\\n \\nfacts.\\n \\n \\nIV.   A\\nCCOUNTING\\n \\nR\\nEQUIREMENTS\\n \\n \\nWSO2  must  maintain  books,  records,  and  accounts,  which,  in  reasonable  detail,  accurately  and  fairly  \\nreflect\\n \\nthe\\n \\nCompany’s\\n \\ntransactions,\\n \\nexpenses,\\n \\nand\\n \\nasset\\n \\ndispositions.\\n \\nWSO2\\n \\nis\\n \\nalso\\n \\ncommitted\\n \\nto\\n \\nmaintaining\\n \\na\\n  \\nsystem\\n \\nof\\n \\ninternal\\n \\naccounting\\n \\ncontrols\\n \\nto\\n \\nprovide\\n \\nreasonable\\n \\nassurances\\n \\nthat\\n \\ntransactions\\n \\nare\\n \\nproperly\\n \\nauthorized\\n \\nby\\n \\nmanagement,\\n \\nexecuted,\\n \\nand\\n \\nrecorded.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\ncomply\\n \\nwith\\n  \\nour\\n \\ninternal\\n \\ncontrols\\n \\nand\\n \\navoid\\n \\nunauthorized\\n \\nactivities\\n \\nor\\n \\nexpenses.\\n  \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\nalso\\n \\ncooperate\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nperiodic\\n \\naudits\\n \\nand\\n \\nother\\n \\nefforts\\n \\nto\\n \\nensure\\n \\nthat\\n \\nour\\n \\ninternal\\n \\ncontrols\\n \\nare\\n \\nbeing\\n \\nobserved.\\n \\n \\nViolations  of  the  above  accounting  standards  can  occur  if  one  conceals  bribes  or  falsifies  other  \\ntransactions\\n \\nor\\n \\nexpenses,\\n \\neven\\n \\nif\\n \\nthey\\n \\nare\\n \\nnot\\n \\nrelated\\n \\nto\\n \\na\\n \\nbribe,\\n \\nin\\n \\nWSO2’s\\n \\nledgers\\n \\nor\\n \\nother\\n \\nrecords.\\n  \\nAlso,\\n \\nthere\\n \\nis\\n \\nno\\n \\nmateriality\\n \\nstandard.\\n \\nThis\\n \\nmeans\\n \\nthat\\n \\neven\\n \\nsmall\\n \\nmisreported\\n \\namounts\\n \\nmay\\n \\nresult\\n \\nin\\n \\nviolations.\\n   \\n \\nV.   F\\nACILITATION\\n \\nP\\nAYMENTS\\n \\n \\n \\nThis  Policy  prohibits  all  corrupt  payments  or  benefits,  including  so-called  grease  or  facilitation  payments  \\nprovided\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\nto\\n \\nexpedite\\n \\nor\\n \\nsecure\\n \\nroutine\\n \\ngovernment\\n \\nactions\\n \\n(collectively,\\n \\n“\\nFacilitation\\n \\nPayments\\n”).\\n  \\nFacilitation\\n \\nPayments\\n \\ninclude\\n \\npayments\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nto\\n \\nexpedite\\n \\nroutine\\n \\nand\\n \\nnondiscretionary\\n \\nactivities,\\n \\nsuch\\n \\nas\\n \\nprocessing\\n \\npermit\\n \\nand\\n \\nlicense\\n \\napplications,\\n \\nscheduling\\n \\ninspections,\\n \\nand/or\\n \\nproviding\\n \\ninfrastructure\\n \\nservices\\n \\n(\\ne.g.\\n,\\n \\nwater,\\n \\nelectricity\\n \\nmail).\\n  \\nWSO2\\n \\nstrictly\\n \\nprohibits\\n \\nthe\\n \\noffer,\\n \\npromise,\\n \\nor\\n \\nprovision\\n \\nof\\n \\nFacilitation\\n \\nPayments\\n \\nto\\n \\nany\\n \\ndomestic\\n \\nor\\n \\nforeign\\n \\nlocal\\n \\nor\\n \\nfederal\\n \\ngovernment\\n \\nofficial,\\n \\nas\\n \\nthey\\n \\ncan\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws\\n \\nand\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndescribed\\n \\nabove.\\n \\n \\nPlease  note  that  in  some  cases,  government  agencies  may  impose  official  fees  that  may  be  paid  directly  in  \\nthe\\n \\nname\\n \\nof\\n \\na\\n \\ngovernmental\\n \\nentity\\n \\nor\\n \\nenterprise\\n \\nitself,\\n \\nas\\n \\nset\\n \\nout\\n \\nin\\n \\npublished\\n \\nfee\\n \\nschedules\\n \\nor\\n \\nother\\n \\nofficial\\n \\ndocuments.\\n  \\nThese\\n \\nofficial\\n \\ngovernment\\n \\nfees\\n \\ncan\\n \\nbe\\n \\npaid\\n \\nto\\n \\nexpedite\\n \\npassports,\\n \\nlicenses,\\n \\nor\\n \\nother\\n \\nservices,\\n \\nprovided\\n \\nthat\\n \\nthey\\n \\nare\\n \\ndeposited\\n \\nin\\n \\nthe\\n \\ntreasury\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nan\\n \\nofficial\\n \\ngovernment\\n \\nreceipt\\n \\nis\\n \\ncollected,\\n \\nand\\n \\nthe\\n \\nexpense\\n \\nis\\n \\naccurately\\n \\nrecorded\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n  \\nHowever,\\n \\nFacilitation\\n \\nPayments\\n \\nprovided\\n \\nfor\\n \\nthe\\n \\nbenefit\\n \\nof\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\n(\\ni.e.\\n,\\n \\nare\\n \\nnot\\n \\ndeposited\\n \\nin\\n \\nan\\n \\nofficial\\n \\ntreasury\\n \\naccount\\n \\nbelonging\\n \\nto\\n \\na\\n \\ngovernment)\\n \\nwill\\n \\nviolate\\n \\nthis\\n \\nPolicy.\\n     \\n \\n \\nVI.   I\\nNTERMEDIARIES\\n \\nAND\\n \\nB\\nUSINESS\\n \\nP\\nARTNERS\\n \\n \\nThis  Policy  prohibits  WSO2  Personnel  from  providing  bribes  or  other  improper  benefits  directly  as  well  \\nas\\n \\nindirectly\\n \\nthrough\\n \\nthird\\n \\nparties.\\n \\nThis\\n \\nrisk\\n \\ncan\\n \\narise\\n \\nin\\n \\ncases\\n \\nwhere\\n \\nthe\\n \\nCompany\\n \\nworks\\n \\nwith\\n \\nagents,\\n 6     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='74c3e57c-f8e3-438c-b95b-bc17adf9d045', embedding=None, metadata={'page_label': '7', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only consultants,  representatives,  lobbyists,  suppliers/vendors,  resellers,  distributors,  customs  or  other  brokers,  \\ncontractors,\\n \\nadvisors,\\n \\nother\\n \\nbusiness\\n \\npartners,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nthat\\n \\nperforms\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\nWSO2\\n \\n(collectively\\n \\n“\\nIntermediaries\\n”).\\n   \\n \\nIn  certain  cases,  WSO2  and  WSO2  Personnel  can  be  held  liable  under  the  FCPA  and  other  laws  even  if  \\none\\n \\ndoes\\n \\nnot\\n \\nexpressly\\n \\nauthorize\\n \\nan\\n \\nIntermediary\\n \\nto\\n \\nengage\\n \\nin\\n \\ncorruption,\\n \\nbut\\n \\nthey\\n \\ndo\\n \\nso\\n \\nanyway.\\n \\nThis\\n \\ncan\\n \\noccur\\n \\nif\\n \\none\\n \\n(i)\\n \\nhas\\n \\nactual\\n \\nknowledge\\n \\nor\\n \\na\\n \\nfirm\\n \\nbelief\\n \\nthat\\n \\na\\n \\nperson\\n \\nwill\\n \\nengage\\n \\nin\\n \\ncorruption\\n \\nor\\n \\n(ii)\\n \\nconsciously\\n \\ndisregards,\\n \\ndeliberately\\n \\nignores,\\n \\nor\\n \\nis\\n \\nwillfully\\n \\nblind\\n \\nto\\n \\nthe\\n \\nIntermediary’s\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\npractices.\\n  \\n \\nGiven  these  risks,  this  Policy  requires  that  (i)  appropriate,  risk-based  anti-corruption  due  diligence  is  \\nperformed\\n \\non\\n \\nIntermediaries\\n \\nto\\n \\nconfirm\\n \\nthat\\n \\nsuch\\n \\nIntermediary\\n \\ndoes\\n \\nnot\\n \\nhave\\n \\na\\n \\nhistory\\n \\nor\\n \\nreputation\\n \\nfor\\n \\ncorruption\\n \\nor\\n \\nsimilar\\n \\nwrong\\n \\ndoing,\\n \\nand\\n \\n(ii)\\n \\nthe\\n \\nIntermediary\\n \\nhas\\n \\nexecuted\\n \\na\\n \\nwritten\\n \\nagreement\\n \\ncontaining\\n \\nanti-corruption\\n \\ncompliance\\n \\nclauses.\\n \\nPlease\\n \\nconsult\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nfor\\n \\ninformation\\n \\nregarding\\n \\nWSO2’s\\n \\nIntermediary\\n \\ndue\\n \\ndiligence\\n \\nprocedures.\\n \\n \\nThroughout  any  relationship  with  an  Intermediary,  WSO2  Personnel  must  monitor  their  performance  to  \\nensure\\n \\nthat\\n \\nthey\\n \\ndo\\n \\nnot\\n \\nengage\\n \\nin\\n \\nactivities\\n \\nthat\\n \\nraise\\n \\ncorruption\\n \\nconcerns.\\n  \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nprovide\\n \\nguidance\\n \\non\\n \\nthe\\n \\ntypes\\n \\nof\\n \\nred\\n \\nflags\\n \\nthat\\n \\none\\n \\nshould\\n \\nmonitor\\n \\nbefore\\n \\nand\\n \\nafter\\n \\nengaging\\n \\nan\\n \\nIntermediary.\\n \\n \\nThis  Policy  requires  WSO2  Personnel  to  notify  the  Compliance  Officer  if  they  learn  of  any  Company  \\nIntermediary\\n \\nthat\\n \\nengages\\n \\nin\\n \\ncorrupt\\n \\nor\\n \\nother\\n \\nimproper\\n \\npractices.\\n \\nAlso,\\n \\nall\\n \\npayments\\n \\nto\\n \\nIntermediaries\\n \\nmust\\n \\nbe\\n \\naccurately\\n \\nreported\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks\\n \\nand\\n \\nrecords\\n \\nin\\n \\naccordance\\n \\nwith\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndiscussed\\n \\nabove.\\n \\n \\nVII.   G\\nIFTS\\n \\nAND\\n \\nH\\nOSPITALITIES\\n \\n \\nAnti-Corruption  Laws  prohibit  the  provision  or  acceptance  of  money  or  things  of  value  for  corrupt  or  \\nimproper\\n \\npurposes.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthis\\n \\nprohibition\\n \\nis\\n \\nlikely\\n \\nin\\n \\ninstances\\n \\nwhere\\n \\npersonal\\n \\nbenefits\\n \\nare\\n \\ngiven\\n \\nor\\n \\naccepted\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nnegotiation\\n \\nor\\n \\ntender\\n \\nbid.\\n \\nHowever,\\n \\nreasonably\\n \\npriced\\n \\ngifts,\\n \\nmeals,\\n \\nentertainment,\\n \\ntravel,\\n \\nand\\n \\nother\\n \\nbenefits\\n \\nprovided\\n \\nfor\\n \\nnon-corrupt\\n \\nbusiness\\n \\npromotion\\n \\nor\\n \\ngoodwill\\n \\npurposes\\n \\nmay\\n \\nbe\\n \\npermissible\\n \\nunder\\n \\nAnti-Corruption\\n \\nLaws\\n \\nin\\n \\ncertain\\n \\ncases.\\n  \\nFor\\n \\ninstance,\\n \\na\\n \\nplastic\\n \\npen,\\n \\na\\n \\nt-shirt,\\n \\na\\n \\ncoffee\\n \\nmug,\\n \\na\\n \\npaper\\n \\nweight,\\n \\nor\\n \\na\\n \\ncap\\n \\nof\\n \\nmoderate\\n \\nvalue\\n \\nand\\n \\nembossed\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nlogo\\n \\nwill\\n \\ngenerally\\n \\nnot\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws.\\n \\nHowever,\\n \\na\\n \\nfur\\n \\ncoat,\\n \\na\\n \\ncar,\\n \\nor\\n \\na\\n \\nvacation\\n \\nwill\\n \\nraise\\n \\nanticorruption\\n \\nconcerns,\\n \\nespecially\\n \\nif\\n \\nsuch\\n \\nbenefits\\n \\nare\\n \\nprovided\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nother\\n \\nperson\\n \\nwho\\n \\nis\\n \\nresponsible\\n \\nfor\\n \\nmaking\\n \\ndecisions\\n \\nin\\n \\nrelation\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n   \\n \\nWSO2  Personnel  must  also  ensure  that  the  provision  of  a  gift  or  other  benefit  does  not  violate  local  laws  \\nor\\n \\npolicies\\n \\nthat\\n \\napply\\n \\nin\\n \\nthe\\n \\ncountry\\n \\nwhere\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nthe\\n \\nbenefit\\n \\nis\\n \\nlocated.\\n  \\nSome\\n \\ncountries\\n \\nimpose\\n \\nexpress\\n \\nlimits\\n \\non\\n \\nthe\\n \\nvalue\\n \\nof\\n \\ngifts/benefits\\n \\nthat\\n \\na\\n \\nrecipient\\n \\ncan\\n \\naccept;\\n \\nother\\n \\ncountries\\n \\nban\\n \\nsuch\\n \\ngifts/benefits\\n \\naltogether\\n \\neven\\n \\nif\\n \\ngiven\\n \\nwith\\n \\nno\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\nintention.\\n \\n \\nWSO2  Personnel  must  obtain  the  approval  of  the  Compliance  Officer  prior  to  providing  gifts,  meals,  \\ntravel\\n \\nbenefits,\\n \\nand\\n \\nother\\n \\nhospitalities\\n \\nto\\n \\nemployees,\\n \\nofficials,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\ngovernment,\\n \\npolitical\\n \\nparty,\\n \\nstateowned\\n \\nentity,\\n \\nor\\n \\npublic\\n \\ninternational\\n \\norganization.\\n \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nhelp\\n \\ndetermine\\n \\n7     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='5e75fc0d-3e5e-4fae-b83d-7225f5dff1a7', embedding=None, metadata={'page_label': '8', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only whether  the  provision  of  the  benefit  is  permissible  under  applicable  Anti-Corruption  Laws.   If  the  expense  \\nis\\n \\napproved,\\n \\nits\\n \\nvalue\\n \\nand\\n \\nbusiness\\n \\npurpose\\n \\nmust\\n \\nbe\\n \\nrecorded\\n \\naccurately\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nCompany\\n \\npersonnel\\n \\nfrom\\n \\nproviding\\n \\ncash\\n \\nor\\n \\ngift\\n \\ncards\\n \\nor\\n \\ngift\\n \\ncertificates\\n \\nthat\\n \\ncan\\n \\neasily\\n \\nbe\\n \\nconverted\\n \\ninto\\n \\ncash.\\n \\n \\nIX.   O\\nTHER\\n \\nA\\nCTIVITIES\\n \\n \\nCorruption  concerns  can  arise  in  a  number  of  other  cases  including,  but  not  limited  to  (i)  joint  ventures  or  \\nteaming\\n \\narrangements\\n \\nwith\\n \\npublic\\n \\nor\\n \\nprivate-sector\\n \\npartners;\\n \\n(ii)\\n \\nmergers\\n \\nand\\n \\nacquisitions,\\n \\nespecially\\n \\nif\\n \\nthe\\n \\ntarget\\n \\nbusiness\\n \\nhas\\n \\nsignificant\\n \\ngovernment\\n \\ninteractions\\n \\nor\\n \\nan\\n \\ninternational\\n \\nprofile;\\n \\nand\\n \\n(iii)\\n \\ncharitable\\n \\nand\\n \\npolitical\\n \\ndonations.\\n \\nPlease\\n \\nconfer\\n \\nwith\\n \\nthe\\n \\nCompliance\\n \\nOfficer\\n \\nbefore\\n \\nengaging\\n \\nin\\n \\nthese\\n \\ntypes\\n \\nof\\n \\nactivities\\n \\nto\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nanti-corruption\\n \\ncompliance\\n \\nmeasures\\n \\nare\\n \\nobserved.\\n \\n \\n \\nX.   V\\nIOLATIONS\\n \\nAND\\n \\nC\\nONSEQUENCES\\n \\n \\nA  violation  of  this  Policy  will  result  in  appropriate  disciplinary  action,  including  demotion,  reassignment,  \\nadditional\\n \\ntraining,\\n \\nprobation,\\n \\nsuspension,\\n \\nor\\n \\neven\\n \\ntermination.\\n \\n \\nBoth  the  Company  and  Company  Personnel  may  be  subject  to  substantial  fines  and  penalties  for  violating  \\nAnti-Corruption\\n \\nLaws.\\n  \\nIn\\n \\nserious\\n \\ncases,\\n \\nindividuals\\n \\nmay\\n \\nface\\n \\nimprisonment,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\nassessment\\n \\nof\\n \\nmonetary\\n \\nfines\\n \\nand\\n \\npenalties.\\n  \\nIn\\n \\naddition,\\n \\nthe\\n \\nCompany\\n \\nmay\\n \\nface\\n \\nsuspension\\n \\nor\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts,\\n \\nthe\\n \\nloss\\n \\nof\\n \\nU.S.\\n \\nexport\\n \\nprivileges,\\n \\nand\\n \\ncertain\\n \\nother\\n \\nconsequences.\\n \\nThese\\n \\nresults\\n \\ncan\\n \\nbe\\n \\ndevastating\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\n \\nXI.   T\\nRAINING\\n \\nAND\\n \\nC\\nERTIFICATION\\n \\n \\n \\nAll  designated  personnel  must  undergo  anti-corruption  training  provided  by  WSO2.   The  nature,  content,  \\nand\\n \\nfrequency\\n \\nof\\n \\nthat\\n \\ntraining\\n \\nwill\\n \\nbe\\n \\ndetermined\\n \\nby\\n \\nWSO2\\n \\nbased\\n \\non\\n \\nrisk\\n \\nprofile.\\n  \\n \\nWSO2  may  require  certain  WSO2  Personnel  to  certify  compliance  with  this  Policy  on  a  periodic  basis.   \\nXII.   S\\nTATUS\\n \\n \\n \\nThe  Compliance  Officer  and/or  outside  counsel  will  review  this  Policy  on  a  periodic  basis  and  update  it,  \\nas\\n \\nappropriate,\\n \\nto\\n \\nreflect\\n \\nany\\n \\nchanges.\\n   \\n \\nThis  Policy  does  not  form  part  of  any  employment  contract  with  you  and  may  be  amended  at  any  time.   \\nThis\\n \\nPolicy\\n \\nshould\\n \\nbe\\n \\nread\\n \\nin\\n \\nconjunction\\n \\nwith\\n \\nWSO2’s\\n \\nother\\n \\npolicies.\\n \\n \\n \\n8     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='43551bf5-0a46-45f8-becb-d4356dd4d8ec', embedding=None, metadata={'page_label': '9', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only XIII.   R\\nEPORTING\\n/Q\\nUESTIONS\\n  \\n \\n \\nWSO2  Personnel  have  an  affirmative  obligation  to  report  all  violations  of  this  Policy  to  the  Compliance  \\nOfficer\\n \\nas\\n \\nfollows:\\n \\n \\nPuny  Navaratne  legal-compliance@wso2.com   \\nReports  may  also  be  submitted  anonymously  by  using  the  Company’s  hotline  number  \\n(800)\\n \\n461-9330\\n \\nor\\n \\nonline\\n \\nat\\n \\nwhistleblower.wso2.com.\\n    \\nHowever,\\n \\nwe\\n \\nencourage\\n \\nyou\\n \\nto\\n \\nconsider\\n \\nrevealing\\n \\nyour\\n \\nidentity\\n \\nso\\n \\nthat\\n \\nwe\\n \\ncan\\n \\nproperly\\n \\nfollow\\n \\nup\\n \\nand\\n \\ninvestigate\\n \\nalleged\\n \\nviolations.\\n \\nThe\\n \\nCompany\\n \\nwill\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nconfidentiality\\n \\nmeasures\\n \\nare\\n \\ntaken\\n \\nand\\n \\nwill\\n \\nnot\\n \\nretaliate\\n \\nagainst\\n \\nany\\n \\nindividual\\n \\nfor\\n \\nreporting\\n \\nviolations\\n \\nin\\n \\ngood\\n \\nfaith.\\n \\n \\nWSO2  Personnel  must  also  notify  the  Compliance  Officer  of  any  corrupt,  improper,  illegal,  or  other  \\nunusual\\n \\nrequests\\n \\nfor\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits\\n \\nmade\\n \\nby\\n \\ncustomers,\\n \\nIntermediaries,\\n \\nvendors,\\n \\nbusiness\\n \\npartners,\\n \\ngovernment\\n \\nofficials,\\n \\nor\\n \\nCompany\\n \\nemployees.\\n   \\nBy\\n \\nreporting\\n \\nsuch\\n \\nmatters,\\n \\nyou\\n \\nwill\\n \\nenable\\n \\nus\\n \\nto\\n \\nexplore\\n \\noptions\\n \\nto\\n \\nachieve\\n \\nour\\n \\nbusiness\\n \\ngoals\\n \\nwithout\\n \\nhaving\\n \\nto\\n \\ninteract\\n \\nwith\\n \\nsuch\\n \\npersons\\n \\nor\\n \\nprovide\\n \\nimproper\\n \\nbenefits.\\n \\n \\nIX.   ACKNOWLEDGEMENT\\n  \\n \\n \\nPlease  click here to  certify  and  acknowledge  that  you  have  read  and  understood  the  contents  of  this  Policy.  \\n9     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='b63099a8-4fb0-4c39-ab1c-43ac45eff3aa', embedding=None, metadata={'page_label': '10', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only ATTACHMENT  1   \\nANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL   \\n  \\nUNITED  KINGDOM   \\nT\\nHE\\n \\nUK\\n \\nB\\nRIBERY\\n \\nA\\nCT\\n \\n2010  \\n \\n  Among  various  matters,  the  UK  Bribery  Act  2010  (the  “ UKBA ”)  prohibits  individuals  and  entities  from  \\noffering,\\n \\npromising,\\n \\nor\\n \\ngiving\\n \\n(directly\\n \\nor\\n \\nindirectly\\n \\nthrough\\n \\na\\n \\nthird\\n \\nparty)\\n \\na\\n \\nfinancial\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nto\\n \\na\\n \\nrecipient\\n \\nwith\\n \\n(i)\\n \\nthe\\n \\nintention\\n \\nthat\\n \\nthe\\n \\nadvantage\\n \\ninduce\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\nperform\\n \\nimproperly\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity\\n \\nor\\n \\nto\\n \\nreward\\n \\na\\n \\nperson\\n \\nfor\\n \\nthe\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\nsuch\\n \\nfunction\\n \\nor\\n \\nactivity,\\n \\nor\\n \\n(ii)\\n \\nthe\\n \\nknowledge\\n \\nor\\n \\nbelief\\n \\nthat\\n \\nthe\\n \\nacceptance\\n \\nof\\n \\nthe\\n \\nadvantage\\n \\nwould\\n \\nitself\\n \\nconstitute\\n \\nan\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nUKBA\\n \\nwill\\n \\noccur\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nadvantage\\n \\nis\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nan\\n \\nemployee\\n \\nof\\n \\na\\n \\nprivatesector\\n \\nentity.\\n \\n   The  UKBA  contains  four  principal  offenses  as  follows:  (i)  offering,  promising,  or  giving  of  a  bribe  to  \\nanother\\n \\nperson\\n \\n(Section\\n \\n1);\\n \\n(ii)\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\na\\n \\nbribe\\n \\n(Section\\n \\n2);\\n \\n(iii)\\n \\nbribery\\n \\nof\\n \\na\\n \\nforeign\\n \\n(non-UK)\\n \\npublic\\n \\nofficial\\n \\n(Section\\n \\n6);\\n \\nand\\n \\n(iv)\\n \\nfailure\\n \\nby\\n \\ncertain\\n \\ncommercial\\n \\norganizations\\n \\nto\\n \\nprevent\\n \\nSection\\n \\n1\\n \\nor\\n \\n6\\n \\nbribery\\n \\noffenses\\n \\nby\\n \\ntheir\\n \\nassociated\\n \\npersons\\n \\n(including\\n \\nemployees,\\n \\ncontractors,\\n \\nIntermediaries,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\na\\n \\ncompany)\\n \\nof\\n \\nany\\n \\nnationality\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(Section\\n \\n7).\\n  \\nThe\\n \\nUKBA\\n \\nprovides\\n \\na\\n \\nstatutory\\n \\ndefense\\n \\nto\\n \\na\\n \\nSection\\n \\n7\\n \\nviolation\\n \\nfor\\n \\ncompanies\\n \\nthat\\n \\ncan\\n \\ndemonstrate\\n \\nthat\\n \\nthey\\n \\nhad\\n \\nin\\n \\nplace\\n \\nadequate\\n \\nsystems\\n \\nand\\n \\ncontrols\\n \\ndesigned\\n \\nto\\n \\nprevent\\n \\noffenses\\n \\nunder\\n \\nUKBA.\\n \\nThis\\n \\nPolicy\\n \\nis\\n \\npart\\n \\nof\\n \\nthe\\n \\nCompany’s\\n \\noverall\\n \\neffort\\n \\nto\\n \\nestablish\\n \\nsuch\\n \\nsystems\\n \\nand\\n \\ncontrols.\\n  \\n   Courts  in  the  United  Kingdom  exercise  broad  jurisdiction  over  UK  as  well  as  non-UK  persons  who  \\ncommit\\n \\nUKBA\\n \\noffenses.\\n  \\nThe\\n \\nCompany\\n \\nmaintains\\n \\na\\n \\nUK\\n \\nsubsidiary.\\n \\nIt\\n \\nis\\n \\nclear\\n \\nthat\\n \\nboth\\n \\nthis\\n \\nUK\\n \\nsubsidiary\\n \\nand\\n \\nmost\\n \\nof\\n \\nits\\n \\nemployees\\n \\nwill\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nthe\\n \\nUKBA.\\n  \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-UK\\n \\nentities\\n \\nand\\n \\nemployees\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nUKBA\\n \\njurisdiction.\\n \\n    Under  the  UKBA,  individuals  guilty  of  bribery  may  be  subject  to  imprisonment  for  up  to  10  years  and/or  \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount.\\n \\nCommercial\\n \\norganizations\\n \\nguilty\\n \\nof\\n \\nbribery\\n \\nor\\n \\nfailure\\n \\nto\\n \\nprevent\\n \\nbribery\\n \\nmay\\n \\nalso\\n \\nbe\\n \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount\\n \\nas\\n \\nwell\\n \\nas\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts.\\n  \\nIn\\n \\naddition,\\n \\nUKBA\\n \\noffenses\\n \\ncould\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws\\n \\nsuch\\n \\nas\\n \\nthe\\n \\nUK\\n \\nProceeds\\n \\nof\\n \\nCrime\\n \\nAct\\n \\n2002,\\n \\nwhich\\n \\ncontains\\n \\nthe\\n \\nUK’s\\n \\nprincipal\\n \\nmoney\\n \\nlaundering\\n \\noffenses.\\n \\n    *   *   *   *   *     SRI  LANKA   \\nThe  legal  framework  for  the  prevention,  investigation  and  punishment  of  corruption  is  primarily  reflected  \\nin\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments).\\n  \\n \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='b4764dc2-e803-434c-8331-41fe495858ee', embedding=None, metadata={'page_label': '11', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only   The  law  prohibits  persons  from  offering  gratifications  and  rewards  to  certain  categories  of  persons  as  \\ninducements\\n \\nand\\n \\nrewards\\n \\nfor\\n \\nthe\\n \\nperformance\\n \\nor\\n \\nnonperformance\\n \\nof\\n \\nspecified\\n \\nactivities.\\n \\nThey\\n \\nare\\n \\n(a)\\n \\njudicial\\n \\nofficers\\n \\nand\\n \\nMembers\\n \\nof\\n \\nParliament\\n \\nin\\n \\nrespect\\n \\nof\\n \\ntheir\\n \\nofficial\\n \\nduties;\\n \\n(b)\\n \\npolice\\n \\nofficers,\\n \\npeace\\n \\nofficers\\n \\nor\\n \\nother\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ninterfering\\n \\nwith\\n \\nthe\\n \\ndue\\n \\nadministration\\n \\nof\\n \\njustice,\\n \\nor\\n \\nprocuring\\n \\nor\\n \\nfacilitating\\n \\nthe\\n \\ncommission\\n \\nof\\n \\nany\\n \\noffence,\\n \\nor\\n \\nprotecting\\n \\noffenders\\n \\nfrom\\n \\ndetection\\n \\nor\\n \\npunishment,\\n \\nor\\n \\nabusing\\n \\nofficial\\n \\npowers\\n \\nto\\n \\nthe\\n \\ninjury\\n \\nor\\n \\ndetriment\\n \\nof\\n \\nany\\n \\nperson;\\n \\n(c)\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ntheir\\n \\nassistance\\n \\nor\\n \\ninfluence\\n \\nin\\n \\npromoting\\n \\nthe\\n \\nprocurement\\n \\nof\\n \\nany\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nfor\\n \\nany\\n \\nwork,\\n \\nservice\\n \\nor\\n \\nthe\\n \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial\\n \\nor\\n \\nsubstance,\\n \\nor\\n \\nin\\n \\nthe\\n \\nexecution\\n \\nof\\n \\nany\\n \\ncontract,\\n \\nor\\n \\nin\\n \\nthe\\n \\npayment\\n \\nof\\n \\nthe\\n \\nprice\\n \\nor\\n \\nconsideration\\n \\nor\\n \\nof\\n \\nany\\n \\nsubsidy\\n \\nin\\n \\nrespect\\n \\nthereof;\\n \\n(d)\\n  \\na\\n \\ntenderer\\n \\nfor\\n \\na\\n \\ncontract\\n \\nto\\n \\nwithdraw\\n \\nthe\\n \\ntender,\\n \\nor\\n \\nfor\\n \\nwithdrawing\\n \\na\\n \\ntender\\n \\nmade\\n \\nfor\\n \\na\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nintent\\n \\nof\\n \\nobtaining\\n \\nsuch\\n \\ncontract\\n \\nfor\\n \\nwork,\\n \\nservice\\n \\nor\\n  \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial,\\n \\nor\\n \\nsubstance;\\n \\n(e)\\n  \\npublic\\n \\nofficers\\n \\nto\\n \\nperform,\\n \\nabstain\\n \\nfrom\\n \\nperforming,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact\\n \\nwhether\\n \\nby\\n \\nthat\\n \\npublic\\n \\nofficer\\n \\nor\\n \\nby\\n \\nany\\n \\nother\\n \\npublic\\n \\nofficer,\\n \\nor\\n \\nassisting,\\n \\nfavoring,\\n \\nhindering\\n \\nor\\n \\ndelaying\\n \\nany\\n \\nperson\\n \\nin\\n \\nthe\\n \\ntransaction\\n \\nof\\n \\nany\\n \\nbusiness\\n \\nwith\\n \\nthe\\n \\nGovernment;\\n \\n(f)\\n \\npersons\\n \\nto\\n \\nprocure\\n \\nthe\\n \\nGovernment\\n \\nto\\n \\npay\\n \\nany\\n \\nclaim,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nprevent\\n \\nappointment\\n \\nto\\n \\nany\\n \\noffice,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nsecure\\n \\nany\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nother\\n \\nbenefit\\n \\nfrom\\n \\nthe\\n \\nGovernment,\\n \\nor\\n \\nprevent\\n \\nthe\\n \\nsecuring\\n \\nof\\n \\nany\\n \\nsuch\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nbenefit\\n \\nby\\n \\nsuch\\n \\nother\\n \\nperson;\\n \\n(g)\\n \\npublic\\n \\nofficer\\n \\nemployed\\n \\nin\\n \\na\\n \\ngovernment\\n \\ndepartment,\\n \\noffice\\n \\nor\\n \\nestablishment\\n \\nwhile\\n \\nhaving\\n \\ndealings\\n \\nof\\n \\nany\\n \\nkind\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nthrough\\n \\nsuch\\n \\nan\\n \\nentity,\\n \\nor\\n \\nwithin\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nsuch\\n \\ndealings\\n \\n(provided\\n \\nthat\\n \\nif\\n \\nsuch\\n \\ngratification\\n \\nwas\\n \\npaid\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nthe\\n \\ndealings\\n \\nit\\n \\nshall\\n \\nnot\\n \\nbe\\n \\nconsidered\\n \\nan\\n \\noffence\\n \\nif\\n \\nit\\n \\ncan\\n \\nbe\\n \\nproved\\n \\nthat\\n \\nit\\n \\nwas\\n \\noffered\\n \\nin\\n \\ngood\\n \\nfaith\\n \\nfor\\n \\na\\n \\npurpose\\n \\nnot\\n \\nconnected\\n \\nwith\\n \\nor\\n \\nunrelated\\n \\nto\\n \\nsuch\\n \\ndealings,\\n \\nand\\n \\nwhen\\n \\nit\\n \\nwas\\n \\noffered,\\n \\nthere\\n \\nwas\\n \\nno\\n \\nhope\\n \\nor\\n \\nexpectation\\n \\nof\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nit\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nfor\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nthe\\n \\ngratification\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nor\\n \\nreward\\n \\nfor\\n \\nthe\\n \\npublic\\n \\nofficer\\n \\ndoing\\n \\nor\\n \\nforbearing\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nconnected\\n \\nwith\\n \\nsuch\\n \\ndealings);\\n \\n(h)\\n \\nmembers,\\n \\nofficers\\n \\nor\\n \\nemployees\\n \\nof\\n \\nlocal\\n \\nauthorities\\n \\nor\\n \\nscheduled\\n \\ninstitutions\\n \\nfor\\n \\nvoting\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nvoting\\n \\nat\\n \\nmeetings\\n \\nof\\n \\nsuch\\n \\nbodies\\n \\nfor\\n \\nor\\n \\nagainst\\n \\nmatters\\n \\narising\\n \\nbefore\\n \\nthem,\\n \\nor\\n \\ntheir\\n \\nperformance,\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nperforming,\\n \\nor\\n \\naiding\\n \\nin\\n \\nprocuring,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact,\\n \\nor\\n \\naiding,\\n \\nprocuring,\\n \\nor\\n \\npreventing\\n \\nthe\\n \\npassing\\n \\nof\\n \\nany\\n \\nvote\\n \\nor\\n \\ngranting\\n \\nof\\n \\nany\\n \\ncontract\\n \\nor\\n \\nadvantage\\n \\nin\\n \\nfavor\\n \\nof\\n \\nany\\n \\nperson.\\n  \\n \\nEach  of  the  above  provisions  have  corresponding  offence  in  respect  of  the  receipt  of  gratifications.      The  Bribery  Act  provides  for  imprisonment  of  up  to  seven  years  and  fines  of  up  to  five  thousand  Sri  \\nLankan\\n \\nrupees\\n \\nfor\\n \\nthe\\n \\ncommission\\n \\nof\\n \\noffences.\\n  \\nSri\\n \\nLankan\\n \\ncourts\\n \\nmay\\n \\nalso\\n \\nimpose\\n \\npenalties\\n \\namounting\\n \\nto\\n \\nthe\\n \\nvalue\\n \\nof\\n \\nthe\\n \\ngratification\\n \\nif\\n \\nthe\\n \\nconviction\\n \\nis\\n \\nentered\\n \\nby\\n \\na\\n \\nHigh\\n \\nCourt.\\n   \\n   Similar  offences  have  been  created  in  respect  of  members  of  public  authorities  by  the  Public  Bodies  \\n(Prevention\\n \\nof\\n \\nCorruption)\\n \\nAct\\n \\nNo\\n \\n13\\n \\nof\\n \\n1950.\\n \\n \\nThe  Bribery  Act  is  enforced  through  the  Commission  to  Investigate  Allegations  of  Bribery  or  Corruption  \\nAct\\n \\nNo.\\n \\n19\\n \\nof\\n \\n1994.\\n  \\nThe\\n \\nCommission\\n \\nhas\\n \\nwide\\n \\npowers\\n \\nof\\n \\ninvestigation\\n \\nincluding\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nas\\n \\nwell\\n \\nas\\n \\nof\\n \\nrequiring\\n \\ndeclaration\\n \\nof\\n \\nassets\\n \\nand\\n \\nliabilities\\n \\nby\\n \\nMembers\\n \\nof\\n \\nParliament,\\n \\njudges,\\n \\npublic\\n \\nofficials\\n \\nof\\n \\nGovernment\\n \\ndepartments,\\n \\nministries,\\n \\nand\\n \\nlocal\\n \\nauthorities,\\n \\nchairpersons\\n \\nand\\n \\nstaff\\n \\nof\\n \\npublic\\n \\ncorporations,\\n \\ncandidates\\n \\nfor\\n \\nelected\\n \\npublic\\n \\noffice\\n \\nand\\n \\nelected\\n \\nofficials\\n \\nunder\\n \\nthe\\n \\nDeclaration\\n \\nof\\n \\nAssets\\n \\nand\\n \\nLiabilities\\n \\nLaw,\\n \\nNo.\\n \\n1\\n \\nof\\n \\n1975.\\n  \\n \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='195d34a8-a985-45cb-8cc6-2d2e1d480c51', embedding=None, metadata={'page_label': '12', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only Sri  Lanka  also  has  a  domestic  legal  regime  against  money-laundering  which  includes  the  Prevention  of  \\nMoney-\\n \\nLaundering\\n \\nAct,\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct\\n \\nand\\n \\nthe\\n \\nConvention\\n \\non\\n \\nthe\\n \\nSuppression\\n \\nof\\n \\nTerrorist\\n \\nFinancing\\n \\nAct.\\n  \\nOffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nand\\n \\nother\\n \\ncorruption-related\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nPenal\\n \\nCode\\n \\nare\\n \\nconsidered\\n \\npredicate\\n \\noffences\\n \\nfor\\n \\nthe\\n \\npurposes\\n \\nof\\n \\nthe\\n \\nPrevention\\n \\nof\\n \\nMoney-Laundering\\n \\nAct\\n \\nand\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct.\\n  \\n \\n     *   *   *   *   *     B\\nRAZIL\\n \\n \\nTHE  BRAZILIAN  ANTICORRUPTION  ACT  2013   \\nLaw  No.  12,846/2013  (the  Brazilian  Anticorruption  Act  or  Lei  Anticorrupção  –  “ LAC ”)  provides  for  \\nstrict\\n \\nliability\\n \\nto\\n \\ncompanies,\\n1\\n \\nin\\n \\nthe\\n \\nadministrative\\n \\nand\\n \\ncivil\\n \\nspheres,\\n \\nfor\\n \\nwrongful\\n \\nacts\\n \\ncarried\\n \\nout\\n \\nin\\n \\ntheir\\n \\ninterest\\n \\nor\\n \\nfor\\n \\ntheir\\n \\nbenefit.\\n \\n   The  LAC  is  applicable  to  activities  after  January  2014  and  the  main  offenses  are:  (i)  promising,  offering  \\nor\\n \\ngiving,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nundue\\n \\nadvantage\\n \\nto\\n \\na\\n \\npublic\\n \\nagent\\n \\nor\\n \\nthird\\n \\nperson\\n \\nrelated\\n \\nto\\n \\nit;\\n \\n(ii)\\n \\nfinancing\\n \\nor\\n \\nin\\n \\nany\\n \\nway\\n \\nsponsoring\\n \\nthe\\n \\npractice\\n \\nof\\n \\nwrongdoings\\n \\ndescribed\\n \\nin\\n \\nthe\\n \\nAct;\\n \\n(iii)\\n \\nusing\\n \\na\\n \\nthird\\n \\nparty\\n \\nto\\n \\nconceal\\n \\nor\\n \\nsimulate\\n \\nits\\n \\nactual\\n \\ninterests\\n \\nor\\n \\nthe\\n \\nidentity\\n \\nof\\n \\nthe\\n \\nbeneficiaries\\n \\nof\\n \\nthe\\n \\nillegal\\n \\nacts\\n \\nagainst\\n \\nthe\\n \\npublic\\n \\nadministration;\\n \\n(iv)\\n \\nengaging\\n \\nin\\n \\nfraudulent\\n \\nacts\\n \\nin\\n \\npublic\\n \\ntenders,\\n \\nsuch\\n \\nas\\n \\nparticipating\\n \\nin\\n \\nbid\\n \\nrigging\\n \\nor\\n \\ndisturbing\\n \\nany\\n \\nstep\\n \\nof\\n \\nthe\\n \\npublic\\n \\ntender;\\n \\nand\\n \\n(v)\\n \\nobstructing\\n \\nor\\n \\nhampering\\n \\nthe\\n \\nsurveillance\\n \\nor\\n \\ninvestigations\\n \\nof\\n \\npublic\\n \\nentities.\\n \\n   In  the  administrative  sphere,  legal  entities  that  are  found  guilty  of  breaching  the  LAC  are  subject  to  a  fine  \\nof\\n \\n0.1%\\n \\nto\\n \\n20%\\n \\nof\\n \\nthe\\n \\ngross\\n \\nrevenue,\\n \\nless\\n \\ntaxes,\\n \\nregistered\\n \\nin\\n \\nthe\\n \\nyear\\n \\nprior\\n \\nto\\n \\nthe\\n \\ninitiation\\n \\nof\\n \\nthe\\n \\nadministrative\\n \\nproceedings,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\npublication\\n \\nof\\n \\nthe\\n \\ndecision.\\n \\nMoreover,\\n \\nother\\n \\npenalties\\n \\nmay\\n \\nbe\\n \\nenforced\\n \\nin\\n \\nthe\\n \\ncivil\\n \\nsphere\\n \\nby\\n \\ncourts,\\n \\nsuch\\n \\nas:\\n \\n(i)\\n \\nseizure\\n \\nof\\n \\nassets\\n \\nobtained\\n \\nthrough\\n \\nillegal\\n \\npractice;\\n \\n(ii)\\n \\nsuspension\\n \\nor\\n \\npartial\\n \\nshutdown\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity’s\\n \\nactivities;\\n \\n(iii)\\n \\ncompulsory\\n \\ntermination\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity;\\n \\nand\\n \\n(iv)\\n \\nprohibition\\n \\nof\\n \\nreceiving\\n \\nincentives,\\n \\nsubsidies,\\n \\ngrants,\\n \\ndonations\\n \\nor\\n \\nloans\\n \\nfrom\\n \\npublic\\n \\nbodies\\n \\nor\\n \\nentities\\n \\nor\\n \\nfrom\\n \\npublic\\n \\nfinancial\\n \\ninstitutions\\n \\nor\\n \\npublicly-controlled\\n \\nfinancial\\n \\ninstitutions,\\n \\nfrom\\n \\n1\\n \\nto\\n \\n5\\n \\nyears.\\n \\n   The  existence  of  an  effective  compliance  program  may  be  considered  a  mitigating  factor  in  the  evaluation  \\nof\\n \\nthe\\n \\napplicable\\n \\nfine\\n \\nto\\n \\nbe\\n \\napplied\\n \\nto\\n \\nthe\\n \\nlegal\\n \\nentity.\\n \\n   The  Company  maintains  a  Brazilian  subsidiary.  It  is  clear  that  the  Brazilian  subsidiary  may  be  held  liable  \\nunder\\n \\nthe\\n \\nLAC.\\n \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-Brazilian\\n \\nentities\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nLAC\\n \\njurisdiction.\\n \\n   \\n1\\n  In  this  sense,  the  authorities  are  not  required  to  show  intent  or  fault  of  the  legal  entity.  The  mere  fact  that  there  is  \\nmateriality\\n \\nas\\n \\nto\\n \\nthe\\n \\nviolation\\n \\nand\\n \\nthat\\n \\nthe\\n \\nviolation\\n \\nhappened\\n \\nin\\n \\nthe\\n \\ninterest\\n \\nor\\n \\nbenefit\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity\\n \\nis\\n \\nsufficient\\n \\nto\\n \\nconsider\\n \\nthat\\n \\nthere\\n \\nis\\n \\na\\n \\nbreach\\n \\nto\\n \\nthe\\n \\nAct.\\n \\n  \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='f6cad862-e011-4155-ad8a-495abdfb0e32', embedding=None, metadata={'page_label': '13', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only T\\nHE\\n \\nB\\nRAZILIAN\\n \\nI\\nMPROBITY\\n \\nA\\nCT\\n \\n1992  \\n \\n  In  a  broad  sense,  Law  No.  8,429/1992  (the  Brazilian  Improbity  Act  or  Lei  de  Improbidade  Administrativa   –  “ LIA ”)  is  applicable  to  (i)  facts  that  also  constitute  a  violation  to  the  LAC  but  happened  before  January   \\n  2014;  and  (ii)  facts  that  happened  after  January  2014  and  do  not  constitute  a  breach  to  the  LAC,  but  are  a  \\nbreach\\n \\nof\\n \\nthe\\n \\nLIA.\\n \\n   There  are  three  broad  types  of  misconduct  provided  for  in  the  LIA:  (i)  unjust  enrichment;  (ii)  damage  to  \\nthe\\n \\npublic\\n \\ntreasury;\\n \\nand\\n \\n(iii)\\n \\nacts\\n \\nin\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nprinciples\\n \\nthat\\n \\ngovern\\n \\nthe\\n \\nPublic\\n \\nAdministration.\\n  \\nThe\\n \\nLIA\\n \\nprovides\\n \\nfor\\n \\nsanctions\\n \\non\\n \\npublic\\n \\nagents,\\n \\nas\\n \\nwell\\n \\nas\\n \\non\\n \\nprivate\\n \\nentities\\n \\nand\\n \\nindividuals\\n \\nthat\\n \\nwillfully\\n \\naided\\n \\nor\\n \\nparticipated\\n \\nin\\n \\nimprobity\\n \\nacts.\\n  \\nIts\\n \\nmain\\n \\nsanctions\\n \\nare\\n \\n(i)\\n \\nforfeiture\\n \\nof\\n \\nassets\\n \\nor\\n \\nvalues\\n \\nunlawfully\\n \\nobtained;\\n \\n(ii)\\n \\ndismissal\\n \\nfrom\\n \\npublic\\n \\noffice;\\n \\n(iii)\\n \\npolitical\\n \\nblacklisting;\\n \\n(iv)\\n \\npayment\\n \\nof\\n \\nfines\\n \\nequivalent\\n \\nto\\n \\nthe\\n \\nunlawfully\\n \\nobtained\\n \\namounts;\\n \\n(v)\\n \\nprohibition\\n \\nagainst\\n \\ncontracting\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nor\\n \\nreceiving\\n \\ntax\\n \\nor\\n \\ncredit\\n \\nincentives,\\n \\ndirectly\\n \\nor\\n \\nindirectly;\\n \\nand\\n \\n(vi)\\n \\npayment\\n \\nof\\n \\nfines.\\n  \\nIn\\n \\norder\\n \\nfor\\n \\na\\n \\nsanction\\n \\nto\\n \\nbe\\n \\napplied,\\n \\nit\\n \\nis\\n \\nnecessary\\n \\nto\\n \\nshow\\n \\nintent\\n \\nof\\n \\nthe\\n \\nwrongdoer.\\n \\n   O\\nTHER\\n \\nP\\nOTENTIAL\\n \\nL\\nIABILITIES\\n \\n \\nIn  addition  to  the  LAC  and  the  LIA,  private  and  public  entities  can  request  compensation  for  collective  or  \\nmoral\\n \\ndamages\\n \\nresulting\\n \\nfrom\\n \\ncorruption\\n \\ncases,\\n \\nas\\n \\nprovided\\n \\nby\\n \\nBrazilian\\n \\nClass\\n \\nAction\\n \\nLaw,\\n \\nand\\n \\nentities\\n \\ndeemed\\n \\nto\\n \\nbe\\n \\nharmed/damaged\\n \\nby\\n \\nthe\\n \\nwrongdoing\\n \\ncan\\n \\nfile\\n \\na\\n \\nlawsuit\\n \\nclaiming\\n \\ncompensation\\n \\nfor\\n \\ndamages.\\n \\nAlso,\\n \\nthe\\n \\nBrazilian\\n \\nFederal\\n \\nCourt\\n \\nof\\n \\nAccounts\\n \\ncan\\n \\nimpose\\n \\nsanctions\\n \\nif\\n \\nthey\\n \\nfind\\n \\ncontract\\n \\nfraud\\n \\nwhile\\n \\nauditing\\n \\npublic\\n \\nentities.\\n \\n   Finally,  the  Brazilian  Criminal  Code  sets  forth  that  corruption  may  result  in  imprisonment  for  up  to  16  \\nyears\\n \\nfor\\n \\nindividuals.\\n \\n        \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "pprint.pprint(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCaXjM9uPQUf",
        "outputId": "4ca9d47f-fa06-4d3a-a05f-d244c699d82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(id_='/content/data/2502.09838v3.pdf_part_0', embedding=None, metadata={'page_label': '1', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='HealthGPT: A Medical Large Vision-Language Model for Unifying\\nComprehension and Generation via Heterogeneous Knowledge Adaptation\\nTianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\\nMengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\\n1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\\n4The Hong Kong University of Science and Technology,5National University of Singapore\\nProject Page\\n Code\\n1. X-Ray \\nComprehension\\n2. CT\\nComprehension\\n3. MRI\\nComprehension\\nComp. Perf.\\n7 Medical Multi-Modal Comprehension Tasks\\nGen. \\nPerformance\\n5 Medical Multi-Modal Generation Tasks\\nList all anatomical locations showing \\npulmonary edema, hazy opacity, or \\nmediastinal displacement.\\nLeft hilar structures, left lung,\\nright hilar structures, right lung.\\nWhich abdominal organ shows any \\nindication of a lesion or abnormality \\nin the CT image?\\nNo abdominal organs show\\nany clear indications of lesions\\nor abnormalities.\\nCould you explain what this mass\\nin the MRI means for my health? \\nIs it very serious?\\nCertainly, the MRI shows a\\ndefined mass in your left nasal\\ncavity. There is no sign of the ...\\n4. Microsopy Comprehension\\n What is the purpose of the different \\nmembrane treatments used in this \\nstudy?\\nThe purpose of the different\\nmembrane treatments used…\\n5. OCT Comprehension\\n What is the purpose of comparing \\nthe OCT structure image and OCTA \\nimage with H&E histology?\\nTo confirm the histological position\\nof the obtained OCT brain images.\\n6. Fundus\\nComprehension\\n What specific findings or pathological \\nchanges can be observed in this \\nfundus image?\\nThe fundus image appears normal with\\nno noticeable signs of pathology…\\n7. Ultrasound\\nComprehension\\n What type of imaging technique \\nis used in this image?\\nThe image is a sagittal gray-\\nscale ultrasonographic…\\n1. CT2MRI\\nGeneration\\nI need a version of this CT representation \\nin MRI.\\nThe image has\\nbeen transformed\\ninto MRI.\\n2. MRI2CT\\nGeneration\\nTransform the MRI display into a \\nCT image.\\nHere is the CT\\nversion of the\\nMRI image.\\n3. Image Reconstruction\\nReconstruct the following \\nmedical images.\\nHere is the reconstructed\\nmedical image you need.\\n4. Super Resolution\\nCould you improve the quality\\nof this MRI image?\\nHere is the image with\\nimproved resolution.\\n5. Report-to-CXR\\nThe X-ray shows no \\npleural effusion or \\npneumothorax.\\nHere is the\\nchest X-ray\\nimage for\\nyou.\\nGen. Perf.\\nFigure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\\nunified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\\nplex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\\nAbstract\\nWe present HealthGPT, a powerful Medical Large Vision-\\nLanguage Model (Med-LVLM) that integrates medical vi-\\nsual comprehension and generation capabilities within a uni-\\nfied autoregressive paradigm. Our bootstrapping philosophy\\nis to progressively adapt heterogeneous comprehension and\\ngeneration knowledge to pre-trained large language mod-\\nels (LLMs). This is achieved through a novel heterogeneous\\nlow-rank adaptation (H-LoRA) technique, which is com-\\nplemented by a tailored hierarchical visual perception ap-\\nproach and a three-stage learning strategy. To effectively\\nlearn the HealthGPT, we devise a comprehensive medi-\\ncal domain-specific comprehension and generation dataset\\ncalled VL-Health. Experimental results demonstrate ex-\\nceptional performance and scalability of HealthGPT in\\nmedical visual unified tasks. Our project can be accessed at\\nhttps://github.com/DCDmllm/HealthGPT.\\n1 Introduction\\nLarge Vision-Language Models (LVLMs) (Liu et al. 2023;\\nOpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\\ndemonstrated outstanding open-world visual comprehension\\nand reasoning abilities through language-based interactive\\ndialogue over the past years, simultaneously opening up\\nnew opportunities for applications in specialized domains.\\n1\\narXiv:2502.09838v3  [cs.CV]  21 Feb 2025', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_1', embedding=None, metadata={'page_label': '2', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Specifically, recent studies (Li et al. 2024a; Tu et al. 2024)\\nhave utilized pre-trained large language models (LLMs) and\\nvisual instruction data to build interactive diagnostic tools\\nand treatment planning systems, revealing the immense po-\\ntential of LVLMs in medical scenarios. However, these stud-\\nies primarily concentrate on visual comprehension tasks that\\nproduce text-based outputs, such as medical visual ques-\\ntion answering (Li et al. 2024a) or report generation (Nath\\net al. 2024), and deficient the “drawing” capability needed\\nfor medical visual generation. In practice, integrating visual\\ncomprehension and generation can significantly enhance the\\nmultifunctionality of medical LVLMs.\\nRecent studies have increasingly focused on developing\\nunified LVLMs capable of comprehending and generating\\ncontent across diverse visual modalities. Earlier approaches\\npredominantly utilized continuous visual tokens fed into\\nLLMs, using the LLMs themselves as conditional genera-\\ntors for external generative models (Ge et al. 2024; Wu et al.\\n2023; Dong et al. 2023). More recent research has explored\\nthe use of discrete visual tokens for image representation and\\ngeneration within a fully autoregressive framework (Team\\n2024; Wang et al. 2024a; Xie et al. 2024). These meth-\\nods not only enhance controllability but also demonstrate\\nearly success in open-world, any-to-any tasks, highlighting\\nthe preliminary potential of a unified autoregressive learning\\nparadigm in multi-modal tasks.\\nWhile unified LVLMs have achieved initial success in\\ngeneral scenarios, such a unified framework remains under-\\nexplored in the medical domain. Adapting the aforemen-\\ntioned general unified model paradigm to the medical do-\\nmain presents two major challenges: (i) High-scale and\\n-quality Data Limitations . Open-world models necessi-\\ntate extensive pre-training on billions or even more diverse,\\nmulti-modal data samples for comprehension and genera-\\ntion tasks (Lu et al. 2024; Team 2024). However, the ac-\\ncessible medical data significantly lacks in scale and qual-\\nity compared to natural multi-modal datasets. Its special-\\nized and domain-specific characteristics make it challenging\\nto develop a unified medical model from scratch. (ii) Con-\\nflicts between Comprehension and Generation . Compre-\\nhension tasks often strip away visual details to focus on\\nabstraction, while generation tasks require detailed preser-\\nvation, making tokens sensitive to all visual alterations. As\\nshown in Figure 2, which features experiments conducted on\\nmedical images, the performance in comprehension (or gen-\\neration) tasks steadily decreases as the proportion of genera-\\ntion (or comprehension) data increases, and vice versa. This\\nhighlights a dilemma in autoregressive multi-modal training,\\nstemming from the need to maintain consistency between\\npre- and post-LVLMs. While some methods have explored\\nmutual enhancement between comprehension and genera-\\ntion (Pan et al. 2024; Tong et al. 2024), improvements still\\nexhibit diminishing returns, with performance degradation\\nremaining a significant issue.\\n(a) (b)\\nFigure 2: With a fixed amount of comprehension (genera-\\ntion) data, increasing the proportion of the other type leads\\nto significant performance degradation.\\nTo tackle the aforementioned challenges, we propose\\nHealthGPT (see Figure 1) , which progressively adapts a\\npre-trained LLM as an unified medical multi-modal model\\nwith a small amount of visual instruction data. We de-\\nvise innovative Parameter-Efficient Fine-Tuning (PEFT) ap-\\nproach (Ding et al. 2023), calledHeterogeneous Low-Rank\\nAdaptation (H-LoRA), which decouples the learning pro-\\ncess of LVLMs for comprehension and generation tasks. In-\\nspired by the plug-and-play nature of LoRA (Hu et al. 2021),\\nH-LoRA enables the model to store heterogeneous compre-\\nhension and generation knowledge in independent “plug-\\nins”, thus avoiding joint optimization issues caused by con-\\nflicts between comprehension and generation tasks. In addi-\\ntion, we also consider the variety of sub-tasks among com-\\nprehension or generation tasks. Qualitative research high-\\nlights the limitations of a single LoRA in handling multi-\\ndimensional task scenarios, mainly due to catastrophic for-\\ngetting and interference (Liu et al. 2024d; Lin et al. 2024).\\nTo address this, we draw on the concept of Mixture of Ex-\\nperts (MoE) (Masoudnia and Ebrahimpour 2014) and in-\\ntroduce LoRA experts. The aim is to dynamically transfer\\ntask-shared knowledge to adapt to downstream tasks. Unlike\\nMoELoRA (Luo et al. 2024a), H-LoRA employs reversible\\nmatrix block multiplication to combine LoRA experts, sig-\\nnificantly reducing the overhead of multiple matrix multi-\\nplications. Notably, when using four experts, it requires\\nonly 67% of the MoELoRA training time.\\nTo effectively leverage H-LoRA inHealthGPT, we fur-\\nther introduce a Hierarchical Visual Perception (HVP)\\nand devise a correspondingThree-stage Learning Strategy\\n(TLS). HVP: we separate visual details learning from Vi-\\nsion transformer (ViT) for comprehension and generation.\\nAs is widely recognized, the ViT encodes visual concepts\\nwith increasing abstraction, generally, becoming finer as we\\nprogress over levels (Vig 2019). Thus, we maintain the vi-\\nsual features of the anterior and posterior layers to accom-\\nmodate the differing requirements for visual granularity in\\ncomprehension and generation tasks while preventing po-\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_2', embedding=None, metadata={'page_label': '3', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tential task interference. TLS: In the first and second stages,\\ngiven the heterogeneity between comprehension and gener-\\nation tasks, we first train H-LoRA plugins for HealthGPT\\nto incorporate both medical comprehension and generation\\nknowledge, thus endowing the LLMs with capabilities for\\nvision-language alignment and vision-to-vision reconstruc-\\ntion. Additionally, through minimal mixed-task training, we\\nbuilt fusion embedding layers and output heads that merge\\ntext and visual tokens, establishing a unified LVLM founda-\\ntion for visual instruction fine-tuning. In the third stage, by\\nonly training the H-LoRA plugins, HealthGPT is able to\\nrapidly adapt to a wide range of downstream medical tasks,\\ncovering various types of medical comprehension and gen-\\neration tasks.\\nTo effectively implement our approach, we have cu-\\nrated a dataset for training unified medical LVLMs, called\\nVL-Health, including seven comprehension tasks and five\\ngeneration tasks (Figure 1). Through quantitative analysis\\nand validation on multi-modal tasks, the results demonstrate\\nthat HealthGPT is capable of unifying medical multi-\\nmodal abilities in data-constrained scenarios, achieving per-\\nformance comparable to or better than existing state-of-the-\\nart (SOTA) models across multiple metrics. Overall, the\\nmain contributions of this paper are summarized as follows:\\n• Unified Med-LVLM. We introduce HealthGPT,\\nwhich, to the best of our knowledge, is the first unified\\nframework for multi-modal comprehension and genera-\\ntion in complex medical scenarios.\\n• Effective Learning Paradigm. We present H-LoRA, an\\noptimized multi-LoRA PEFT architecture based on task-\\ngated decoupling, is designed to effectively mitigate data\\nconflict issues.\\n• Holistic Training Dataset. We curated VL-Health, a\\ncomprehensive dataset designed for both comprehension\\nand generation tasks.\\n• Superior Downstream Improvements : Extensive ex-\\nperiments are conducted and the results confirm\\nHealthGPT’s effectiveness in medical vision-language\\ncomprehension and generation.\\n2 Related Work\\nMedical Vision Large Language Models. Recently, medi-\\ncal vision large language models (Med-VLLMs) have made\\nsignificant progress, demonstrating excellent performance\\nin understanding medical images and responding to human\\nqueries based on these images (Zhou et al. 2023; Tian et al.\\n2023). XrayGPT (Thawkar et al. 2023) combines a med-\\nical visual encoder (MedClip) (Wang et al. 2022) with a\\nfine-tuned LLM , using a simple linear transformation layer\\nto achieve alignment between visual and textual informa-\\ntion, significantly enhancing the understanding of medical\\nimages. On this basis, LLaV A-Med (Li et al. 2024b) fur-\\nther enhances visual-text alignment in medical contexts by\\nselecting high-quality image-text pairs from PubMed pa-\\npers and synthesized VQA datasets. BiomedGPT (Luo et al.\\n2024b) employs a BERT-style encoder and GPT-style de-\\ncoder architecture, pre-trained on interdisciplinary datasets.\\nCompared to commercial models like Med-PaLM (Singhal\\net al. 2023), BiomedGPT significantly reduces model size\\nwhile maintaining superior performance. However, issues\\nof language adaptability and dataset specificity still remain.\\nTo address these, HuatuoGPT-Vision (Chen et al. 2024a)\\nintroduces the PubMedVision dataset, which contains 1.3\\nmillion high-quality medical samples, significantly improv-\\ning the model’s adaptability across diverse medical applica-\\ntions. However, current Med-VLLMs mainly focus on med-\\nical comprehension and lack the capability for the medical\\nvision-language generation.\\nUnified Visual Comprehension and Generation Mod-\\nels. Recent research has increasingly concentrated on cre-\\nating unified LVLMs that are adept at understanding and\\nproducing content across various visual modalities. NExT-\\nGPT (Wu et al. 2023) achieves perception and generation for\\narbitrary combinations of multi-modal inputs and outputs by\\naligning LLMs. Similarly, SEED (Ge et al. 2023), SEED-\\nX (Ge et al. 2024), and DreamLLM (Dong et al. 2023) em-\\nploy learnable queries and leverage next-token prediction to\\ngenerate visual tokens, providing conditional inputs to exter-\\nnal generation modules. Unlike these methods, which func-\\ntion as external conditioners, Unified-IO (Lu et al. 2022),\\nUnified-IO 2 (Lu et al. 2024), and Chameleon (Team 2024)\\ninternalize multi-modal generation tasks within a unified\\nTransformer architecture by extending multi-modal vocab-\\nularies, enabling direct generation based on next-token pre-\\ndiction. Building on this concept, Lumina-mGPT (Liu et al.\\n2024a) and ANOLE (Chern et al. 2024) further enhance the\\ngeneration capabilities of unified models using high-quality\\ndata, particularly improving the quality and flexibility of im-\\nage generation.\\n3 Preliminaries\\nLarge Vision-Language Models.The input to a LVLM typ-\\nically consists of an image ximg and a discrete text sequence\\nxtxt. The visual encoder Eimg converts the input image ximg\\ninto a sequence of visual tokens V = [ vi]Nv\\ni=1, while the\\ntext sequence xtxt is mapped into a sequence of text to-\\nkens T = [ ti]Nt\\ni=1 using an embedding function Etxt. The\\nLLM MLLM(·|θ) models the joint probability of the token\\nsequence U = {V, T }, which is expressed as:\\nPθ(R|U) =\\nNrY\\ni=1\\nPθ(ri|{U, r<i}), (1)\\nwhere R = [ri]Nr\\ni=1 is the text response sequence. The LVLM\\niteratively generates the next token ri based on r<i. The op-\\ntimization objective is to minimize the cross-entropy loss of\\nthe response R. It is worth noting that most LVLMs adopt\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_3', embedding=None, metadata={'page_label': '4', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 3: The HealthGPT architecture integrates hierarchical visual perception and H-LoRA, employing a task-specific hard\\nrouter to select visual features and H-LoRA plugins, ultimately generating outputs with an autoregressive manner.\\na design paradigm based on ViT, alignment adapters, and\\npre-trained LLMs(Liu et al. 2023, 2024b), enabling quick\\nadaptation to downstream tasks.\\nVQGAN. VQGAN (Esser, Rombach, and Ommer 2021)\\nemploys latent space compression and indexing mechanisms\\nto effectively learn a complete discrete representation of im-\\nages. VQGAN first maps the input imageximg to a latent rep-\\nresentation z = E(x) through a encoder E. Then, the latent\\nrepresentation is quantized using a codebookZ = {zk}K\\nk=1,\\ngenerating a discrete index sequence I = [im]N\\nm=1, where\\nim ∈ Zrepresents the quantized code index:\\nI = Quantize(z|Z) = arg min\\nzk∈Z\\n∥z − zk∥2. (2)\\nIn our approach, the discrete index sequence I serves as\\na supervisory signal for the generation task, enabling the\\nmodel to predict the index sequence ˆI from input conditions\\nsuch as text or other modality signals. Finally, the predicted\\nindex sequence ˆI is upsampled by the VQGAN decoder G,\\ngenerating the high-quality image ˆximg = G(ˆI).\\nLow Rank Adaptation. LoRA(Hu et al. 2021) effectively\\ncaptures the characteristics of downstream tasks by intro-\\nducing low-rank adapters. The core idea is to decompose\\nthe bypass weight matrix ∆W ∈ Rdin×dout\\ninto two low-\\nrank matrices {A ∈ Rdin×r, B ∈ Rr×dout\\n}, where r ≪\\nmin{din, dout}, significantly reducing learnable parameters.\\nThe output with the LoRA adapter for the input x is then\\ngiven by:\\nh = xW0 + αx∆W/r = xW0 + αxAB/r, (3)\\nwhere matrix A is initialized with a Gaussian distribution,\\nwhile the matrixB is initialized as a zero matrix. The scaling\\nfactor α/r controls the impact of ∆W on the model.\\n4 HealthGPT\\n4.1 Unified Autoregressive Generation.\\nHealthGPT (Figure 3) utilizes a discrete token representa-\\ntion that covers both text and visual outputs, unifying visual\\ncomprehension and generation as an autoregressive task. For\\ncomprehension, Mllm receives the input joint sequence U\\nand outputs a series of text token R = [ r1, r2, . . . , rNr ],\\nwhere ri ∈ Vtxt, and Vtxt represents the LLM’s vocabulary:\\nPθ(R | U) =\\nNrY\\ni=1\\nPθ(ri | U, r<i). (4)\\nFor generation, Mllm first receives a special start token\\n⟨START IMG⟩, then generates a series of tokens corre-\\nsponding to the VQGAN indices I = [ i1, i2, . . . , iNi ],\\nwhere ij ∈ Vvq, and Vvq represents the index range of VQ-\\nGAN. Upon completion of generation, the LLM outputs an\\nend token ⟨END IMG⟩:\\nPθ(I | U) =\\nNiY\\nj=1\\nPθ(ij | U, i<j). (5)\\nFinally, the generated index sequence I is fed into the de-\\ncoder G, which reconstructs the target image ˆximg = G(I).\\n4.2 Hierarchical Visual Perception\\nGiven the differences in visual perception between compre-\\nhension and generation tasks—where the former focuses on\\nabstract semantics and the latter emphasizes complete se-\\nmantics—we employ ViT to compress the image into dis-\\ncrete visual tokens at multiple hierarchical levels. Specif-\\nically, the image is converted into a series of features\\n{f1, f2, . . . , fL} as it passes through L ViT blocks.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_4', embedding=None, metadata={'page_label': '5', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='To address the needs of various tasks, the hidden states\\nare divided into two types: (i) Concrete-grained features\\nFCon = {f1, f2, . . . , fk}, k < L, derived from the shal-\\nlower layers of ViT, containing sufficient global features,\\nsuitable for generation tasks; (ii) Abstract-grained features\\nFAbs = {fk+1, fk+2, . . . , fL}, derived from the deeper\\nlayers of ViT, which contain abstract semantic information\\ncloser to the text space, suitable for comprehension tasks.\\nThe task type T (comprehension or generation) deter-\\nmines which set of features is selected as the input for the\\ndownstream large language model:\\nFimg\\nT =\\n(\\nFCon, if T = generation task\\nFAbs, if T = comprehension task (6)\\nWe integrate the image featuresFimg\\nT and text featuresT into\\na joint sequence through simple concatenation, which is then\\nfed into the LLM Mllm for autoregressive generation.\\n4.3 Heterogeneous Knowledge Adaptation\\nWe devise H-LoRA, which stores heterogeneous knowledge\\nfrom comprehension and generation tasks in separate mod-\\nules and dynamically routes to extract task-relevant knowl-\\nedge from these modules. At the task level, for each task type\\nT, we dynamically assign a dedicated H-LoRA submodule\\nθT , which is expressed as:\\nR = MLLM(U|θ, θT ), θ T = {AT , BT , RT\\nouter}. (7)\\nAt the feature level for a single task, H-LoRA integrates the\\nidea of Mixture of Experts (MoE) (Masoudnia and Ebrahim-\\npour 2014) and designs an efficient matrix merging and rout-\\ning weight allocation mechanism, thus avoiding the signif-\\nicant computational delay introduced by matrix splitting in\\nexisting MoELoRA (Luo et al. 2024a). Specifically, we first\\nmerge the low-rank matrices (rank = r) of k LoRA experts\\ninto a unified matrix:\\nAmerged, Bmerged = Concat({Ai}k\\n1 ), Concat({Bi}k\\n1 ), (8)\\nwhere Amerged ∈ Rdin×rk and Bmerged ∈ Rrk×dout\\n. The\\nk-dimension routing layer generates expert weights W ∈\\nRtoken num×k based on the input hidden state x, and these are\\nexpanded to Rtoken num×rk as follows:\\nWexpanded = αkW/r ⊗ 1r, (9)\\nwhere ⊗ denotes the replication operation. The overall out-\\nput of H-LoRA is computed as:\\nOH-LoRA = (xAmerged ⊙ Wexpanded)Bmerged, (10)\\nwhere ⊙ represents element-wise multiplication. Finally, the\\noutput of H-LoRA is added to the frozen pre-trained weights\\nto produce the final output:\\nO = xW0 + OH-LoRA. (11)\\n900\\n800\\n700\\n600\\n500\\n400\\n300\\n200\\n100\\n0\\nComp. Gen.\\n(a) (b)\\n783K765K\\n（K）\\nFigure 4: Data statistics of VL-Health.\\n4.4 Training Pipeline\\n1st Stage: Multi-modal Alignment. In the first stage, we\\ndesign separate visual adapters and H-LoRA submodules for\\nmedical unified tasks. For the medical comprehension task,\\nwe train abstract-grained visual adapters using high-quality\\nimage-text pairs to align visual embeddings with textual\\nembeddings, thereby enabling the model to accurately de-\\nscribe medical visual content. During this process, the pre-\\ntrained LLM and its corresponding H-LoRA submodules\\nremain frozen. In contrast, the medical generation task re-\\nquires training concrete-grained adapters and H-LoRA sub-\\nmodules while keeping the LLM frozen. Meanwhile, we ex-\\ntend the textual vocabulary to include multimodal tokens,\\nenabling the support of additional VQGAN vector quanti-\\nzation indices. The model trains on image-VQ pairs, en-\\ndowing the pre-trained LLM with the capability for image\\nreconstruction. This design ensures pixel-level consistency\\nof pre- and post-LVLM. The processes establish the initial\\nalignment between the LLM’s outputs and the visual inputs.\\n2nd Stage: Heterogeneous H-LoRA Plugin Adaptation.\\nThe submodules of H-LoRA share the word embedding\\nlayer and output head but may encounter issues such as\\nbias and scale inconsistencies during training across dif-\\nferent tasks. To ensure that the multiple H-LoRA plugins\\nseamlessly interface with the LLMs and form a unified base,\\nwe fine-tune the word embedding layer and output head us-\\ning a small amount of mixed data to maintain consistency\\nin the model weights. Specifically, during this stage, all H-\\nLoRA submodules for different tasks are kept frozen, with\\nonly the word embedding layer and output head being op-\\ntimized. Through this stage, the model accumulates foun-\\ndational knowledge for unified tasks by adapting H-LoRA\\nplugins.\\n3rd Stage: Visual Instruction Fine-Tuning. In the third\\nstage, we introduce additional task-specific data to fur-\\nther optimize the model and enhance its adaptability to\\ndownstream tasks such as medical visual comprehension\\n(e.g., medical QA, medical dialogues, and report generation)\\nor generation tasks (e.g., super-resolution, denoising, and\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_5', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 1: Comparison of HealthGPT with other LVLMs and unified multi-modal models on medical visual comprehension\\ntasks. Bold and underlined text indicates the best performance and second-best performance, respectively.\\nVQA-RAD↑ SLAKE↑ PathVQA↑Type Model # Params Medical\\nLVLM close all close all close all\\nMMMU\\n-Med ↑ OMVQA↑ Avg.↑\\nComp. Only\\nMed-Flamingo 8.3B ✓ 58.6 43.0 47.0 25.5 61.9 31.3 28.7 34.9 41.4\\nLLaV A-Med 7B ✓ 60.2 48.1 58.4 44.8 62.3 35.7 30.0 41.3 47.6\\nHuatuoGPT-Vision 7B ✓ 66.9 53.0 59.8 49.1 52.9 32.0 42.0 50.0 50.7\\nBLIP-2 6.7B ✗ 43.4 36.8 41.6 35.3 48.5 28.8 27.3 26.9 36.1\\nLLaV A-v1.5 7B ✗ 51.8 42.8 37.1 37.7 53.5 31.4 32.7 44.7 41.5\\nInstructBLIP 7B ✗ 61.0 44.8 66.8 43.3 56.0 32.3 25.3 29.0 44.8\\nYi-VL 6B ✗ 52.6 42.1 52.4 38.4 54.9 30.9 38.0 50.2 44.9\\nInternVL2 8B ✗ 64.9 49.0 66.6 50.1 60.0 31.9 43.3 54.5 52.5\\nLlama-3.2 11B ✗ 68.9 45.5 72.4 52.1 62.8 33.6 39.3 63.2 54.7\\nComp. & Gen.\\nShow-o 1.3B ✗ 50.6 33.9 31.5 17.9 52.9 28.2 22.7 45.7 42.6\\nUnified-IO 2 7B ✗ 46.2 32.6 35.9 21.9 52.5 27.0 25.3 33.0 33.8\\nJanus 1.3B ✗ 70.9 52.8 34.7 26.9 51.9 27.9 30.0 26.8 33.5\\nHealthGPT-M3 3.8B ✓ 73.7 55.9 74.6 56.4 78.7 39.7 43.3 68.5 61.3\\nHealthGPT-L14 14B ✓ 77.7 58.3 76.4 64.5 85.9 44.4 49.2 74.4 66.4\\nTable 2: The experimental results for the four modality conversion tasks.\\nCT to MRI (Brain) CT to MRI (Pelvis) MRI to CT (Brain) MRI to CT (Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\npix2pix 71.09 32.65 36.85 59.17 31.02 51.91 78.79 33.85 28.33 72.31 32.98 36.19\\nCycleGAN 54.76 32.23 40.56 54.54 30.77 55.00 63.75 31.02 52.78 50.54 29.89 67.78\\nBBDM 71.69 32.91 34.44 57.37 31.37 48.06 86.40 34.12 26.61 79.26 33.15 33.60\\nVmanba 69.54 32.67 36.42 63.01 31.47 46.99 79.63 34.12 26.49 77.45 33.53 31.85\\nDiffMa 71.47 32.74 35.77 62.56 31.43 47.38 79.00 34.13 26.45 78.53 33.68 30.51\\nHealthGPT-M3 79.38 33.03 33.48 71.81 31.83 43.45 85.06 34.40 25.49 84.23 34.29 27.99\\nHealthGPT-L14 79.73 33.10 32.96 71.92 31.87 43.09 85.31 34.29 26.20 84.96 34.14 28.13\\nmodality conversion). Notably, by this stage, the word em-\\nbedding layer and output head have been fine-tuned, only the\\nH-LoRA modules and adapter modules need to be trained.\\nThis strategy significantly improves the model’s adaptability\\nand flexibility across different tasks.\\n5 Experiments\\n5.1 Data and Experimental Setup\\nData Details. We curate VL-Health dataset (see Fig-\\nure 4). For medical visual comprehension, we leverage\\nmultiple medical-specific datasets, including PubMedVi-\\nsion (Chen et al. 2024a), LLaV A-Med (Li et al. 2024b),\\nPathVQA (He et al. 2020), MIMIC-CXR-VQA (Bae et al.\\n2024), SLAKE (Liu et al. 2021), and VQA-RAD (Lau\\net al. 2018). Additionally, we incorporate high-quality open-\\nworld data from LLaV A-1.5 (Liu et al. 2024b) to preserve\\nthe model’s general knowledge and instruction-following\\ncapabilities. For generation tasks, we construct a recon-\\nstruction dataset based on LLaV A-558k (Liu et al. 2024b),\\nand also explore two key tasks in personalized medical\\nimage enhancement—super-resolution and modality con-\\nversion—using the IXI (Davies et al. 2014) and Syn-\\nthRAD2023 (Thummerer et al. 2023) datasets. Detailed data\\nselection and instruction templates are in the Appendix.\\nModel Details. We select CLIP-L/14 (Radford et al. 2021)\\nas the visual encoder and used the hidden states of its\\nsecond and penultimate layers as concrete-grained and\\nabstract-grained features for model’s dynamic hierarchical\\nvisual perception. Drawing on the successful experiences of\\nLLaV A, we employ a MLP to align the multi-modal fea-\\nture embeddings. We choose the parameter-efficient phi-3-\\nmini (Abdin et al. 2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al. 2023), LLaV A-Med (Li et al.\\n2024b), HuatuoGPT-Vision (Chen et al. 2024a)) as well\\nas recent open-world LVLMs (e.g., BLIP-2 (Li et al.\\n2023b), LLaV A-v1.5 (Liu et al. 2024b), InstructBLIP (Dai\\net al. 2023), Yi-VL (Young et al. 2024), InternVL2 (Chen\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_6', embedding=None, metadata={'page_label': '7', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 3: Comparison results of super-resolution task.\\nModel SSIM↑ PSNR↑ MSE↓ LPIPS↓\\nSRGAN 71.34 32.01 41.27 24.50\\nDASR 71.57 32.34 38.25 19.17\\nReal-ESRGAN 67.30 31.87 42.57 20.64\\nLIIF 73.27 32.13 40.14 22.93\\nBSRGAN 69.97 31.97 41.52 28.72\\nHealthGPT-M3 78.19 32.76 34.47 12.02\\nHealthGPT-L14 77.94 32.71 35.19 12.43\\nFigure 5: Performance comparison of LoRA, MoELoRA,\\nand H-LoRA under different rank settings.\\net al. 2024b), Llama-3.2 (Dubey et al. 2024)). Addition-\\nally, we test several SOTA unified visual comprehension\\nand generation models, including Show-o (Xie et al. 2024),\\nUnified-IO 2 (Lu et al. 2024), and Janus (Wu et al.\\n2024). The experimental results are shown in Table 1, with\\nthe following key observations: (i) SOTA Results Com-\\npared with LVLMs: In medical visual comprehension\\ntasks, HealthGPT demonstrates superior performance,\\nsignificantly outperforming both medical-specific models\\n(e.g., HuatuoGPT-Vision) and general-purpose models (e.g.,\\nLlama-3.2). (ii) Surpassing Current Unified LVLMs: De-\\nspite being trained on billions of data points, unified mod-\\nels still exhibit poor generalization performance in medi-\\ncal visual comprehension. For instance, Unified-IO 2 scored\\nonly 33.8. In contrast,HealthGPT-M3, with only 3.8B pa-\\nrameters, scored 61.3 on the medical multi-modal unified\\ntask, significantly outperforming existing unified models in\\nmedical downstream scenarios. (iii) Stable Improvement\\nwith Large Base Model: Our method demonstrates excel-\\nlent scalability, with HealthGPT-L14 achieving a score\\nof 66.4 in the larger model configuration. This result signif-\\nicantly outperforms all other models, highlighting the effec-\\ntiveness of scaling up the base model for enhanced perfor-\\nmance in medical tasks.\\nGeneration. We study three key tasks in medical imag-\\ning. (i) Modality Conversion: In this task, we focus on\\nthe conversion between CT and MRI modalities for the\\nbrain and pelvic regions, designing four specific sub-tasks.\\nAll comparative models (Pix2Pix (Isola et al. 2017), Cy-\\ncleGAN (Zhu et al. 2017), BBDM (Li et al. 2023a),\\n(%)\\n(%)\\n（a） （b）\\nFigure 6: The loss visualization (a) and performance com-\\nparison (b) with respect to different visual perceptions.\\nVmamba (Liu et al. 2024e), and DiffMa (Wang et al.\\n2024b)) trained a separate model for each sub-task, while\\nHealthGPT unify all tasks into a single training process.\\nThe experimental results, shown in Table 11, demonstrate\\nthat our approach outperforms other methods across multi-\\nple evaluation metrics. For instance, in the CT2MRI-Brain\\ntask, HealthGPT-M3 achieves an SSIM of 79.38, signif-\\nicantly surpassing traditional methods like Pix2Pix (71.09)\\nand the recent DiffMa (71.47). (ii) Super-Resolution: We\\nconduct 4× super-resolution experiments on the IXI dataset,\\nwith the results presented in Table 3. Notably, most exist-\\ning methods fail to fully leverage the prior knowledge of key\\nstructures in medical images, resulting in significant short-\\ncomings in detail recovery. In contrast, our method signif-\\nicantly mitigates this issue. Specifically, HealthGPT-M3\\nexcels in key metrics such as SSIM, PSNR, and ISE, achiev-\\ning scores of 78.19, 32.76, and 34.47, respectively. Ad-\\nditionally, HealthGPT-M3 achieves the lowest score of\\n12.34, further validating its exceptional performance in hu-\\nman visual perception. (iii) Reconstruction: We compare\\nHealthGPT-M3 with unified models with reconstruction\\ncapabilities, such as Unified-IO 2 and SEED-X. The results\\nshow that our approach performs better controllability for vi-\\nsual reconstruction. We also trainHealthGPT-L14 with a\\nsimilar number of trainable parameters to the M3 version.\\nHence, the similar performance between the two models\\nmeets our expectations. Details are in the Appendix.\\n5.3 In-Depth Study\\nEffect of Heterogeneous Low-Rank Adaptation.H-LoRA\\nprovides an optimized multi-LoRA architecture for multi-\\ntask learning. We conduct extensive validation of this struc-\\nture, with results presented in Table 4, comparing the per-\\nformance of LoRA, MoELoRA, and H-LoRA in medical\\nunified comprehension and generation tasks. In the majority\\nof comprehension tasks and all generation tasks, H-LoRA\\ndemonstrates superior performance, particularly in the Om-\\nniMedVQA benchmark, where it improved from 64.90 to\\n68.50. Notably, despite some applications of MoELoRA in\\ncertain scenarios, it do not show advantages in this task and\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_7', embedding=None, metadata={'page_label': '8', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 4: We present the performance and speed differences of LoRA, MoELoRA (n=4), and H-LoRA (n=4) on medical visual\\ncomprehension and generation tasks.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQAModel\\nclose all close all close all\\nMMMU\\n-Med OMVQA RECOM MTRANS SR\\nTraining\\nTime\\nHealthGPT w/\\n+LoRA 71.3 57.2 70.0 53.4 76.4 38.6 41.30 65.10 62.67 59.99 65.88 1.00×\\n+MoELoRA 72.5 57.2 66.4 52.4 73.2 36.0 39.30 64.90 67.31 59.76 65.91 1.49×\\n+H-LoRA 73.7 55.9 74.6 56.4 78.7 39.7 43.30 68.50 67.69 60.30 66.14 1.00×\\nTable 5: Comparison between the H-LoRA-based Three-Stage Learning Strategy and the mixed-training approach.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQA CT MRITraining Strategy\\nclose all close all close all\\nMMMU\\n-Med OMVQA Brain Pelvis Brain Pelvis\\nMixed-Training 56.6 37.9 45.0 32.9 65.7 33.6 44.0 48.9 65.64 62.75 56.61 50.77HealthGPT w/ 3-stage-Training 72.5 55.2 77.9 59.6 79.7 49.0 42.7 68.5 70.84 72.99 65.26 61.33\\nFigure 7: Case study of report-to-CXR under different instructions. (a) shows a normal CXR image for comparison. (b) and (c)\\nillustrate generated cases with varying severity and affected regions. The graffiti areas indicate abnormal conditions.\\nhad a training time approximately 50% longer than LoRA.\\nFigure 5 illustrates the performance of the three PEFT meth-\\nods in medical visual comprehension and generation tasks\\nacross different ranks, with H-LoRA consistently outper-\\nforming the other methods in all scenarios, demonstrating\\nsignificant advantages in handling diverse tasks.\\nDifferent Learning Strategy. We propose a three-stage\\nlearning strategy for H-LoRA that decouples comprehension\\nand generation tasks. Unlike methods that train both tasks\\nsimultaneously, our approach reduces performance degra-\\ndation from task conflicts (see Table 5). In the medical vi-\\nsual comprehension task, mixed training causes catastrophic\\nforgetting and degrades visual reconstruction, whereas our\\nstrategy effectively uses the medical embedding knowledge\\nin pre-trained LLMs to mitigate these conflicts. Meanwhile,\\nwe examine how fusing heterogeneous H-LoRA plugins in\\nthe second training stage results in minimal performance\\ndegradation. Detailed results are in the Appendix.\\nHierarchical Visual Perception Analysis. We conduct an\\nablation analysis on visual perceptual inputs for comprehen-\\nsion and generation tasks. Figure 6 shows that comprehen-\\nsion tasks converge more efficiently with abstract-grained\\ninputs, while generation tasks perform better with concrete-\\ngrained inputs. This highlights the importance of the hier-\\narchical visual perception we propose, suggesting that tai-\\nloring visual inputs for specific tasks at different hierarchies\\ncan significantly improve efficiency.\\nReport-to-CXR Task. We further explore the medical im-\\nage generation task without reference images, using a small\\namount of MIMIC-CXR data (Johnson et al. 2019) for in-\\nstruction fine-tuning. Figure 7 annotates images with vary-\\ning injury degrees and locations, comparing them to healthy\\nCXR images. We observe thatHealthGPT effectively gen-\\nerates CXR images based on the instructions, showcasing its\\npotential in healthcare education and auxiliary diagnosis.\\n6 Conclusion\\nIn this paper, we introduceHealthGPT, a Med-LVLM that\\nunifies medical vision-language comprehension and gen-\\neration through a novel heterogeneous knowledge adap-\\ntation approach. Experimental results demonstrate that\\nHealthGPT achieves significant performance improve-\\nments across multiple medical comprehension and genera-\\ntion tasks, showcasing its potential for healthcare applica-\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_8', embedding=None, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='tions.\\nReferences\\nAbdin, M.; Aneja, J.; Behl, H.; Bubeck, S.; Eldan, R.; Gu-\\nnasekar, S.; Harrison, M.; Hewett, R. J.; Javaheripi, M.;\\nKauffmann, P.; et al. 2024. Phi-4 technical report. arXiv\\npreprint arXiv:2412.08905.\\nBae, S.; Kyung, D.; Ryu, J.; Cho, E.; Lee, G.; Kweon, S.;\\nOh, J.; JI, L.; Chang, E.; Kim, T.; et al. 2024. MIMIC-Ext-\\nMIMIC-CXR-VQA: A Complex, Diverse, And Large-Scale\\nVisual Question Answering Dataset for Chest X-ray Images.\\nChen, J.; Gui, C.; Ouyang, R.; Gao, A.; Chen, S.; Chen,\\nG. H.; Wang, X.; Zhang, R.; Cai, Z.; Ji, K.; et al.\\n2024a. Huatuogpt-vision, towards injecting medical visual\\nknowledge into multimodal llms at scale. arXiv preprint\\narXiv:2406.19280.\\nChen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong,\\nW.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024b. How far are we to\\ngpt-4v? closing the gap to commercial multimodal models\\nwith open-source suites. arXiv preprint arXiv:2404.16821.\\nChern, E.; Su, J.; Ma, Y .; and Liu, P. 2024. Anole:\\nAn open, autoregressive, native large multimodal mod-\\nels for interleaved image-text generation. arXiv preprint\\narXiv:2407.06135.\\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\\nGeneral-purpose Vision-Language Models with Instruction\\nTuning. arXiv:2305.06500.\\nDavies, R. L.; Royston, P. A.; Leung, M. S.; Haider, M. E.\\nA. M. J.; Barkhof, S. G. A. L.; and B., P. E. T. M. 2014. The\\nIXI Dataset. Accessed: 2025-01-30.\\nDing, N.; Qin, Y .; Yang, G.; Wei, F.; Yang, Z.; Su, Y .;\\nHu, S.; Chen, Y .; Chan, C.-M.; Chen, W.; et al. 2023.\\nParameter-efficient fine-tuning of large-scale pre-trained\\nlanguage models. Nature Machine Intelligence, 5(3): 220–\\n235.\\nDong, R.; Han, C.; Peng, Y .; Qi, Z.; Ge, Z.; Yang, J.; Zhao,\\nL.; Sun, J.; Zhou, H.; Wei, H.; et al. 2023. Dreamllm:\\nSynergistic multimodal comprehension and creation. arXiv\\npreprint arXiv:2309.11499.\\nDubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.;\\nLetman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;\\net al. 2024. The llama 3 herd of models. arXiv preprint\\narXiv:2407.21783.\\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming trans-\\nformers for high-resolution image synthesis. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern\\nrecognition, 12873–12883.\\nGe, Y .; Ge, Y .; Zeng, Z.; Wang, X.; and Shan, Y . 2023. Plant-\\ning a seed of vision in large language model. arXiv preprint\\narXiv:2307.08041.\\nGe, Y .; Zhao, S.; Zhu, J.; Ge, Y .; Yi, K.; Song, L.; Li, C.;\\nDing, X.; and Shan, Y . 2024. Seed-x: Multimodal models\\nwith unified multi-granularity comprehension and genera-\\ntion. arXiv preprint arXiv:2404.14396.\\nHe, X.; Zhang, Y .; Mou, L.; Xing, E.; and Xie, P. 2020.\\nPathvqa: 30000+ questions for medical visual question an-\\nswering. arXiv preprint arXiv:2003.10286.\\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\\nof large language models. arXiv preprint arXiv:2106.09685.\\nHu, Y .; Li, T.; Lu, Q.; Shao, W.; He, J.; Qiao, Y .; and Luo,\\nP. 2024. Omnimedvqa: A new large-scale comprehensive\\nevaluation benchmark for medical lvlm. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 22170–22183.\\nIsola, P.; Zhu, J.-Y .; Zhou, T.; and Efros, A. A. 2017. Image-\\nto-image translation with conditional adversarial networks.\\nIn Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 1125–1134.\\nJohnson, A. E.; Pollard, T. J.; Greenbaum, N. R.; Lungren,\\nM. P.; Deng, C.-y.; Peng, Y .; Lu, Z.; Mark, R. G.; Berkowitz,\\nS. J.; and Horng, S. 2019. MIMIC-CXR-JPG, a large pub-\\nlicly available database of labeled chest radiographs. arXiv\\npreprint arXiv:1901.07042.\\nLau, J. J.; Gayen, S.; Ben Abacha, A.; and Demner-\\nFushman, D. 2018. A dataset of clinically generated visual\\nquestions and answers about radiology images. Scientific\\ndata, 5(1): 1–10.\\nLi, B.; Xue, K.; Liu, B.; and Lai, Y .-K. 2023a. Bbdm: Image-\\nto-image translation with brownian bridge diffusion models.\\nIn Proceedings of the IEEE/CVF conference on computer\\nvision and pattern Recognition, 1952–1961.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024a. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024b. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. Blip-2:\\nBootstrapping language-image pre-training with frozen im-\\nage encoders and large language models. In International\\nconference on machine learning, 19730–19742. PMLR.\\nLin, T.; Liu, J.; Zhang, W.; Li, Z.; Dai, Y .; Li, H.; Yu, Z.; He,\\nW.; Li, J.; Jiang, H.; et al. 2024. Teamlora: Boosting low-\\nrank adaptation with expert collaboration and competition.\\narXiv preprint arXiv:2408.09856.\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_9', embedding=None, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Liu, B.; Zhan, L.-M.; Xu, L.; Ma, L.; Yang, Y .; and Wu,\\nX.-M. 2021. Slake: A semantically-labeled knowledge-\\nenhanced dataset for medical visual question answering. In\\n2021 IEEE 18th International Symposium on Biomedical\\nImaging (ISBI), 1650–1654. IEEE.\\nLiu, D.; Zhao, S.; Zhuo, L.; Lin, W.; Qiao, Y .; Li, H.; and\\nGao, P. 2024a. Lumina-mgpt: Illuminate flexible photore-\\nalistic text-to-image generation with multimodal generative\\npretraining. arXiv preprint arXiv:2408.02657.\\nLiu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2024b. Improved\\nbaselines with visual instruction tuning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 26296–26306.\\nLiu, H.; Li, C.; Li, Y .; Li, B.; Zhang, Y .; Shen, S.; and Lee,\\nY . J. 2024c. LLaV A-NeXT: Improved reasoning, OCR, and\\nworld knowledge. https://llava-vl.github.io/blog/2024-01-\\n30-llava-next/.\\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruc-\\ntion Tuning. In NeurIPS.\\nLiu, Q.; Wu, X.; Zhao, X.; Zhu, Y .; Xu, D.; Tian, F.; and\\nZheng, Y . 2024d. When moe meets llms: Parameter efficient\\nfine-tuning for multi-task medical applications. In Proceed-\\nings of the 47th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval, 1104–\\n1114.\\nLiu, Y .; Tian, Y .; Zhao, Y .; Yu, H.; Xie, L.; Wang, Y .; Ye,\\nQ.; and Liu, Y . 2024e. VMamba: Visual State Space Model.\\narXiv preprint arXiv:2401.10166.\\nLu, J.; Clark, C.; Lee, S.; Zhang, Z.; Khosla, S.; Marten, R.;\\nHoiem, D.; and Kembhavi, A. 2024. Unified-IO 2: Scaling\\nAutoregressive Multimodal Models with Vision Language\\nAudio and Action. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , 26439–\\n26455.\\nLu, J.; Clark, C.; Zellers, R.; Mottaghi, R.; and Kembhavi,\\nA. 2022. Unified-io: A unified model for vision, language,\\nand multi-modal tasks. In The Eleventh International Con-\\nference on Learning Representations.\\nLuo, T.; Lei, J.; Lei, F.; Liu, W.; He, S.; Zhao, J.; and Liu,\\nK. 2024a. Moelora: Contrastive learning guided mixture of\\nexperts on parameter-efficient fine-tuning for large language\\nmodels. arXiv preprint arXiv:2402.12851.\\nLuo, Y .; Zhang, J.; Fan, S.; Yang, K.; Hong, M.; Wu, Y .;\\nQiao, M.; and Nie, Z. 2024b. Biomedgpt: An open multi-\\nmodal large language model for biomedicine. IEEE Journal\\nof Biomedical and Health Informatics.\\nMasoudnia, S.; and Ebrahimpour, R. 2014. Mixture of ex-\\nperts: a literature survey. Artificial Intelligence Review, 42:\\n275–293.\\nMoor, M.; Huang, Q.; Wu, S.; Yasunaga, M.; Dalmia, Y .;\\nLeskovec, J.; Zakka, C.; Reis, E. P.; and Rajpurkar, P. 2023.\\nMed-flamingo: a multimodal medical few-shot learner. In\\nMachine Learning for Health (ML4H), 353–367. PMLR.\\nNath, V .; Li, W.; Yang, D.; Myronenko, A.; Zheng, M.; Lu,\\nY .; Liu, Z.; Yin, H.; Law, Y . M.; Tang, Y .; et al. 2024. Vila-\\nm3: Enhancing vision-language models with medical expert\\nknowledge. arXiv preprint arXiv:2411.12915.\\nOpenAI. 2023. GPT-4V(ision) System Card. https://cdn.\\nopenai.com/papers/GPTV System Card.pdf.\\nPan, K.; Tang, S.; Li, J.; Fan, Z.; Chow, W.; Yan, S.;\\nChua, T.-S.; Zhuang, Y .; and Zhang, H. 2024. Auto-\\nEncoding Morph-Tokens for Multimodal LLM. arXiv\\npreprint arXiv:2405.01926.\\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\\net al. 2021. Learning transferable visual models from nat-\\nural language supervision. In International conference on\\nmachine learning, 8748–8763. PMLR.\\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\\net al. 2023. Large language models encode clinical knowl-\\nedge. Nature, 620(7972): 172–180.\\nTeam, C. 2024. Chameleon: Mixed-modal early-fusion\\nfoundation models. arXiv preprint arXiv:2405.09818.\\nThawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal,\\nH.; Anwer, R. M.; Khan, S.; Laaksonen, J.; and Khan,\\nF. S. 2023. Xraygpt: Chest radiographs summarization\\nusing medical vision-language models. arXiv preprint\\narXiv:2306.07971.\\nThummerer, A.; van der Bijl, E.; Galapon Jr, A.; Verhoeff,\\nJ. J.; Langendijk, J. A.; Both, S.; van den Berg, C. N. A.;\\nand Maspero, M. 2023. SynthRAD2023 Grand Challenge\\ndataset: Generating synthetic CT for radiotherapy. Medical\\nphysics, 50(7): 4664–4674.\\nTian, D.; Jiang, S.; Zhang, L.; Lu, X.; and Xu, Y . 2023. The\\nrole of large language models in medical image processing:\\na narrative review. Quantitative Imaging in Medicine and\\nSurgery, 14(1): 1108.\\nTong, S.; Fan, D.; Zhu, J.; Xiong, Y .; Chen, X.; Sinha, K.;\\nRabbat, M.; LeCun, Y .; Xie, S.; and Liu, Z. 2024. Meta-\\nMorph: Multimodal Understanding and Generation via In-\\nstruction Tuning. arXiv preprint arXiv:2412.14164.\\nTu, T.; Azizi, S.; Driess, D.; Schaekermann, M.; Amin, M.;\\nChang, P.-C.; Carroll, A.; Lau, C.; Tanno, R.; Ktena, I.; et al.\\n2024. Towards generalist biomedical AI. NEJM AI, 1(3):\\nAIoa2300138.\\nVig, J. 2019. A multiscale visualization of attention in the\\ntransformer model. arXiv preprint arXiv:1906.05714.\\nWang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y .; Wang, J.;\\nZhang, F.; Wang, Y .; Li, Z.; Yu, Q.; et al. 2024a. Emu3:\\nNext-token prediction is all you need. arXiv preprint\\narXiv:2409.18869.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_10', embedding=None, metadata={'page_label': '11', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Wang, Z.; Wu, Z.; Agarwal, D.; and Sun, J. 2022. Medclip:\\nContrastive learning from unpaired medical images and text.\\narXiv preprint arXiv:2210.10163.\\nWang, Z.; Zhang, L.; Wang, L.; and Zhang, Z. 2024b. Soft\\nMasked Mamba Diffusion Model for CT to MRI Conver-\\nsion. arXiv preprint arXiv:2406.15910.\\nWu, C.; Chen, X.; Wu, Z.; Ma, Y .; Liu, X.; Pan, Z.; Liu, W.;\\nXie, Z.; Yu, X.; Ruan, C.; and Luo, P. 2024. Janus: Decou-\\npling Visual Encoding for Unified Multimodal Understand-\\ning and Generation. arXiv:2410.13848.\\nWu, S.; Fei, H.; Qu, L.; Ji, W.; and Chua, T.-S. 2023.\\nNext-gpt: Any-to-any multimodal llm. arXiv preprint\\narXiv:2309.05519.\\nXie, J.; Mao, W.; Bai, Z.; Zhang, D. J.; Wang, W.; Lin, K. Q.;\\nGu, Y .; Chen, Z.; Yang, Z.; and Shou, M. Z. 2024. Show-o:\\nOne single transformer to unify multimodal understanding\\nand generation. arXiv preprint arXiv:2408.12528.\\nYoung, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.; Zhang,\\nG.; Li, H.; Zhu, J.; Chen, J.; Chang, J.; et al. 2024.\\nYi: Open foundation models by 01. ai. arXiv preprint\\narXiv:2403.04652.\\nZhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li,\\nY .; Chen, S. S.; Zhou, P.; Liu, J.; et al. 2023. A survey of\\nlarge language models in medicine: Progress, application,\\nand challenge. arXiv preprint arXiv:2311.05112.\\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\\npaired image-to-image translation using cycle-consistent ad-\\nversarial networks. InProceedings of the IEEE international\\nconference on computer vision, 2223–2232.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_11', embedding=None, metadata={'page_label': '12', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Appendix\\nThis is the Appendix for “HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation\\nvia Heterogeneous Knowledge Adaptation”. This Appendix is organized as follows:\\n• Section A presents the experimental implementation details, the training process of HealthGPT, and the specifics of\\nVL-Health.\\n• Section B systematically provides an analysis of Heterogeneous Low-Rank Adaptation.\\n• Section C shows supplementary experimental results to validate the effectiveness ofHealthGPT.\\nA Implementation Details\\nA.1 Model Details\\nWe employ CLIP-L/14 (Radford et al. 2021) as the visual feature extractor, extracting both shallow and deep features to serve as\\nvisual tokens. The model uses alignment adapters, implemented with two-layer MLPs, to align shallow features, representing\\nconcrete visual granularity, and deep features, representing abstract visual granularity. These visual tokens are concatenated\\nwith text tokens and input into the large language models (LLMs).\\nHealthGPT offers two versions: HealthGPT-M3 and HealthGPT-L14, which are based on Phi-3-mini (Abdin et al.\\n2024) and Phi-4 (Abdin et al. 2024) as the pre-trained LLMs, respectively. In addition, we expand the LLM vocabulary with\\n8192 VQ indices derived from VQGAN-f8-8192 (Esser, Rombach, and Ommer 2021), serving as multi-modal tokens to further\\naugment the model’s capacity for understanding both visual and textual input. Figure 6 shows the details.\\nTable 6: Overview of the Components of HealthGPT.\\nModel ViT Adapter MLP-dims Model dims LLM Params Vocab Size H-LoRA Rank\\nHealthGPT-M3 CLIP-L/14 2-layer MLP 1024 3072 Phi-3-mini 3.8B 40206 16(Comp.), 64(Gen.)\\nHealthGPT-L14 CLIP-L/14 2-layer MLP 1024 5120 Phi-4 14B 108547 8(Comp.), 32(Gen.)\\nA.2 Training Details\\nIn this study, we propose a three-stage learning strategy that is compatible with our innovative heterogeneous low-rank adapta-\\ntion (H-LoRA). We provide a detailed hyperparameter configuration for the model’s three-stage training process. The specific\\nhyperparameter settings used are listed in Table 7. These hyperparameters are crucial for ensuring the model’s learning efficacy\\nand final performance.\\nTable 7: Overview of Hyperparameter Configurations.\\nHealthGPT-M3 HealthGPT-L14\\nStage-1 Stage-2 Stage-3 Stage-1 Stage-2 Stage-3Hyperparameter\\nComp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen.\\nOptimizer AdamW AdamW AdamW AdamW AdamW AdamW\\nAdapter LR 1e-3 2e-5 2e-5 2e-5 1e-3 2e-5 2e-5 2e-5\\nLearning Rate / 2e-4 2e-4 2e-4 / 1e-4 2e-4 2e-4\\nGlobal Batch Size 256 64 32 128 64 256 64 32 128 64\\nWeight Decay 0 0 0 0 0 0\\nDropout Rate 0 0.05 0.05 0.05 0 0.05 0.05 0.05\\nLR Scheduler Warm Up Constant Warm Up Warm Up Constant Warm Up\\nMax Sequence Length 2048 2048 2048 2048 2048 2048\\nIt is worth noting that we sometimes observe instances of loss spikes during the training of medical visual comprehension\\nand generation tasks. Through repeated validation, we discovered that larger model parameters and learning rates tend to lead to\\nthis issue, which is the reason for the slight differences in hyperparameters betweenHealthGPT-M3 and HealthGPT-L14.\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_12', embedding=None, metadata={'page_label': '13', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='（a） （b）\\nFigure 8: VL-Health dataset collection distribution.\\nA.3 VL-Health\\nThe construction of theVL-Health dataset involves two key steps:(i) data collection, (ii) data processing, as detailed below:\\nData Collection: During the collection phase, we carefully considered the diversity of medical images and the complexity of\\nthe tasks, selecting appropriate subsets for comprehension and generation tasks. For comprehension tasks, we selected datasets\\nsuch as VQA-RAD (Lau et al. 2018), SLAKE (Liu et al. 2021), PathVQA (He et al. 2020), and MIMIC-CXR-VQA (Bae et al.\\n2024), which cover various medical imaging modalities like radiology and pathology, and include professional annotations\\nto assist the model in learning tasks such as lesion detection and disease diagnosis. Additionally, large-scale multi-modal\\ndatasets like LLaV A-Med (Li et al. 2024b) and PubMedVision (Chen et al. 2024a) were included to provide broader medical\\nknowledge support and facilitate the training of complex reasoning tasks. For generation tasks, we focused on four mainstream\\ntask categories: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction.\\nThe IXI (Davies et al. 2014) dataset, containing a large number of healthy brain MRI images, is suitable for training super-\\nresolution models; the MIMIC-CHEST-XRAY (Bae et al. 2024) dataset, with X-ray images and their corresponding textual\\nreports, is appropriate for text-to-image generation tasks; the SynthRAD2023 (Thummerer et al. 2023) dataset provides a large\\nnumber of paired CT and MRI images, supporting modality conversion model training; for image reconstruction tasks, we\\nrewrote and adjusted the LLaV A-558k (Liu et al. 2024b) dataset.\\nData Processing: After data collection, we performed filtering and processing of the raw data. For VisualQA tasks, we stan-\\ndardized the data entries into two forms: open-ended questions and single-choice questions, enabling flexible training and\\nevaluation. Additionally, considering that multi-image data has a minimal impact on performance but introduces extra padding\\nand training time, we excluded multi-image data. For the scanned image data in generation tasks, we applied slicing extrac-\\ntion, image registration, data augmentation, and normalization to treat 2D images as visual inputs for model training or used\\nVQGAN-generated indices to supervise the generation tasks.\\nData Statistics This section provides detailed statistical information about the VL-Health dataset to offer a more compre-\\nhensive understanding.\\nData Overview: To ensure a balanced development of the model’s comprehension and generation capabilities, in addition\\nto the LLaV A-558k and PubMedVision-PT datasets used for alignment, the VL-Health dataset ultimately selected 765,802\\nadditional visual question-answering (VQA) training samples (to endow the model with visual comprehension and instruction-\\nfollowing capabilities) and 783,045 generation training samples (to provide the model with reconstruction and visual generation\\ninstruction-following abilities). This contributes to the transfer of knowledge between comprehension and generation tasks, en-\\nhancing the model’s overall performance. For medical image comprehension tasks, images were selected from VQA-RAD (ap-\\nproximately 450 images), SLAKE (approximately 630 images), PathVQA (approximately 2,600 images), MIMIC-CXR-VQA\\n(approximately 52,000 images), LLaV A-Med (approximately 61,000 images), and PubMedVision (approximately 500,000 im-\\nages). Multiple question-answer pairs were retained for each image to enhance the model’s understanding and generalization\\nof the image content. Table 8 shows the data distribution ofVL-Health for three-stage learning strategy, where mixed-47k is\\nbased on the sampling of all data in stage-1.\\nDiversity and Quality Assessment: VL-Health covers 11 modalities, including CT, MRI, X-ray, microscopy, OCT, ultra-\\nsound, and fundus photography, which aids the model in learning features from various modalities. The dataset also encom-\\npasses a wide range of diseases, from common to rare, and from localized lesions to systemic diseases, including pulmonary\\ndiseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides com-\\nprehensive training support to the model, enabling it to learn the characteristics and diagnosis of various diseases.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_13', embedding=None, metadata={'page_label': '14', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Table 8: Data distribution of VL-Health in three-stage learning strategy.\\nMedical Task Stage-1 Stage-2\\nComp. LLaV A-558k, PubMedVision-PT Mixed-47kGen. LLaV A-558k\\nMedical Task Stage-3\\nComp. LLaV A Med, MIMIC CXR VQA, PubMedVision-FT, LLaV A-665k, PathVQA, SLAKE, VQA-RAD\\nGen. IXI, SynthRAD2023, MIMIC-CHEST-XRAY\\nData Format. All data samples are converted into a unified instruction-response format for training and evaluation. Specifi-\\ncally, the VL-Health dataset consists of the following components:\\n• Task Type: Specifies the granularity of visual features output by the visual encoder and selects the corresponding H-\\nLoRA submodule. For generation tasks, the response also includes multi-modal tokens corresponding to VQ indices.\\n• Task Instruction: Guides the model to interpret the image and generate a response, covering various aspects of the\\nimage and specifying the output format.\\n• Response: The textual output generated based on the task instruction and input image, ensuring it meets the question and\\nformatting requirements.\\n• Input Image: Provides the visual signal for the model to process.\\n• Target Image Index: In generation tasks, this is added as a multi-modal token to the response for autoregressive\\ngeneration.\\nB Analysis of Heterogeneous Low-Rank Adaptation\\nWe propose H-LoRA, which utilizes hard routing selection to allocate plugins for knowledge learning and representation across\\ntasks, thereby preventing conflicts arising from heterogeneous knowledge. Furthermore, within each task, we optimized based\\non MoELoRA, enhancing performance while reducing computational overhead. The pseudocode is detailed Algorithm 1.\\nAlgorithm 1: H-LoRA Algorithm\\nInput: concrete-grained visual features FCon, abstract-grained visual featuresFAbs, comprehension-based H-LoRA modules\\n({AComp.\\ni }k\\ni=1, RComp.\\nouter ), generation-based H-LoRA modules({AGen.\\ni }k\\ni=1, RGen.\\nouter), task type T (comprehension or generation),\\nnumber of LoRA experts k, origin linear layer weights W0, text features T , hidden state h\\nOutput: final output O\\n// Select task-specific image features\\nif T = generation task then\\nFimg ← FCon\\nelse if T = comprehension task then\\nFimg ← FAbs\\nend if\\nU ←concat(Fimg, T ) // Concatenate image features and text features\\n{Ai}k\\ni=1, {Bi}k\\ni=1, Router ← {AT\\ni }k\\ni=1, {BT\\ni }k\\ni=1, RT\\nouter // Assign task-specific H-LoRA submodule\\n// Merge LoRA experts’ matrices\\nAmerged ← concat({Ai}k\\ni=1)\\nBmerged ← concat({Bi}k\\ni=1)\\nW ←R(h) // Generate routing weights based on input hidden state x\\nWexpanded ← α × W/r ⊗ 1r // Expand routing weights to match merged matrices\\nOH-LoRA ← (x · Amerged ⊙ Wexpanded) · Bmerged // Compute H-LoRA output using element-wise multiplication\\nO ←x · W0 + OH-LoRA // Add H-LoRA output to pre-trained weights to get final output\\nReturn O\\nWe further analyzed the computational overhead differences between MoELoRA and H-LoRA. Assuming that both methods\\nuse the same number of LoRA experts k, we can compare their time complexity from the perspective of the operational steps\\ninvolved.\\nComputational Overhead of MoELoRA. In MoELoRA, the operations involving the expert matrix mainly include the fol-\\nlowing steps: (i) Expert Multiplication : MoELoRA requires 2k multiplications with the LoRA experts. (ii) Router Multi-\\nplication: One multiplication with the Router is required. (iii) Router Output Expansion : MoELoRA needs to perform k\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_14', embedding=None, metadata={'page_label': '15', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='expansion operations on the Router’s output weights to generate the appropriate shapes that match the dimensions of the in-\\nput and LoRA experts while iterating through the experts. (iv) Dot Product: For each expanded Router weight, a dot product\\nwith the intermediate state of the expert is required, resulting in k multiplications. (v) Addition: Finally, k addition operations\\nare required to accumulate the results from each LoRA expert into the final output. Assuming the time complexity of each\\noperation is the same, the additional time complexity introduced when equipping a fully connected layer with MoELoRA is:\\nO(2k+1+ k+k+k) = O(5k+1). Thus, MoELoRA introduces an additional time overhead ofO(5k+1) during computation.\\nH-LoRA. In contrast to MoELoRA, H-LoRA reduces the computational overhead by concatenating the LoRA expert matrices.\\nSpecifically: (i) Expert Multiplication: H-LoRA merges all LoRA experts by directly creating a larger A and B matrix, instead\\nof performing independent operations for each expert. This process can be implemented through matrix initialization without\\nadditional concatenation operations. Therefore, only 2 multiplications with the LoRA experts are required. (ii) Router Multi-\\nplication: H-LoRA still requires one multiplication with the Router. (iii) Router Output Expansion : H-LoRA only requires\\none expansion operation on the Router’s output weights. (iv) Dot Product: H-LoRA only requires one dot product between\\nthe Router’s output and the expert’s intermediate state. (v) Addition: Finally, H-LoRA only requires one addition operation\\nto accumulate the LoRA expert results into the intermediate state. Therefore, the additional time complexity introduced by\\nH-LoRA is: O(2 + 1 + 1 + 1 + 1) =O(6).\\nComparing the two, we see that MoELoRA introduces a linear increase in additional time complexity with respect to the\\nnumber of experts k, resulting in a complexity of O(5k + 1), while H-LoRA’s additional time complexity is fixed at O(6),\\nindependent of k. We observe that when k is small, the time complexity differences between MoELoRA and H-LoRA are\\nnegligible. However, as k increases, MoELoRA’s computational overhead grows linearly, while H-LoRA’s remains constant.\\nThis makes H-LoRA significantly more computationally efficient than MoELoRA, particularly in large-scale tasks. We will\\nfurther demonstrate the significant advantage of H-LoRA in training time in subsequent experiments, validating its efficiency\\nin practical applications.\\nC Supplemental Experimental Results\\nIn this section, we include additional experiments to demonstrate the superiority of HealthGPT and articulate our design\\nphilosophy.\\nC.1 Results: OmniMedVQA Benchmark\\nOmniMedVQA (Hu et al. 2024) is a novel, large-scale medical visual question answering (VQA) benchmark designed to\\nencompass various modalities and anatomical regions by collecting diverse images from multiple medical datasets. Our exper-\\nimental results are presented in Table 9.\\nTable 9: Performance comparison of OmniMedVQA Benchmark.\\nOmniMedVQA↑Type Model # Params Medical\\nLVLM CT X-ray FDM MiS OCT MRI USS Avg.\\nComp. Only\\nMed-Flamingo 8.3B ✓ 30.1 33.9 25.5 37.0 60.0 27.6 30.4 34.9\\nLLaV A-Med 7B ✓ 28.4 32.8 42.7 31.6 55.3 45.0 53.6 41.3\\nHuatuoGPT-Vision 7B ✓ 35.3 41.5 51.4 62.3 59.3 40.4 60.1 50.0\\nBLIP-2 6.7B ✗ 26.6 29.1 22.3 36.9 29.1 22.7 21.4 26.9\\nLLaV A-v1.5 7B ✗ 28.0 55.7 35.5 42.1 49.2 52.9 49.7 44.7\\nInstructBLIP 7B ✗ 20.1 22.2 34.1 30.6 38.6 31.9 25.5 29.0\\nYi-VL 6B ✗ 51.2 47.1 27.7 62.6 67.6 55.0 40.3 50.2\\nInternVL2 8B ✗ 40.2 57.9 53.2 64.0 59.1 58.1 49.1 54.5\\nLlama-3.2 11B ✗ 37.6 55.2 71.4 82.1 62.5 65.2 68.6 63.2\\nComp. & Gen.\\nShow-o 1.3B ✗ 29.0 50.4 30.9 22.0 30.8 34.2 33.8 33.0\\nUnified-IO 2 7B ✗ 10.8 37.7 12.3 25.3 32.6 30.9 37.7 26.8\\nJanus 1.3B ✗ 24.9 54.8 35.9 62.7 54.2 50.7 36.8 45.7\\nHealthGPT-M3 3.8B ✓ 35.3 81.9 54.6 88.2 89.3 78.5 51.4 68.5\\nHealthGPT-L14 14B ✓ 39.0 86.6 64.1 88.6 99.7 80.9 62.2 74.4\\nThrough our analysis, we make the following observations: (i) HealthGPT-M3 outperforms other models in 4 out of 7\\nsub-tasks, achieving an average score that exceeds cutting-edge medical Large Vision-Language Models (LVLMs) as well as\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_15', embedding=None, metadata={'page_label': '16', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='general LVLMs; (ii) the unified model demonstrates relatively weak performance on OmniMedVQA; however, our approach\\neffectively mitigates performance degradation caused by generation tasks, serving as a unified model; (iii) HealthGPT-L14\\nexcels across all sub-tasks, achieving optimal or near-optimal results with an average score of 74.4, significantly surpassing\\nother models.\\nC.2 Stability Analysis of Number of Experts\\nWe investigated the impact of the number of LoRA experts on model performance within a multi-LoRA architecture, conducting\\nextensive experiments on MoELoRA and H-LoRA with varying numbers of experts. The experimental results are presented\\nin Table 10. As the number of experts increases, the training time for MoELoRA is significantly prolonged. When n = 8 ,\\nthe training time for MoELoRA is twice that of LoRA, whereas H-LoRA incurs no additional training delay and performs\\nbetter. It is estimated that at n = 32, the training time for MoELoRA could reach eight times that of LoRA, preventing it from\\ncompleting training and inference. This result aligns with the analysis in Appendix B, indicating that H-LoRA not only avoids\\nintroducing additional training delays compared to LoRA but also outperforms MoELoRA.\\nTable 10: We explored the performance of MoELoRA and H-LoRA with different numbers of LoRA experts. At n = 32 ,\\nMoELoRA was unable to complete training.\\nn=2 n=4 n=8 n=32Model Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time\\n+MoELoRA 50.3 62.98 1.22 × 50.0 64.33 1.49 × 50.8 63.71 2.09 × / / 5.81 ×HealthGPT w/ +H-LoRA 51.5 63.48 0.99× 52.8 64.71 1.00× 53.6 64.98 0.99× 53.5 64.74 1.01×\\nC.3 Impact of Heterogeneous Knowledge Fusion on Performance\\nTraditional unified models often utilize mixed training methods, which may result in performance degradation due to variations\\nin task modes. To address this, we propose a three-phase learning strategy to support H-LoRA, effectively mitigating inter-task\\nconflicts. Specifically, the second phase (Heterogeneous H-LoRA Plugin Adaptation) integrates LLMs with different H-LoRA\\nplugins into a new unified foundation by mixing the training of the embedding layers and output heads for two tasks. Figure\\n9 illustrates the impact of this phase on the performance of medical comprehension and generation tasks. We observe that the\\nsecond phase effectively unifies the model with minimal impact on overall performance, significantly alleviating the conflict\\nissues arising from mixed training in medical scenarios.\\nC.4 Human Evaluation.\\n65.7 65.4 67.7 67.0\\nFigure 9: Performance changes before and after the\\nstage-2.\\nWe further conduct human evaluation on the VQA-RAD, SLAKE,\\nand PathVQA benchmarks, which contain 1,000 open-ended ques-\\ntions. Specifically, we recruit 5 clinicians to rank the randomly shuf-\\nfled responses from HealthGPT-L14, LLaV A-Med, HuatuoGPT-\\nVision, Llama-3.2, InternVL-2 and Show-o. During the evaluation,\\nquestions were randomly selected, and the model-generated responses\\nwere anonymized and ranked. The results, as shown in Figure 10, in-\\ndicate that HealthGPT was frequently selected as the best answer.\\nThis suggests that HealthGPT has further application potential in\\nmedical care scenarios.\\nC.5 Reconstruction Performance\\nCurrently, unified models that align visual features based on recon-\\nstruction tasks include pre-LVLMs, post-LVLMs, as well as Unified-\\nIO 2 (Lu et al. 2024) and SEED-X (Ge et al. 2024). To investigate the\\ncontrollability of visual generation in rigorous settings such as med-\\nical contexts, we evaluated the performance of these models in med-\\nical image reconstruction in Table 11. Experimental results demon-\\nstrate that HealthGPT exhibits the most stable reconstruction per-\\nformance with a small amount of data.\\n16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_16', embedding=None, metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='C.6 Case Study\\nFigures 11 and 12 illustrate examples of modality transformation and super-resolution reconstruction. In Figure 11, the results\\ngenerated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively\\nguiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure 12\\ndemonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the\\nimage.\\nTable 11: The experimental results for the four reconstruction tasks.\\nCT(Brain) CT(Pelvis) MRI (Brain) MRI(Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\nSEED-X 20.18 27.66 112.11 21.53 28.02 102.87 4.90 27.62 112.86 6.31 27.89 106.21\\nUnified-IO 2 83.93 36.09 17.95 85.36 35.10 25.46 87.50 34.25 25.47 86.31 33.53 29.80\\nHealthGPT-M3 91.73 36.42 15.46 94.26 37.30 12.53 88.76 33.97 27.05 84.40 33.11 32.62\\n34.08\\n15.87\\n9.16\\n5.62\\n13.33\\n21.94\\nHealthGPT\\nLLaVA-Med\\nHuatuoGPT-Vision\\nLlama-3.2\\nInternVL-2\\nShow-o\\nHuman\\nEvaluation\\n(a) (b)\\nFigure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_17', embedding=None, metadata={'page_label': '18', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 11: Case of modality transfer.\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/2502.09838v3.pdf_part_18', embedding=None, metadata={'page_label': '19', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Figure 12: Case of MRI image super-resolution.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_0', embedding=None, metadata={'page_label': '1', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' \\n1     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_1', embedding=None, metadata={'page_label': '2', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only \\nVersion  control     Classification  Level:   Internal   Asset  Owner:   Legal     Asset:     WSO2  LLC  -  Anti-Corruption  Policy     Document  History:  \\nDate  Revision  Author(s)  Description  Reviewed  &  Approved  By  28/07/2022  V1.0  \\nLegal  Team  and  External  Counsel  (Cooley  LLP)   \\nInitial  Version  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n19/10/2023  V1.0  \\nLegal  Team  Reviewed.  No  changes  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n \\n2     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_2', embedding=None, metadata={'page_label': '3', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only  \\nContents  I.   PURPOSE \\nII.   POLICY  STATEMENTS \\nIII.   ANTI-BRIBERY  PROHIBITIONS \\nIV.   ACCOUNTING  REQUIREMENTS \\nV.   FACILITATION  PAYMENTS \\nVI.   INTERMEDIARIES  AND  BUSINESS  PARTNERS \\nVII.   GIFTS  AND  HOSPITALITIES \\nIX.   OTHER  ACTIVITIES \\nX.   VIOLATIONS  AND  CONSEQUENCES \\nXI.   TRAINING  AND  CERTIFICATION \\nXII.   STATUS \\nXIII.   REPORTING/QUESTIONS \\nIX.   ACKNOWLEDGEMENT   \\nATTACHMENT  1:  ANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL \\n●  UNITED  KINGDOM ●  THE  UK  BRIBERY  ACT  2010 ●  SRI  LANKA ●  BRAZIL o  THE  BRAZILIAN  ANTI  CORRUPTION  ACT  2013 o  THE  BRAZILIAN  IMPROBITY  ACT  1992 o  OTHER  POTENTIAL  LIABILITIES  \\n \\n3     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_3', embedding=None, metadata={'page_label': '4', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only WSO2\\n \\nLLC   A\\nNTI\\n-C\\nORRUPTION\\n \\nP\\nOLICY\\n  \\nA\\nPPROVED\\n \\nBY\\n \\nTHE\\n \\nB\\nOARD\\n \\nOF\\n \\nD\\nIRECTORS\\n  JULY  12,  2022   \\n  \\nI.   P\\nURPOSE\\n \\n \\nWSO2  LLC  (together  with  its  worldwide  subsidiaries,  “ WSO2 ”  or  the  “ Company ”)  has  implemented  this  \\nAnti-Corruption\\n \\nPolicy\\n \\n(the\\n \\n“\\nPolicy\\n”)\\n \\nfor\\n \\nthe\\n \\npurpose\\n \\nof\\n \\nensuring\\n \\ncompliance\\n \\nwith\\n \\nthe\\n \\nU.S.\\n \\nForeign\\n \\nCorrupt\\n \\nPractices\\n \\nAct\\n \\nof\\n \\n1977,\\n \\nas\\n \\namended\\n \\n(the\\n \\n“\\nFCPA\\n”),\\n \\nthe\\n \\nU.S.\\n \\nTravel\\n \\nAct,\\n \\nthe\\n \\nU.S.\\n \\nDomestic\\n \\nBribery\\n \\nStatute,\\n \\nthe\\n \\nUK\\n \\nBribery\\n \\nAct\\n \\n2010,\\n \\nthe\\n \\nSri\\n \\nLankan\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments),\\n \\nthe\\n \\nBrazilian\\n \\nAnti-corruption\\n \\nAct\\n \\n(Law\\n \\nNo.\\n \\n12,846/2013),\\n \\nthe\\n \\nBrazilian\\n \\nImprobity\\n \\nAct\\n \\n1992\\n \\n(Law\\n \\nNo.\\n \\n8.429/1992),\\n \\nand\\n  \\nall\\n  \\nother\\n  \\nanti-corruption\\n  \\nlaws\\n  \\nand\\n  \\nregulations\\n  \\napplicable\\n  \\nto\\n  \\nWSO2’s\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(collectively,\\n \\n“\\nAnti-Corruption\\n \\nLaws\\n”).\\n  \\nThis\\n \\nPolicy\\n \\napplies\\n \\nto\\n \\nall\\n \\nworld-wide\\n \\ndirectors,\\n \\nofficers,\\n \\nemployees,\\n \\nand\\n \\nindividuals\\n \\nserving\\n \\nas\\n \\nindependent\\n \\ncontractors\\n \\nof\\n \\nWSO2\\n \\n(collectively,\\n \\n“\\nWSO2\\n \\nPersonnel\\n”)\\n \\nto\\n \\ncomply\\n \\nwith\\n \\nthe\\n \\nprinciples\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy.\\n  \\nPlease\\n \\nreport\\n \\nall\\n \\nquestions\\n \\nor\\n \\nconcerns\\n \\nto\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nwhose\\n \\ncontact\\n \\ninformation\\n \\nappears\\n \\nbelow.\\n \\n \\nII.   P\\nOLICY\\n \\nS\\nTATEMENTS\\n \\n \\nWSO2  Personnel  are  strictly  prohibited  from  promising,  offering,  providing,  or  authorizing  cash  \\npayments\\n \\n(such\\n \\nas\\n \\nbribes\\n \\nor\\n \\nkickbacks)\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nalso\\n \\nstrictly\\n \\nprohibited\\n \\nfrom\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue\\n \\nfrom\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n    \\n \\nWSO2  Personnel  must  comply  with  all  of  the  Company’s  internal  controls,  especially  those  designed  \\nto\\n \\nensure\\n \\naccurate\\n \\nand\\n \\ncomplete\\n \\nbooks\\n \\nand\\n \\nrecords,\\n \\nor\\n \\notherwise\\n \\nprevent\\n \\ncorruption,\\n \\nself-dealing,\\n \\nembezzlement,\\n \\nfraud,\\n \\nmoney\\n \\nlaundering,\\n \\nor\\n \\nother\\n \\nimproper\\n \\nactivities.\\n \\n \\nThere  are  no  exceptions  to  this  Policy,  even  if  our  competitors  engage  in  improper  behavior  or  corruption  \\nis\\n \\nan\\n \\naccepted\\n \\npractice\\n \\nin\\n \\na\\n \\ncountry\\n \\nwhere\\n \\nwe\\n \\noperate.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nrequired\\n \\nto\\n \\nadhere\\n \\nto\\n \\nboth\\n \\nthe\\n \\nspirit\\n \\nand\\n \\nthe\\n \\nletter\\n \\nof\\n \\nthis\\n \\nPolicy\\n \\nwith\\n \\nrespect\\n \\nto\\n \\nour\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld.\\n \\n \\nIII.   A\\nNTI\\n-B\\nRIBERY\\n \\nP\\nROHIBITIONS\\n \\n \\nThe  FCPA  and  other  Anti-Corruption  Laws  prohibit  WSO2  and  WSO2  Personnel  from  corruptly  \\npromising,\\n \\noffering,\\n \\nproviding,\\n \\nor\\n \\nauthorizing\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nof\\n \\nvalue\\n \\ndirectly\\n \\nor\\n \\nindirectly\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nand\\n \\ncertain\\n \\nother\\n \\npersons\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose.\\n \\n“Improper\\n \\npurposes”\\n \\ninclude\\n \\ninfluencing\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\nthe\\n \\nrecipient\\n \\nin\\n \\nhis/her\\n \\nofficial\\n \\ncapacity,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ndo\\n \\nor\\n \\nomit\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nin\\n \\nviolation\\n \\nof\\n \\nhis/her\\n \\nlawful\\n \\nduty,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ninfluence\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\na\\n \\ngovernment\\n \\nor\\n \\ninstrumentality\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nor\\n \\nsecuring\\n \\nany\\n \\nimproper\\n \\nadvantage,\\n \\nin\\n \\norder\\n \\nto\\n \\nobtain,\\n \\nretain,\\n \\nor\\n \\ndirect\\n \\nregulatory\\n \\napprovals,\\n \\ncontracts,\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nbenefits.\\n   \\n \\n4     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_4', embedding=None, metadata={'page_label': '5', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only The  FCPA  prohibits  improper  payments  provided  to  officials  of  governments,  state-affiliated  entities,  and  \\npolitical\\n \\nparties\\n \\noutside\\n \\nthe\\n \\nUnited\\n \\nStates.\\n \\nHowever,\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\ngovernment\\n \\nor\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\nthe\\n \\nUnited\\n \\nStates\\n \\nwill\\n \\nviolate\\n \\nU.S.\\n \\ndomestic\\n \\nbribery\\n \\nstatutes.\\n \\n \\nIn  addition  to  the  United  States,  almost  all  other  countries,  including  the  United  Kingdom,  Brazil,  and  Sri  \\nLanka,\\n \\nhave\\n \\npromulgated\\n \\ntheir\\n \\nown\\n \\nanti-bribery\\n \\nlegislation.\\n \\nMost\\n \\nof\\n \\nthose\\n \\ncountries\\n \\nprohibit\\n \\nmaking\\n \\nimproper\\n \\npayments\\n \\nto\\n \\ngovernment\\n \\nand\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\ntheir\\n \\nborders.\\n \\nHowever,\\n \\nseveral\\n \\ncountries\\n \\nhave\\n \\nalso\\n \\nadopted\\n \\nlegislation\\n \\nsimilar\\n \\nto\\n \\nthe\\n \\nFCPA\\n \\nthat\\n \\nprohibit\\n \\nimproper\\n \\npayments\\n \\noutside\\n \\nthose\\n \\ncountries.\\n  \\nThe\\n \\nexistence\\n \\nof\\n \\nall\\n \\nof\\n \\nthese\\n \\nlaws\\n \\nmeans\\n \\nthat\\n \\nthere\\n \\nis\\n \\npotential\\n \\nfor\\n \\na\\n \\ncompany\\n \\nor\\n \\nan\\n \\nindividual\\n \\nto\\n \\nface\\n \\nliability\\n \\nin\\n \\nseveral\\n \\ncountries\\n \\nfor\\n \\nthe\\n \\nsame\\n \\nsingle\\n \\nact\\n \\nof\\n \\ncorruption.\\n  \\nAttachment\\n \\n1\\n \\ncontains\\n \\nan\\n \\noverview\\n \\nof\\n \\nthe\\n \\nAnti-Corruption\\n \\nLaws\\n \\nof\\n \\nother\\n \\njurisdictions\\n \\nwhich\\n \\nare\\n \\napplicable\\n \\nto\\n \\nWSO2.\\n \\n \\nGiven  the  broad  prohibitions  under  Anti-Corruption  Laws  applicable  to  WSO2,  this  Policy  \\nprohibits\\n \\nbribes,\\n \\nkickbacks,\\n \\nand\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nand\\n \\nadvantages\\n \\nto\\n \\nany\\n \\nperson,\\n \\nentity,\\n \\nor\\n \\norganization,\\n \\nincluding,\\n \\nbut\\n \\nnot\\n \\nlimited\\n \\nto,\\n \\nemployees,\\n \\nofficials,\\n \\nrepresentatives,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\n \\n(i)  government;   \\n(ii)  state-owned  or  affiliated  entity,  including,  but  not  limited  to,  a  state  hospital,  research  \\ninstitution,\\n \\nutility,\\n \\npublic\\n \\nuniversity,\\n \\nor\\n \\nsovereign\\n \\nwealth\\n \\nfund;\\n \\n \\n(iii)  public  international  organization  such  as  the  United  Nations  or  the  World  Bank;    \\n(iv)  political  party,  including  the  party  itself  as  well  as  candidates  for  public  office;    \\n(v)  non-governmental  organization;  or   (vi)   private-sector  company.      \\nOne  may  be  asked  by  certain  parties  to  provide  a  bribe  or  other  improper  benefit  in  exchange  for  the  \\naward\\n \\nof\\n \\na\\n \\ncontract,\\n \\nsponsorship\\n \\nopportunity,\\n \\nor\\n \\nother\\n \\nbusiness;\\n \\nthe\\n \\nissuance\\n \\nor\\n \\nrenewal\\n \\nof\\n \\na\\n \\nconcession,\\n \\nlicense,\\n \\nor\\n \\nbusiness,\\n \\nconstruction,\\n \\nor\\n \\nother\\n \\npermit\\n \\nor\\n \\nregistration;\\n \\nthe\\n \\nsuccessful\\n \\nfiling\\n \\nof\\n \\na\\n \\npatent\\n \\nor\\n \\ntrademark\\n \\napplication;\\n \\nan\\n \\nimpermissible\\n \\nreduction\\n \\nin\\n \\nduties\\n \\nor\\n \\nother\\n \\ntaxes;\\n \\nobtaining\\n \\na\\n \\nfavorable\\n \\ninspection\\n \\nresult\\n \\nor\\n \\ncourt\\n \\ndecision,\\n \\neven\\n \\nif\\n \\nthe\\n \\nfacts\\n \\nor\\n \\ncircumstances\\n \\ndo\\n \\nnot\\n \\nsupport\\n \\nsuch\\n \\na\\n \\nresult;\\n \\nor\\n \\nthe\\n \\ngrant\\n \\nof\\n \\nsome\\n \\nother\\n \\nimproper\\n \\nadvantage.\\n  \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nWSO2\\n \\nPersonnel\\n \\nfrom\\n \\nproviding\\n \\nbribes\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nany\\n \\nof\\n \\nthe\\n \\nabove\\n \\npurposes.\\n \\n \\nA  violation  of  this  Policy  can  occur  even  if  the  bribe  fails  to  achieve  the  purpose  for  which  it  was  \\nintended.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\na\\n \\nperson\\n \\ncan\\n \\nviolate\\n \\nthis\\n \\nPolicy\\n \\nif\\n \\nthat\\n \\nperson\\n \\nprovides\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nbenefit\\n \\nto\\n \\na\\n \\nrecipient\\n \\nand\\n \\nthe\\n \\nrecipient\\n \\ndoes\\n \\nnot\\n \\ngrant\\n \\nany\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nin\\n \\nreturn.\\n   \\nIn\\n \\naddition,\\n \\nthe\\n \\nmere\\n \\noffer\\n \\nor\\n \\npromise\\n \\nof\\n \\na\\n \\nbribe\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefit\\n \\nis\\n \\nsufficient\\n \\nto\\n \\ncause\\n \\na\\n \\nviolation.\\n  \\nAll\\n \\nof\\n \\nthe\\n \\nanti-bribery\\n \\nprohibitions\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy\\n \\napply\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\none\\n \\nuses\\n \\nWSO2\\n \\nfunds\\n \\nor\\n \\npersonal\\n \\nfunds\\n \\nto\\n \\nfinance\\n \\nimproper\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits.\\n \\n \\nThis  Policy  also  prohibits  WSO2  Personnel  from  soliciting  or  accepting  bribes,  kickbacks,  or  other  \\nimproper\\n \\npayments/benefits\\n \\nfrom\\n \\nthe\\n \\nCompany’s\\n \\nvendors\\n \\nor\\n \\nother\\n \\npersons\\n \\nin\\n \\nrelation\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\nFor\\n \\n5     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_5', embedding=None, metadata={'page_label': '6', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only instance,  a  violation  of  this  Policy  will  occur  if  you  cause  WSO2  to  overpay  a  vendor  and  that  vendor  then  \\nshares\\n \\nall\\n \\nor\\n \\na\\n \\nportion\\n \\nof\\n \\nthat\\n \\noverpayment\\n \\nwith\\n \\nyou.\\n   \\n \\nThis  Policy  requires  WSO2  Personnel  to  adhere  to  high  ethical  standards  and  to  comply  with  all  \\napplicable\\n \\nlaws\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nthe\\n \\nCompany.\\n  \\nAnti-corruption\\n \\nviolations\\n \\ntypically\\n \\ninvolve\\n \\ncircumstances\\n \\nthat\\n \\nalso\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws,\\n \\nincluding\\n \\nthose\\n \\nthat\\n \\naddress\\n \\nmoney\\n \\nlaundering,\\n \\nembezzlement,\\n \\nfraud,\\n \\nexport\\n \\ncontrols,\\n \\nand\\n \\nsanctions/embargoes.\\n \\nGuilty\\n \\npersons\\n \\ncan\\n \\nface\\n \\nmultiple\\n \\ncharges\\n \\nbased\\n \\non\\n \\nthe\\n \\nsame\\n \\nset\\n \\nof\\n \\nfacts.\\n \\n \\nIV.   A\\nCCOUNTING\\n \\nR\\nEQUIREMENTS\\n \\n \\nWSO2  must  maintain  books,  records,  and  accounts,  which,  in  reasonable  detail,  accurately  and  fairly  \\nreflect\\n \\nthe\\n \\nCompany’s\\n \\ntransactions,\\n \\nexpenses,\\n \\nand\\n \\nasset\\n \\ndispositions.\\n \\nWSO2\\n \\nis\\n \\nalso\\n \\ncommitted\\n \\nto\\n \\nmaintaining\\n \\na\\n  \\nsystem\\n \\nof\\n \\ninternal\\n \\naccounting\\n \\ncontrols\\n \\nto\\n \\nprovide\\n \\nreasonable\\n \\nassurances\\n \\nthat\\n \\ntransactions\\n \\nare\\n \\nproperly\\n \\nauthorized\\n \\nby\\n \\nmanagement,\\n \\nexecuted,\\n \\nand\\n \\nrecorded.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\ncomply\\n \\nwith\\n  \\nour\\n \\ninternal\\n \\ncontrols\\n \\nand\\n \\navoid\\n \\nunauthorized\\n \\nactivities\\n \\nor\\n \\nexpenses.\\n  \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\nalso\\n \\ncooperate\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nperiodic\\n \\naudits\\n \\nand\\n \\nother\\n \\nefforts\\n \\nto\\n \\nensure\\n \\nthat\\n \\nour\\n \\ninternal\\n \\ncontrols\\n \\nare\\n \\nbeing\\n \\nobserved.\\n \\n \\nViolations  of  the  above  accounting  standards  can  occur  if  one  conceals  bribes  or  falsifies  other  \\ntransactions\\n \\nor\\n \\nexpenses,\\n \\neven\\n \\nif\\n \\nthey\\n \\nare\\n \\nnot\\n \\nrelated\\n \\nto\\n \\na\\n \\nbribe,\\n \\nin\\n \\nWSO2’s\\n \\nledgers\\n \\nor\\n \\nother\\n \\nrecords.\\n  \\nAlso,\\n \\nthere\\n \\nis\\n \\nno\\n \\nmateriality\\n \\nstandard.\\n \\nThis\\n \\nmeans\\n \\nthat\\n \\neven\\n \\nsmall\\n \\nmisreported\\n \\namounts\\n \\nmay\\n \\nresult\\n \\nin\\n \\nviolations.\\n   \\n \\nV.   F\\nACILITATION\\n \\nP\\nAYMENTS\\n \\n \\n \\nThis  Policy  prohibits  all  corrupt  payments  or  benefits,  including  so-called  grease  or  facilitation  payments  \\nprovided\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\nto\\n \\nexpedite\\n \\nor\\n \\nsecure\\n \\nroutine\\n \\ngovernment\\n \\nactions\\n \\n(collectively,\\n \\n“\\nFacilitation\\n \\nPayments\\n”).\\n  \\nFacilitation\\n \\nPayments\\n \\ninclude\\n \\npayments\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nto\\n \\nexpedite\\n \\nroutine\\n \\nand\\n \\nnondiscretionary\\n \\nactivities,\\n \\nsuch\\n \\nas\\n \\nprocessing\\n \\npermit\\n \\nand\\n \\nlicense\\n \\napplications,\\n \\nscheduling\\n \\ninspections,\\n \\nand/or\\n \\nproviding\\n \\ninfrastructure\\n \\nservices\\n \\n(\\ne.g.\\n,\\n \\nwater,\\n \\nelectricity\\n \\nmail).\\n  \\nWSO2\\n \\nstrictly\\n \\nprohibits\\n \\nthe\\n \\noffer,\\n \\npromise,\\n \\nor\\n \\nprovision\\n \\nof\\n \\nFacilitation\\n \\nPayments\\n \\nto\\n \\nany\\n \\ndomestic\\n \\nor\\n \\nforeign\\n \\nlocal\\n \\nor\\n \\nfederal\\n \\ngovernment\\n \\nofficial,\\n \\nas\\n \\nthey\\n \\ncan\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws\\n \\nand\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndescribed\\n \\nabove.\\n \\n \\nPlease  note  that  in  some  cases,  government  agencies  may  impose  official  fees  that  may  be  paid  directly  in  \\nthe\\n \\nname\\n \\nof\\n \\na\\n \\ngovernmental\\n \\nentity\\n \\nor\\n \\nenterprise\\n \\nitself,\\n \\nas\\n \\nset\\n \\nout\\n \\nin\\n \\npublished\\n \\nfee\\n \\nschedules\\n \\nor\\n \\nother\\n \\nofficial\\n \\ndocuments.\\n  \\nThese\\n \\nofficial\\n \\ngovernment\\n \\nfees\\n \\ncan\\n \\nbe\\n \\npaid\\n \\nto\\n \\nexpedite\\n \\npassports,\\n \\nlicenses,\\n \\nor\\n \\nother\\n \\nservices,\\n \\nprovided\\n \\nthat\\n \\nthey\\n \\nare\\n \\ndeposited\\n \\nin\\n \\nthe\\n \\ntreasury\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nan\\n \\nofficial\\n \\ngovernment\\n \\nreceipt\\n \\nis\\n \\ncollected,\\n \\nand\\n \\nthe\\n \\nexpense\\n \\nis\\n \\naccurately\\n \\nrecorded\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n  \\nHowever,\\n \\nFacilitation\\n \\nPayments\\n \\nprovided\\n \\nfor\\n \\nthe\\n \\nbenefit\\n \\nof\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\n(\\ni.e.\\n,\\n \\nare\\n \\nnot\\n \\ndeposited\\n \\nin\\n \\nan\\n \\nofficial\\n \\ntreasury\\n \\naccount\\n \\nbelonging\\n \\nto\\n \\na\\n \\ngovernment)\\n \\nwill\\n \\nviolate\\n \\nthis\\n \\nPolicy.\\n     \\n \\n \\nVI.   I\\nNTERMEDIARIES\\n \\nAND\\n \\nB\\nUSINESS\\n \\nP\\nARTNERS\\n \\n \\nThis  Policy  prohibits  WSO2  Personnel  from  providing  bribes  or  other  improper  benefits  directly  as  well  \\nas\\n \\nindirectly\\n \\nthrough\\n \\nthird\\n \\nparties.\\n \\nThis\\n \\nrisk\\n \\ncan\\n \\narise\\n \\nin\\n \\ncases\\n \\nwhere\\n \\nthe\\n \\nCompany\\n \\nworks\\n \\nwith\\n \\nagents,\\n 6     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_6', embedding=None, metadata={'page_label': '7', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only consultants,  representatives,  lobbyists,  suppliers/vendors,  resellers,  distributors,  customs  or  other  brokers,  \\ncontractors,\\n \\nadvisors,\\n \\nother\\n \\nbusiness\\n \\npartners,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nthat\\n \\nperforms\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\nWSO2\\n \\n(collectively\\n \\n“\\nIntermediaries\\n”).\\n   \\n \\nIn  certain  cases,  WSO2  and  WSO2  Personnel  can  be  held  liable  under  the  FCPA  and  other  laws  even  if  \\none\\n \\ndoes\\n \\nnot\\n \\nexpressly\\n \\nauthorize\\n \\nan\\n \\nIntermediary\\n \\nto\\n \\nengage\\n \\nin\\n \\ncorruption,\\n \\nbut\\n \\nthey\\n \\ndo\\n \\nso\\n \\nanyway.\\n \\nThis\\n \\ncan\\n \\noccur\\n \\nif\\n \\none\\n \\n(i)\\n \\nhas\\n \\nactual\\n \\nknowledge\\n \\nor\\n \\na\\n \\nfirm\\n \\nbelief\\n \\nthat\\n \\na\\n \\nperson\\n \\nwill\\n \\nengage\\n \\nin\\n \\ncorruption\\n \\nor\\n \\n(ii)\\n \\nconsciously\\n \\ndisregards,\\n \\ndeliberately\\n \\nignores,\\n \\nor\\n \\nis\\n \\nwillfully\\n \\nblind\\n \\nto\\n \\nthe\\n \\nIntermediary’s\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\npractices.\\n  \\n \\nGiven  these  risks,  this  Policy  requires  that  (i)  appropriate,  risk-based  anti-corruption  due  diligence  is  \\nperformed\\n \\non\\n \\nIntermediaries\\n \\nto\\n \\nconfirm\\n \\nthat\\n \\nsuch\\n \\nIntermediary\\n \\ndoes\\n \\nnot\\n \\nhave\\n \\na\\n \\nhistory\\n \\nor\\n \\nreputation\\n \\nfor\\n \\ncorruption\\n \\nor\\n \\nsimilar\\n \\nwrong\\n \\ndoing,\\n \\nand\\n \\n(ii)\\n \\nthe\\n \\nIntermediary\\n \\nhas\\n \\nexecuted\\n \\na\\n \\nwritten\\n \\nagreement\\n \\ncontaining\\n \\nanti-corruption\\n \\ncompliance\\n \\nclauses.\\n \\nPlease\\n \\nconsult\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nfor\\n \\ninformation\\n \\nregarding\\n \\nWSO2’s\\n \\nIntermediary\\n \\ndue\\n \\ndiligence\\n \\nprocedures.\\n \\n \\nThroughout  any  relationship  with  an  Intermediary,  WSO2  Personnel  must  monitor  their  performance  to  \\nensure\\n \\nthat\\n \\nthey\\n \\ndo\\n \\nnot\\n \\nengage\\n \\nin\\n \\nactivities\\n \\nthat\\n \\nraise\\n \\ncorruption\\n \\nconcerns.\\n  \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nprovide\\n \\nguidance\\n \\non\\n \\nthe\\n \\ntypes\\n \\nof\\n \\nred\\n \\nflags\\n \\nthat\\n \\none\\n \\nshould\\n \\nmonitor\\n \\nbefore\\n \\nand\\n \\nafter\\n \\nengaging\\n \\nan\\n \\nIntermediary.\\n \\n \\nThis  Policy  requires  WSO2  Personnel  to  notify  the  Compliance  Officer  if  they  learn  of  any  Company  \\nIntermediary\\n \\nthat\\n \\nengages\\n \\nin\\n \\ncorrupt\\n \\nor\\n \\nother\\n \\nimproper\\n \\npractices.\\n \\nAlso,\\n \\nall\\n \\npayments\\n \\nto\\n \\nIntermediaries\\n \\nmust\\n \\nbe\\n \\naccurately\\n \\nreported\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks\\n \\nand\\n \\nrecords\\n \\nin\\n \\naccordance\\n \\nwith\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndiscussed\\n \\nabove.\\n \\n \\nVII.   G\\nIFTS\\n \\nAND\\n \\nH\\nOSPITALITIES\\n \\n \\nAnti-Corruption  Laws  prohibit  the  provision  or  acceptance  of  money  or  things  of  value  for  corrupt  or  \\nimproper\\n \\npurposes.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthis\\n \\nprohibition\\n \\nis\\n \\nlikely\\n \\nin\\n \\ninstances\\n \\nwhere\\n \\npersonal\\n \\nbenefits\\n \\nare\\n \\ngiven\\n \\nor\\n \\naccepted\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nnegotiation\\n \\nor\\n \\ntender\\n \\nbid.\\n \\nHowever,\\n \\nreasonably\\n \\npriced\\n \\ngifts,\\n \\nmeals,\\n \\nentertainment,\\n \\ntravel,\\n \\nand\\n \\nother\\n \\nbenefits\\n \\nprovided\\n \\nfor\\n \\nnon-corrupt\\n \\nbusiness\\n \\npromotion\\n \\nor\\n \\ngoodwill\\n \\npurposes\\n \\nmay\\n \\nbe\\n \\npermissible\\n \\nunder\\n \\nAnti-Corruption\\n \\nLaws\\n \\nin\\n \\ncertain\\n \\ncases.\\n  \\nFor\\n \\ninstance,\\n \\na\\n \\nplastic\\n \\npen,\\n \\na\\n \\nt-shirt,\\n \\na\\n \\ncoffee\\n \\nmug,\\n \\na\\n \\npaper\\n \\nweight,\\n \\nor\\n \\na\\n \\ncap\\n \\nof\\n \\nmoderate\\n \\nvalue\\n \\nand\\n \\nembossed\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nlogo\\n \\nwill\\n \\ngenerally\\n \\nnot\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws.\\n \\nHowever,\\n \\na\\n \\nfur\\n \\ncoat,\\n \\na\\n \\ncar,\\n \\nor\\n \\na\\n \\nvacation\\n \\nwill\\n \\nraise\\n \\nanticorruption\\n \\nconcerns,\\n \\nespecially\\n \\nif\\n \\nsuch\\n \\nbenefits\\n \\nare\\n \\nprovided\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nother\\n \\nperson\\n \\nwho\\n \\nis\\n \\nresponsible\\n \\nfor\\n \\nmaking\\n \\ndecisions\\n \\nin\\n \\nrelation\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n   \\n \\nWSO2  Personnel  must  also  ensure  that  the  provision  of  a  gift  or  other  benefit  does  not  violate  local  laws  \\nor\\n \\npolicies\\n \\nthat\\n \\napply\\n \\nin\\n \\nthe\\n \\ncountry\\n \\nwhere\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nthe\\n \\nbenefit\\n \\nis\\n \\nlocated.\\n  \\nSome\\n \\ncountries\\n \\nimpose\\n \\nexpress\\n \\nlimits\\n \\non\\n \\nthe\\n \\nvalue\\n \\nof\\n \\ngifts/benefits\\n \\nthat\\n \\na\\n \\nrecipient\\n \\ncan\\n \\naccept;\\n \\nother\\n \\ncountries\\n \\nban\\n \\nsuch\\n \\ngifts/benefits\\n \\naltogether\\n \\neven\\n \\nif\\n \\ngiven\\n \\nwith\\n \\nno\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\nintention.\\n \\n \\nWSO2  Personnel  must  obtain  the  approval  of  the  Compliance  Officer  prior  to  providing  gifts,  meals,  \\ntravel\\n \\nbenefits,\\n \\nand\\n \\nother\\n \\nhospitalities\\n \\nto\\n \\nemployees,\\n \\nofficials,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\ngovernment,\\n \\npolitical\\n \\nparty,\\n \\nstateowned\\n \\nentity,\\n \\nor\\n \\npublic\\n \\ninternational\\n \\norganization.\\n \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nhelp\\n \\ndetermine\\n \\n7     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_7', embedding=None, metadata={'page_label': '8', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only whether  the  provision  of  the  benefit  is  permissible  under  applicable  Anti-Corruption  Laws.   If  the  expense  \\nis\\n \\napproved,\\n \\nits\\n \\nvalue\\n \\nand\\n \\nbusiness\\n \\npurpose\\n \\nmust\\n \\nbe\\n \\nrecorded\\n \\naccurately\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nCompany\\n \\npersonnel\\n \\nfrom\\n \\nproviding\\n \\ncash\\n \\nor\\n \\ngift\\n \\ncards\\n \\nor\\n \\ngift\\n \\ncertificates\\n \\nthat\\n \\ncan\\n \\neasily\\n \\nbe\\n \\nconverted\\n \\ninto\\n \\ncash.\\n \\n \\nIX.   O\\nTHER\\n \\nA\\nCTIVITIES\\n \\n \\nCorruption  concerns  can  arise  in  a  number  of  other  cases  including,  but  not  limited  to  (i)  joint  ventures  or  \\nteaming\\n \\narrangements\\n \\nwith\\n \\npublic\\n \\nor\\n \\nprivate-sector\\n \\npartners;\\n \\n(ii)\\n \\nmergers\\n \\nand\\n \\nacquisitions,\\n \\nespecially\\n \\nif\\n \\nthe\\n \\ntarget\\n \\nbusiness\\n \\nhas\\n \\nsignificant\\n \\ngovernment\\n \\ninteractions\\n \\nor\\n \\nan\\n \\ninternational\\n \\nprofile;\\n \\nand\\n \\n(iii)\\n \\ncharitable\\n \\nand\\n \\npolitical\\n \\ndonations.\\n \\nPlease\\n \\nconfer\\n \\nwith\\n \\nthe\\n \\nCompliance\\n \\nOfficer\\n \\nbefore\\n \\nengaging\\n \\nin\\n \\nthese\\n \\ntypes\\n \\nof\\n \\nactivities\\n \\nto\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nanti-corruption\\n \\ncompliance\\n \\nmeasures\\n \\nare\\n \\nobserved.\\n \\n \\n \\nX.   V\\nIOLATIONS\\n \\nAND\\n \\nC\\nONSEQUENCES\\n \\n \\nA  violation  of  this  Policy  will  result  in  appropriate  disciplinary  action,  including  demotion,  reassignment,  \\nadditional\\n \\ntraining,\\n \\nprobation,\\n \\nsuspension,\\n \\nor\\n \\neven\\n \\ntermination.\\n \\n \\nBoth  the  Company  and  Company  Personnel  may  be  subject  to  substantial  fines  and  penalties  for  violating  \\nAnti-Corruption\\n \\nLaws.\\n  \\nIn\\n \\nserious\\n \\ncases,\\n \\nindividuals\\n \\nmay\\n \\nface\\n \\nimprisonment,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\nassessment\\n \\nof\\n \\nmonetary\\n \\nfines\\n \\nand\\n \\npenalties.\\n  \\nIn\\n \\naddition,\\n \\nthe\\n \\nCompany\\n \\nmay\\n \\nface\\n \\nsuspension\\n \\nor\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts,\\n \\nthe\\n \\nloss\\n \\nof\\n \\nU.S.\\n \\nexport\\n \\nprivileges,\\n \\nand\\n \\ncertain\\n \\nother\\n \\nconsequences.\\n \\nThese\\n \\nresults\\n \\ncan\\n \\nbe\\n \\ndevastating\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\n \\nXI.   T\\nRAINING\\n \\nAND\\n \\nC\\nERTIFICATION\\n \\n \\n \\nAll  designated  personnel  must  undergo  anti-corruption  training  provided  by  WSO2.   The  nature,  content,  \\nand\\n \\nfrequency\\n \\nof\\n \\nthat\\n \\ntraining\\n \\nwill\\n \\nbe\\n \\ndetermined\\n \\nby\\n \\nWSO2\\n \\nbased\\n \\non\\n \\nrisk\\n \\nprofile.\\n  \\n \\nWSO2  may  require  certain  WSO2  Personnel  to  certify  compliance  with  this  Policy  on  a  periodic  basis.   \\nXII.   S\\nTATUS\\n \\n \\n \\nThe  Compliance  Officer  and/or  outside  counsel  will  review  this  Policy  on  a  periodic  basis  and  update  it,  \\nas\\n \\nappropriate,\\n \\nto\\n \\nreflect\\n \\nany\\n \\nchanges.\\n   \\n \\nThis  Policy  does  not  form  part  of  any  employment  contract  with  you  and  may  be  amended  at  any  time.   \\nThis\\n \\nPolicy\\n \\nshould\\n \\nbe\\n \\nread\\n \\nin\\n \\nconjunction\\n \\nwith\\n \\nWSO2’s\\n \\nother\\n \\npolicies.\\n \\n \\n \\n8     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_8', embedding=None, metadata={'page_label': '9', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only XIII.   R\\nEPORTING\\n/Q\\nUESTIONS\\n  \\n \\n \\nWSO2  Personnel  have  an  affirmative  obligation  to  report  all  violations  of  this  Policy  to  the  Compliance  \\nOfficer\\n \\nas\\n \\nfollows:\\n \\n \\nPuny  Navaratne  legal-compliance@wso2.com   \\nReports  may  also  be  submitted  anonymously  by  using  the  Company’s  hotline  number  \\n(800)\\n \\n461-9330\\n \\nor\\n \\nonline\\n \\nat\\n \\nwhistleblower.wso2.com.\\n    \\nHowever,\\n \\nwe\\n \\nencourage\\n \\nyou\\n \\nto\\n \\nconsider\\n \\nrevealing\\n \\nyour\\n \\nidentity\\n \\nso\\n \\nthat\\n \\nwe\\n \\ncan\\n \\nproperly\\n \\nfollow\\n \\nup\\n \\nand\\n \\ninvestigate\\n \\nalleged\\n \\nviolations.\\n \\nThe\\n \\nCompany\\n \\nwill\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nconfidentiality\\n \\nmeasures\\n \\nare\\n \\ntaken\\n \\nand\\n \\nwill\\n \\nnot\\n \\nretaliate\\n \\nagainst\\n \\nany\\n \\nindividual\\n \\nfor\\n \\nreporting\\n \\nviolations\\n \\nin\\n \\ngood\\n \\nfaith.\\n \\n \\nWSO2  Personnel  must  also  notify  the  Compliance  Officer  of  any  corrupt,  improper,  illegal,  or  other  \\nunusual\\n \\nrequests\\n \\nfor\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits\\n \\nmade\\n \\nby\\n \\ncustomers,\\n \\nIntermediaries,\\n \\nvendors,\\n \\nbusiness\\n \\npartners,\\n \\ngovernment\\n \\nofficials,\\n \\nor\\n \\nCompany\\n \\nemployees.\\n   \\nBy\\n \\nreporting\\n \\nsuch\\n \\nmatters,\\n \\nyou\\n \\nwill\\n \\nenable\\n \\nus\\n \\nto\\n \\nexplore\\n \\noptions\\n \\nto\\n \\nachieve\\n \\nour\\n \\nbusiness\\n \\ngoals\\n \\nwithout\\n \\nhaving\\n \\nto\\n \\ninteract\\n \\nwith\\n \\nsuch\\n \\npersons\\n \\nor\\n \\nprovide\\n \\nimproper\\n \\nbenefits.\\n \\n \\nIX.   ACKNOWLEDGEMENT\\n  \\n \\n \\nPlease  click here to  certify  and  acknowledge  that  you  have  read  and  understood  the  contents  of  this  Policy.  \\n9     \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_9', embedding=None, metadata={'page_label': '10', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only ATTACHMENT  1   \\nANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL   \\n  \\nUNITED  KINGDOM   \\nT\\nHE\\n \\nUK\\n \\nB\\nRIBERY\\n \\nA\\nCT\\n \\n2010  \\n \\n  Among  various  matters,  the  UK  Bribery  Act  2010  (the  “ UKBA ”)  prohibits  individuals  and  entities  from  \\noffering,\\n \\npromising,\\n \\nor\\n \\ngiving\\n \\n(directly\\n \\nor\\n \\nindirectly\\n \\nthrough\\n \\na\\n \\nthird\\n \\nparty)\\n \\na\\n \\nfinancial\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nto\\n \\na\\n \\nrecipient\\n \\nwith\\n \\n(i)\\n \\nthe\\n \\nintention\\n \\nthat\\n \\nthe\\n \\nadvantage\\n \\ninduce\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\nperform\\n \\nimproperly\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity\\n \\nor\\n \\nto\\n \\nreward\\n \\na\\n \\nperson\\n \\nfor\\n \\nthe\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\nsuch\\n \\nfunction\\n \\nor\\n \\nactivity,\\n \\nor\\n \\n(ii)\\n \\nthe\\n \\nknowledge\\n \\nor\\n \\nbelief\\n \\nthat\\n \\nthe\\n \\nacceptance\\n \\nof\\n \\nthe\\n \\nadvantage\\n \\nwould\\n \\nitself\\n \\nconstitute\\n \\nan\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nUKBA\\n \\nwill\\n \\noccur\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nadvantage\\n \\nis\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nan\\n \\nemployee\\n \\nof\\n \\na\\n \\nprivatesector\\n \\nentity.\\n \\n   The  UKBA  contains  four  principal  offenses  as  follows:  (i)  offering,  promising,  or  giving  of  a  bribe  to  \\nanother\\n \\nperson\\n \\n(Section\\n \\n1);\\n \\n(ii)\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\na\\n \\nbribe\\n \\n(Section\\n \\n2);\\n \\n(iii)\\n \\nbribery\\n \\nof\\n \\na\\n \\nforeign\\n \\n(non-UK)\\n \\npublic\\n \\nofficial\\n \\n(Section\\n \\n6);\\n \\nand\\n \\n(iv)\\n \\nfailure\\n \\nby\\n \\ncertain\\n \\ncommercial\\n \\norganizations\\n \\nto\\n \\nprevent\\n \\nSection\\n \\n1\\n \\nor\\n \\n6\\n \\nbribery\\n \\noffenses\\n \\nby\\n \\ntheir\\n \\nassociated\\n \\npersons\\n \\n(including\\n \\nemployees,\\n \\ncontractors,\\n \\nIntermediaries,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\na\\n \\ncompany)\\n \\nof\\n \\nany\\n \\nnationality\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(Section\\n \\n7).\\n  \\nThe\\n \\nUKBA\\n \\nprovides\\n \\na\\n \\nstatutory\\n \\ndefense\\n \\nto\\n \\na\\n \\nSection\\n \\n7\\n \\nviolation\\n \\nfor\\n \\ncompanies\\n \\nthat\\n \\ncan\\n \\ndemonstrate\\n \\nthat\\n \\nthey\\n \\nhad\\n \\nin\\n \\nplace\\n \\nadequate\\n \\nsystems\\n \\nand\\n \\ncontrols\\n \\ndesigned\\n \\nto\\n \\nprevent\\n \\noffenses\\n \\nunder\\n \\nUKBA.\\n \\nThis\\n \\nPolicy\\n \\nis\\n \\npart\\n \\nof\\n \\nthe\\n \\nCompany’s\\n \\noverall\\n \\neffort\\n \\nto\\n \\nestablish\\n \\nsuch\\n \\nsystems\\n \\nand\\n \\ncontrols.\\n  \\n   Courts  in  the  United  Kingdom  exercise  broad  jurisdiction  over  UK  as  well  as  non-UK  persons  who  \\ncommit\\n \\nUKBA\\n \\noffenses.\\n  \\nThe\\n \\nCompany\\n \\nmaintains\\n \\na\\n \\nUK\\n \\nsubsidiary.\\n \\nIt\\n \\nis\\n \\nclear\\n \\nthat\\n \\nboth\\n \\nthis\\n \\nUK\\n \\nsubsidiary\\n \\nand\\n \\nmost\\n \\nof\\n \\nits\\n \\nemployees\\n \\nwill\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nthe\\n \\nUKBA.\\n  \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-UK\\n \\nentities\\n \\nand\\n \\nemployees\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nUKBA\\n \\njurisdiction.\\n \\n    Under  the  UKBA,  individuals  guilty  of  bribery  may  be  subject  to  imprisonment  for  up  to  10  years  and/or  \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount.\\n \\nCommercial\\n \\norganizations\\n \\nguilty\\n \\nof\\n \\nbribery\\n \\nor\\n \\nfailure\\n \\nto\\n \\nprevent\\n \\nbribery\\n \\nmay\\n \\nalso\\n \\nbe\\n \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount\\n \\nas\\n \\nwell\\n \\nas\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts.\\n  \\nIn\\n \\naddition,\\n \\nUKBA\\n \\noffenses\\n \\ncould\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws\\n \\nsuch\\n \\nas\\n \\nthe\\n \\nUK\\n \\nProceeds\\n \\nof\\n \\nCrime\\n \\nAct\\n \\n2002,\\n \\nwhich\\n \\ncontains\\n \\nthe\\n \\nUK’s\\n \\nprincipal\\n \\nmoney\\n \\nlaundering\\n \\noffenses.\\n \\n    *   *   *   *   *     SRI  LANKA   \\nThe  legal  framework  for  the  prevention,  investigation  and  punishment  of  corruption  is  primarily  reflected  \\nin\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments).\\n  \\n \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_10', embedding=None, metadata={'page_label': '11', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only   The  law  prohibits  persons  from  offering  gratifications  and  rewards  to  certain  categories  of  persons  as  \\ninducements\\n \\nand\\n \\nrewards\\n \\nfor\\n \\nthe\\n \\nperformance\\n \\nor\\n \\nnonperformance\\n \\nof\\n \\nspecified\\n \\nactivities.\\n \\nThey\\n \\nare\\n \\n(a)\\n \\njudicial\\n \\nofficers\\n \\nand\\n \\nMembers\\n \\nof\\n \\nParliament\\n \\nin\\n \\nrespect\\n \\nof\\n \\ntheir\\n \\nofficial\\n \\nduties;\\n \\n(b)\\n \\npolice\\n \\nofficers,\\n \\npeace\\n \\nofficers\\n \\nor\\n \\nother\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ninterfering\\n \\nwith\\n \\nthe\\n \\ndue\\n \\nadministration\\n \\nof\\n \\njustice,\\n \\nor\\n \\nprocuring\\n \\nor\\n \\nfacilitating\\n \\nthe\\n \\ncommission\\n \\nof\\n \\nany\\n \\noffence,\\n \\nor\\n \\nprotecting\\n \\noffenders\\n \\nfrom\\n \\ndetection\\n \\nor\\n \\npunishment,\\n \\nor\\n \\nabusing\\n \\nofficial\\n \\npowers\\n \\nto\\n \\nthe\\n \\ninjury\\n \\nor\\n \\ndetriment\\n \\nof\\n \\nany\\n \\nperson;\\n \\n(c)\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ntheir\\n \\nassistance\\n \\nor\\n \\ninfluence\\n \\nin\\n \\npromoting\\n \\nthe\\n \\nprocurement\\n \\nof\\n \\nany\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nfor\\n \\nany\\n \\nwork,\\n \\nservice\\n \\nor\\n \\nthe\\n \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial\\n \\nor\\n \\nsubstance,\\n \\nor\\n \\nin\\n \\nthe\\n \\nexecution\\n \\nof\\n \\nany\\n \\ncontract,\\n \\nor\\n \\nin\\n \\nthe\\n \\npayment\\n \\nof\\n \\nthe\\n \\nprice\\n \\nor\\n \\nconsideration\\n \\nor\\n \\nof\\n \\nany\\n \\nsubsidy\\n \\nin\\n \\nrespect\\n \\nthereof;\\n \\n(d)\\n  \\na\\n \\ntenderer\\n \\nfor\\n \\na\\n \\ncontract\\n \\nto\\n \\nwithdraw\\n \\nthe\\n \\ntender,\\n \\nor\\n \\nfor\\n \\nwithdrawing\\n \\na\\n \\ntender\\n \\nmade\\n \\nfor\\n \\na\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nintent\\n \\nof\\n \\nobtaining\\n \\nsuch\\n \\ncontract\\n \\nfor\\n \\nwork,\\n \\nservice\\n \\nor\\n  \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial,\\n \\nor\\n \\nsubstance;\\n \\n(e)\\n  \\npublic\\n \\nofficers\\n \\nto\\n \\nperform,\\n \\nabstain\\n \\nfrom\\n \\nperforming,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact\\n \\nwhether\\n \\nby\\n \\nthat\\n \\npublic\\n \\nofficer\\n \\nor\\n \\nby\\n \\nany\\n \\nother\\n \\npublic\\n \\nofficer,\\n \\nor\\n \\nassisting,\\n \\nfavoring,\\n \\nhindering\\n \\nor\\n \\ndelaying\\n \\nany\\n \\nperson\\n \\nin\\n \\nthe\\n \\ntransaction\\n \\nof\\n \\nany\\n \\nbusiness\\n \\nwith\\n \\nthe\\n \\nGovernment;\\n \\n(f)\\n \\npersons\\n \\nto\\n \\nprocure\\n \\nthe\\n \\nGovernment\\n \\nto\\n \\npay\\n \\nany\\n \\nclaim,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nprevent\\n \\nappointment\\n \\nto\\n \\nany\\n \\noffice,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nsecure\\n \\nany\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nother\\n \\nbenefit\\n \\nfrom\\n \\nthe\\n \\nGovernment,\\n \\nor\\n \\nprevent\\n \\nthe\\n \\nsecuring\\n \\nof\\n \\nany\\n \\nsuch\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nbenefit\\n \\nby\\n \\nsuch\\n \\nother\\n \\nperson;\\n \\n(g)\\n \\npublic\\n \\nofficer\\n \\nemployed\\n \\nin\\n \\na\\n \\ngovernment\\n \\ndepartment,\\n \\noffice\\n \\nor\\n \\nestablishment\\n \\nwhile\\n \\nhaving\\n \\ndealings\\n \\nof\\n \\nany\\n \\nkind\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nthrough\\n \\nsuch\\n \\nan\\n \\nentity,\\n \\nor\\n \\nwithin\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nsuch\\n \\ndealings\\n \\n(provided\\n \\nthat\\n \\nif\\n \\nsuch\\n \\ngratification\\n \\nwas\\n \\npaid\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nthe\\n \\ndealings\\n \\nit\\n \\nshall\\n \\nnot\\n \\nbe\\n \\nconsidered\\n \\nan\\n \\noffence\\n \\nif\\n \\nit\\n \\ncan\\n \\nbe\\n \\nproved\\n \\nthat\\n \\nit\\n \\nwas\\n \\noffered\\n \\nin\\n \\ngood\\n \\nfaith\\n \\nfor\\n \\na\\n \\npurpose\\n \\nnot\\n \\nconnected\\n \\nwith\\n \\nor\\n \\nunrelated\\n \\nto\\n \\nsuch\\n \\ndealings,\\n \\nand\\n \\nwhen\\n \\nit\\n \\nwas\\n \\noffered,\\n \\nthere\\n \\nwas\\n \\nno\\n \\nhope\\n \\nor\\n \\nexpectation\\n \\nof\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nit\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nfor\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nthe\\n \\ngratification\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nor\\n \\nreward\\n \\nfor\\n \\nthe\\n \\npublic\\n \\nofficer\\n \\ndoing\\n \\nor\\n \\nforbearing\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nconnected\\n \\nwith\\n \\nsuch\\n \\ndealings);\\n \\n(h)\\n \\nmembers,\\n \\nofficers\\n \\nor\\n \\nemployees\\n \\nof\\n \\nlocal\\n \\nauthorities\\n \\nor\\n \\nscheduled\\n \\ninstitutions\\n \\nfor\\n \\nvoting\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nvoting\\n \\nat\\n \\nmeetings\\n \\nof\\n \\nsuch\\n \\nbodies\\n \\nfor\\n \\nor\\n \\nagainst\\n \\nmatters\\n \\narising\\n \\nbefore\\n \\nthem,\\n \\nor\\n \\ntheir\\n \\nperformance,\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nperforming,\\n \\nor\\n \\naiding\\n \\nin\\n \\nprocuring,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact,\\n \\nor\\n \\naiding,\\n \\nprocuring,\\n \\nor\\n \\npreventing\\n \\nthe\\n \\npassing\\n \\nof\\n \\nany\\n \\nvote\\n \\nor\\n \\ngranting\\n \\nof\\n \\nany\\n \\ncontract\\n \\nor\\n \\nadvantage\\n \\nin\\n \\nfavor\\n \\nof\\n \\nany\\n \\nperson.\\n  \\n \\nEach  of  the  above  provisions  have  corresponding  offence  in  respect  of  the  receipt  of  gratifications.      The  Bribery  Act  provides  for  imprisonment  of  up  to  seven  years  and  fines  of  up  to  five  thousand  Sri  \\nLankan\\n \\nrupees\\n \\nfor\\n \\nthe\\n \\ncommission\\n \\nof\\n \\noffences.\\n  \\nSri\\n \\nLankan\\n \\ncourts\\n \\nmay\\n \\nalso\\n \\nimpose\\n \\npenalties\\n \\namounting\\n \\nto\\n \\nthe\\n \\nvalue\\n \\nof\\n \\nthe\\n \\ngratification\\n \\nif\\n \\nthe\\n \\nconviction\\n \\nis\\n \\nentered\\n \\nby\\n \\na\\n \\nHigh\\n \\nCourt.\\n   \\n   Similar  offences  have  been  created  in  respect  of  members  of  public  authorities  by  the  Public  Bodies  \\n(Prevention\\n \\nof\\n \\nCorruption)\\n \\nAct\\n \\nNo\\n \\n13\\n \\nof\\n \\n1950.\\n \\n \\nThe  Bribery  Act  is  enforced  through  the  Commission  to  Investigate  Allegations  of  Bribery  or  Corruption  \\nAct\\n \\nNo.\\n \\n19\\n \\nof\\n \\n1994.\\n  \\nThe\\n \\nCommission\\n \\nhas\\n \\nwide\\n \\npowers\\n \\nof\\n \\ninvestigation\\n \\nincluding\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nas\\n \\nwell\\n \\nas\\n \\nof\\n \\nrequiring\\n \\ndeclaration\\n \\nof\\n \\nassets\\n \\nand\\n \\nliabilities\\n \\nby\\n \\nMembers\\n \\nof\\n \\nParliament,\\n \\njudges,\\n \\npublic\\n \\nofficials\\n \\nof\\n \\nGovernment\\n \\ndepartments,\\n \\nministries,\\n \\nand\\n \\nlocal\\n \\nauthorities,\\n \\nchairpersons\\n \\nand\\n \\nstaff\\n \\nof\\n \\npublic\\n \\ncorporations,\\n \\ncandidates\\n \\nfor\\n \\nelected\\n \\npublic\\n \\noffice\\n \\nand\\n \\nelected\\n \\nofficials\\n \\nunder\\n \\nthe\\n \\nDeclaration\\n \\nof\\n \\nAssets\\n \\nand\\n \\nLiabilities\\n \\nLaw,\\n \\nNo.\\n \\n1\\n \\nof\\n \\n1975.\\n  \\n \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_11', embedding=None, metadata={'page_label': '12', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only Sri  Lanka  also  has  a  domestic  legal  regime  against  money-laundering  which  includes  the  Prevention  of  \\nMoney-\\n \\nLaundering\\n \\nAct,\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct\\n \\nand\\n \\nthe\\n \\nConvention\\n \\non\\n \\nthe\\n \\nSuppression\\n \\nof\\n \\nTerrorist\\n \\nFinancing\\n \\nAct.\\n  \\nOffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nand\\n \\nother\\n \\ncorruption-related\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nPenal\\n \\nCode\\n \\nare\\n \\nconsidered\\n \\npredicate\\n \\noffences\\n \\nfor\\n \\nthe\\n \\npurposes\\n \\nof\\n \\nthe\\n \\nPrevention\\n \\nof\\n \\nMoney-Laundering\\n \\nAct\\n \\nand\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct.\\n  \\n \\n     *   *   *   *   *     B\\nRAZIL\\n \\n \\nTHE  BRAZILIAN  ANTICORRUPTION  ACT  2013   \\nLaw  No.  12,846/2013  (the  Brazilian  Anticorruption  Act  or  Lei  Anticorrupção  –  “ LAC ”)  provides  for  \\nstrict\\n \\nliability\\n \\nto\\n \\ncompanies,\\n1\\n \\nin\\n \\nthe\\n \\nadministrative\\n \\nand\\n \\ncivil\\n \\nspheres,\\n \\nfor\\n \\nwrongful\\n \\nacts\\n \\ncarried\\n \\nout\\n \\nin\\n \\ntheir\\n \\ninterest\\n \\nor\\n \\nfor\\n \\ntheir\\n \\nbenefit.\\n \\n   The  LAC  is  applicable  to  activities  after  January  2014  and  the  main  offenses  are:  (i)  promising,  offering  \\nor\\n \\ngiving,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nundue\\n \\nadvantage\\n \\nto\\n \\na\\n \\npublic\\n \\nagent\\n \\nor\\n \\nthird\\n \\nperson\\n \\nrelated\\n \\nto\\n \\nit;\\n \\n(ii)\\n \\nfinancing\\n \\nor\\n \\nin\\n \\nany\\n \\nway\\n \\nsponsoring\\n \\nthe\\n \\npractice\\n \\nof\\n \\nwrongdoings\\n \\ndescribed\\n \\nin\\n \\nthe\\n \\nAct;\\n \\n(iii)\\n \\nusing\\n \\na\\n \\nthird\\n \\nparty\\n \\nto\\n \\nconceal\\n \\nor\\n \\nsimulate\\n \\nits\\n \\nactual\\n \\ninterests\\n \\nor\\n \\nthe\\n \\nidentity\\n \\nof\\n \\nthe\\n \\nbeneficiaries\\n \\nof\\n \\nthe\\n \\nillegal\\n \\nacts\\n \\nagainst\\n \\nthe\\n \\npublic\\n \\nadministration;\\n \\n(iv)\\n \\nengaging\\n \\nin\\n \\nfraudulent\\n \\nacts\\n \\nin\\n \\npublic\\n \\ntenders,\\n \\nsuch\\n \\nas\\n \\nparticipating\\n \\nin\\n \\nbid\\n \\nrigging\\n \\nor\\n \\ndisturbing\\n \\nany\\n \\nstep\\n \\nof\\n \\nthe\\n \\npublic\\n \\ntender;\\n \\nand\\n \\n(v)\\n \\nobstructing\\n \\nor\\n \\nhampering\\n \\nthe\\n \\nsurveillance\\n \\nor\\n \\ninvestigations\\n \\nof\\n \\npublic\\n \\nentities.\\n \\n   In  the  administrative  sphere,  legal  entities  that  are  found  guilty  of  breaching  the  LAC  are  subject  to  a  fine  \\nof\\n \\n0.1%\\n \\nto\\n \\n20%\\n \\nof\\n \\nthe\\n \\ngross\\n \\nrevenue,\\n \\nless\\n \\ntaxes,\\n \\nregistered\\n \\nin\\n \\nthe\\n \\nyear\\n \\nprior\\n \\nto\\n \\nthe\\n \\ninitiation\\n \\nof\\n \\nthe\\n \\nadministrative\\n \\nproceedings,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\npublication\\n \\nof\\n \\nthe\\n \\ndecision.\\n \\nMoreover,\\n \\nother\\n \\npenalties\\n \\nmay\\n \\nbe\\n \\nenforced\\n \\nin\\n \\nthe\\n \\ncivil\\n \\nsphere\\n \\nby\\n \\ncourts,\\n \\nsuch\\n \\nas:\\n \\n(i)\\n \\nseizure\\n \\nof\\n \\nassets\\n \\nobtained\\n \\nthrough\\n \\nillegal\\n \\npractice;\\n \\n(ii)\\n \\nsuspension\\n \\nor\\n \\npartial\\n \\nshutdown\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity’s\\n \\nactivities;\\n \\n(iii)\\n \\ncompulsory\\n \\ntermination\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity;\\n \\nand\\n \\n(iv)\\n \\nprohibition\\n \\nof\\n \\nreceiving\\n \\nincentives,\\n \\nsubsidies,\\n \\ngrants,\\n \\ndonations\\n \\nor\\n \\nloans\\n \\nfrom\\n \\npublic\\n \\nbodies\\n \\nor\\n \\nentities\\n \\nor\\n \\nfrom\\n \\npublic\\n \\nfinancial\\n \\ninstitutions\\n \\nor\\n \\npublicly-controlled\\n \\nfinancial\\n \\ninstitutions,\\n \\nfrom\\n \\n1\\n \\nto\\n \\n5\\n \\nyears.\\n \\n   The  existence  of  an  effective  compliance  program  may  be  considered  a  mitigating  factor  in  the  evaluation  \\nof\\n \\nthe\\n \\napplicable\\n \\nfine\\n \\nto\\n \\nbe\\n \\napplied\\n \\nto\\n \\nthe\\n \\nlegal\\n \\nentity.\\n \\n   The  Company  maintains  a  Brazilian  subsidiary.  It  is  clear  that  the  Brazilian  subsidiary  may  be  held  liable  \\nunder\\n \\nthe\\n \\nLAC.\\n \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-Brazilian\\n \\nentities\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nLAC\\n \\njurisdiction.\\n \\n   \\n1\\n  In  this  sense,  the  authorities  are  not  required  to  show  intent  or  fault  of  the  legal  entity.  The  mere  fact  that  there  is  \\nmateriality\\n \\nas\\n \\nto\\n \\nthe\\n \\nviolation\\n \\nand\\n \\nthat\\n \\nthe\\n \\nviolation\\n \\nhappened\\n \\nin\\n \\nthe\\n \\ninterest\\n \\nor\\n \\nbenefit\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity\\n \\nis\\n \\nsufficient\\n \\nto\\n \\nconsider\\n \\nthat\\n \\nthere\\n \\nis\\n \\na\\n \\nbreach\\n \\nto\\n \\nthe\\n \\nAct.\\n \\n  \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
            " Document(id_='/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf_part_12', embedding=None, metadata={'page_label': '13', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only T\\nHE\\n \\nB\\nRAZILIAN\\n \\nI\\nMPROBITY\\n \\nA\\nCT\\n \\n1992  \\n \\n  In  a  broad  sense,  Law  No.  8,429/1992  (the  Brazilian  Improbity  Act  or  Lei  de  Improbidade  Administrativa   –  “ LIA ”)  is  applicable  to  (i)  facts  that  also  constitute  a  violation  to  the  LAC  but  happened  before  January   \\n  2014;  and  (ii)  facts  that  happened  after  January  2014  and  do  not  constitute  a  breach  to  the  LAC,  but  are  a  \\nbreach\\n \\nof\\n \\nthe\\n \\nLIA.\\n \\n   There  are  three  broad  types  of  misconduct  provided  for  in  the  LIA:  (i)  unjust  enrichment;  (ii)  damage  to  \\nthe\\n \\npublic\\n \\ntreasury;\\n \\nand\\n \\n(iii)\\n \\nacts\\n \\nin\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nprinciples\\n \\nthat\\n \\ngovern\\n \\nthe\\n \\nPublic\\n \\nAdministration.\\n  \\nThe\\n \\nLIA\\n \\nprovides\\n \\nfor\\n \\nsanctions\\n \\non\\n \\npublic\\n \\nagents,\\n \\nas\\n \\nwell\\n \\nas\\n \\non\\n \\nprivate\\n \\nentities\\n \\nand\\n \\nindividuals\\n \\nthat\\n \\nwillfully\\n \\naided\\n \\nor\\n \\nparticipated\\n \\nin\\n \\nimprobity\\n \\nacts.\\n  \\nIts\\n \\nmain\\n \\nsanctions\\n \\nare\\n \\n(i)\\n \\nforfeiture\\n \\nof\\n \\nassets\\n \\nor\\n \\nvalues\\n \\nunlawfully\\n \\nobtained;\\n \\n(ii)\\n \\ndismissal\\n \\nfrom\\n \\npublic\\n \\noffice;\\n \\n(iii)\\n \\npolitical\\n \\nblacklisting;\\n \\n(iv)\\n \\npayment\\n \\nof\\n \\nfines\\n \\nequivalent\\n \\nto\\n \\nthe\\n \\nunlawfully\\n \\nobtained\\n \\namounts;\\n \\n(v)\\n \\nprohibition\\n \\nagainst\\n \\ncontracting\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nor\\n \\nreceiving\\n \\ntax\\n \\nor\\n \\ncredit\\n \\nincentives,\\n \\ndirectly\\n \\nor\\n \\nindirectly;\\n \\nand\\n \\n(vi)\\n \\npayment\\n \\nof\\n \\nfines.\\n  \\nIn\\n \\norder\\n \\nfor\\n \\na\\n \\nsanction\\n \\nto\\n \\nbe\\n \\napplied,\\n \\nit\\n \\nis\\n \\nnecessary\\n \\nto\\n \\nshow\\n \\nintent\\n \\nof\\n \\nthe\\n \\nwrongdoer.\\n \\n   O\\nTHER\\n \\nP\\nOTENTIAL\\n \\nL\\nIABILITIES\\n \\n \\nIn  addition  to  the  LAC  and  the  LIA,  private  and  public  entities  can  request  compensation  for  collective  or  \\nmoral\\n \\ndamages\\n \\nresulting\\n \\nfrom\\n \\ncorruption\\n \\ncases,\\n \\nas\\n \\nprovided\\n \\nby\\n \\nBrazilian\\n \\nClass\\n \\nAction\\n \\nLaw,\\n \\nand\\n \\nentities\\n \\ndeemed\\n \\nto\\n \\nbe\\n \\nharmed/damaged\\n \\nby\\n \\nthe\\n \\nwrongdoing\\n \\ncan\\n \\nfile\\n \\na\\n \\nlawsuit\\n \\nclaiming\\n \\ncompensation\\n \\nfor\\n \\ndamages.\\n \\nAlso,\\n \\nthe\\n \\nBrazilian\\n \\nFederal\\n \\nCourt\\n \\nof\\n \\nAccounts\\n \\ncan\\n \\nimpose\\n \\nsanctions\\n \\nif\\n \\nthey\\n \\nfind\\n \\ncontract\\n \\nfraud\\n \\nwhile\\n \\nauditing\\n \\npublic\\n \\nentities.\\n \\n   Finally,  the  Brazilian  Criminal  Code  sets  forth  that  corruption  may  result  in  imprisonment  for  up  to  16  \\nyears\\n \\nfor\\n \\nindividuals.\\n \\n        \\n \\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "pprint.pprint(docs_nam_as_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzFOVgD2PQXT",
        "outputId": "57b8969c-87ee-4cdd-e6cb-664b8e898d15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id_': '487b9093-1ad3-4685-ab72-3428a9365856',\n",
              " 'embedding': None,\n",
              " 'metadata': {'page_label': '1',\n",
              "  'file_name': '2502.09838v3.pdf',\n",
              "  'file_path': '/content/data/2502.09838v3.pdf',\n",
              "  'file_type': 'application/pdf',\n",
              "  'file_size': 8786923,\n",
              "  'creation_date': '2025-09-01',\n",
              "  'last_modified_date': '2025-09-01'},\n",
              " 'excluded_embed_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'excluded_llm_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'relationships': {},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text_resource': MediaResource(embeddings=None, data=None, text='HealthGPT: A Medical Large Vision-Language Model for Unifying\\nComprehension and Generation via Heterogeneous Knowledge Adaptation\\nTianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\\nMengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\\n1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\\n4The Hong Kong University of Science and Technology,5National University of Singapore\\nProject Page\\n Code\\n1. X-Ray \\nComprehension\\n2. CT\\nComprehension\\n3. MRI\\nComprehension\\nComp. Perf.\\n7 Medical Multi-Modal Comprehension Tasks\\nGen. \\nPerformance\\n5 Medical Multi-Modal Generation Tasks\\nList all anatomical locations showing \\npulmonary edema, hazy opacity, or \\nmediastinal displacement.\\nLeft hilar structures, left lung,\\nright hilar structures, right lung.\\nWhich abdominal organ shows any \\nindication of a lesion or abnormality \\nin the CT image?\\nNo abdominal organs show\\nany clear indications of lesions\\nor abnormalities.\\nCould you explain what this mass\\nin the MRI means for my health? \\nIs it very serious?\\nCertainly, the MRI shows a\\ndefined mass in your left nasal\\ncavity. There is no sign of the ...\\n4. Microsopy Comprehension\\n What is the purpose of the different \\nmembrane treatments used in this \\nstudy?\\nThe purpose of the different\\nmembrane treatments used…\\n5. OCT Comprehension\\n What is the purpose of comparing \\nthe OCT structure image and OCTA \\nimage with H&E histology?\\nTo confirm the histological position\\nof the obtained OCT brain images.\\n6. Fundus\\nComprehension\\n What specific findings or pathological \\nchanges can be observed in this \\nfundus image?\\nThe fundus image appears normal with\\nno noticeable signs of pathology…\\n7. Ultrasound\\nComprehension\\n What type of imaging technique \\nis used in this image?\\nThe image is a sagittal gray-\\nscale ultrasonographic…\\n1. CT2MRI\\nGeneration\\nI need a version of this CT representation \\nin MRI.\\nThe image has\\nbeen transformed\\ninto MRI.\\n2. MRI2CT\\nGeneration\\nTransform the MRI display into a \\nCT image.\\nHere is the CT\\nversion of the\\nMRI image.\\n3. Image Reconstruction\\nReconstruct the following \\nmedical images.\\nHere is the reconstructed\\nmedical image you need.\\n4. Super Resolution\\nCould you improve the quality\\nof this MRI image?\\nHere is the image with\\nimproved resolution.\\n5. Report-to-CXR\\nThe X-ray shows no \\npleural effusion or \\npneumothorax.\\nHere is the\\nchest X-ray\\nimage for\\nyou.\\nGen. Perf.\\nFigure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\\nunified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\\nplex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\\nAbstract\\nWe present HealthGPT, a powerful Medical Large Vision-\\nLanguage Model (Med-LVLM) that integrates medical vi-\\nsual comprehension and generation capabilities within a uni-\\nfied autoregressive paradigm. Our bootstrapping philosophy\\nis to progressively adapt heterogeneous comprehension and\\ngeneration knowledge to pre-trained large language mod-\\nels (LLMs). This is achieved through a novel heterogeneous\\nlow-rank adaptation (H-LoRA) technique, which is com-\\nplemented by a tailored hierarchical visual perception ap-\\nproach and a three-stage learning strategy. To effectively\\nlearn the HealthGPT, we devise a comprehensive medi-\\ncal domain-specific comprehension and generation dataset\\ncalled VL-Health. Experimental results demonstrate ex-\\nceptional performance and scalability of HealthGPT in\\nmedical visual unified tasks. Our project can be accessed at\\nhttps://github.com/DCDmllm/HealthGPT.\\n1 Introduction\\nLarge Vision-Language Models (LVLMs) (Liu et al. 2023;\\nOpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\\ndemonstrated outstanding open-world visual comprehension\\nand reasoning abilities through language-based interactive\\ndialogue over the past years, simultaneously opening up\\nnew opportunities for applications in specialized domains.\\n1\\narXiv:2502.09838v3  [cs.CV]  21 Feb 2025', path=None, url=None, mimetype=None),\n",
              " 'image_resource': None,\n",
              " 'audio_resource': None,\n",
              " 'video_resource': None,\n",
              " 'text_template': '{metadata_str}\\n\\n{content}'}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hide some keys from llm\n",
        "\n",
        "docs[0].__dict__ # too much data about one doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjOSD0tlPQ8G",
        "outputId": "b1c11e71-2107-464a-9a9a-86f4a207e589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The LLM sees this: \n",
            " file_name: super_secret_document.txt\n",
            "category: finance\n",
            "author: LlamaIndex\n",
            "\n",
            "This is a super-customized document\n",
            "The Embedding model sees this: \n",
            " file_name: super_secret_document.txt\n",
            "category: finance\n",
            "author: LlamaIndex\n",
            "\n",
            "This is a super-customized document\n"
          ]
        }
      ],
      "source": [
        "# quick example of what the LLM and Embeddings see when with a test document\n",
        "\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "document = Document(\n",
        "    text=\"This is a super-customized document\",\n",
        "    metadata={\n",
        "        \"file_name\": \"super_secret_document.txt\",\n",
        "        \"category\": \"finance\",\n",
        "        \"author\": \"LlamaIndex\",\n",
        "    },\n",
        "    # excluded_embed_metadata_keys=[\"file_name\"],\n",
        "    # excluded_llm_metadata_keys=[\"category\"],\n",
        "    # metadata_seperator=\"\\n\",\n",
        "    # metadata_template=\"{key}:{value}\",\n",
        "    # text_template=\"Metadata:\\n{metadata_str}\\n-----\\nContent:\\n{content}\",\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"The LLM sees this: \\n\",\n",
        "    document.get_content(metadata_mode=MetadataMode.LLM),\n",
        ")\n",
        "print(\n",
        "    \"The Embedding model sees this: \\n\",\n",
        "    document.get_content(metadata_mode=MetadataMode.EMBED),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0VfpjztPQ_H",
        "outputId": "bf0ce473-802c-4d45-fe7a-3f322a24e8e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The LLM sees this: \n",
            " Metadata:\n",
            "file_name:super_secret_document.txt\n",
            "author:LlamaIndex\n",
            "-----\n",
            "Content:\n",
            "This is a super-customized document\n",
            "The Embedding model sees this: \n",
            " Metadata:\n",
            "file_name:super_secret_document.txt\n",
            "category:finance\n",
            "author:LlamaIndex\n",
            "-----\n",
            "Content:\n",
            "This is a super-customized document\n"
          ]
        }
      ],
      "source": [
        "# quick example of what the LLM and Embeddings see when with a test document\n",
        "\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "document = Document(\n",
        "    text=\"This is a super-customized document\",\n",
        "    metadata={\n",
        "        \"file_name\": \"super_secret_document.txt\",\n",
        "        \"category\": \"finance\",\n",
        "        \"author\": \"LlamaIndex\",\n",
        "    },\n",
        "    # excluded_embed_metadata_keys=[\"file_name\"],\n",
        "    excluded_llm_metadata_keys=[\"category\"],\n",
        "    metadata_seperator=\"\\n\",\n",
        "    metadata_template=\"{key}:{value}\",\n",
        "    text_template=\"Metadata:\\n{metadata_str}\\n-----\\nContent:\\n{content}\",\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"The LLM sees this: \\n\",\n",
        "    document.get_content(metadata_mode=MetadataMode.LLM),\n",
        ")\n",
        "print(\n",
        "    \"The Embedding model sees this: \\n\",\n",
        "    document.get_content(metadata_mode=MetadataMode.EMBED),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukDymcYvTUhB",
        "outputId": "fa8cd309-04ee-44d9-b9be-df429636f8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_label: 1\n",
            "file_path: /content/data/2502.09838v3.pdf\n",
            "\n",
            "HealthGPT: A Medical Large Vision-Language Model for Unifying\n",
            "Comprehension and Generation via Heterogeneous Knowledge Adaptation\n",
            "Tianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\n",
            "Mengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\n",
            "1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\n",
            "4The Hong Kong University of Science and Technology,5National University of Singapore\n",
            "Project Page\n",
            " Code\n",
            "1. X-Ray \n",
            "Comprehension\n",
            "2. CT\n",
            "Comprehension\n",
            "3. MRI\n",
            "Comprehension\n",
            "Comp. Perf.\n",
            "7 Medical Multi-Modal Comprehension Tasks\n",
            "Gen. \n",
            "Performance\n",
            "5 Medical Multi-Modal Generation Tasks\n",
            "List all anatomical locations showing \n",
            "pulmonary edema, hazy opacity, or \n",
            "mediastinal displacement.\n",
            "Left hilar structures, left lung,\n",
            "right hilar structures, right lung.\n",
            "Which abdominal organ shows any \n",
            "indication of a lesion or abnormality \n",
            "in the CT image?\n",
            "No abdominal organs show\n",
            "any clear indications of lesions\n",
            "or abnormalities.\n",
            "Could you explain what this mass\n",
            "in the MRI means for my health? \n",
            "Is it very serious?\n",
            "Certainly, the MRI shows a\n",
            "defined mass in your left nasal\n",
            "cavity. There is no sign of the ...\n",
            "4. Microsopy Comprehension\n",
            " What is the purpose of the different \n",
            "membrane treatments used in this \n",
            "study?\n",
            "The purpose of the different\n",
            "membrane treatments used…\n",
            "5. OCT Comprehension\n",
            " What is the purpose of comparing \n",
            "the OCT structure image and OCTA \n",
            "image with H&E histology?\n",
            "To confirm the histological position\n",
            "of the obtained OCT brain images.\n",
            "6. Fundus\n",
            "Comprehension\n",
            " What specific findings or pathological \n",
            "changes can be observed in this \n",
            "fundus image?\n",
            "The fundus image appears normal with\n",
            "no noticeable signs of pathology…\n",
            "7. Ultrasound\n",
            "Comprehension\n",
            " What type of imaging technique \n",
            "is used in this image?\n",
            "The image is a sagittal gray-\n",
            "scale ultrasonographic…\n",
            "1. CT2MRI\n",
            "Generation\n",
            "I need a version of this CT representation \n",
            "in MRI.\n",
            "The image has\n",
            "been transformed\n",
            "into MRI.\n",
            "2. MRI2CT\n",
            "Generation\n",
            "Transform the MRI display into a \n",
            "CT image.\n",
            "Here is the CT\n",
            "version of the\n",
            "MRI image.\n",
            "3. Image Reconstruction\n",
            "Reconstruct the following \n",
            "medical images.\n",
            "Here is the reconstructed\n",
            "medical image you need.\n",
            "4. Super Resolution\n",
            "Could you improve the quality\n",
            "of this MRI image?\n",
            "Here is the image with\n",
            "improved resolution.\n",
            "5. Report-to-CXR\n",
            "The X-ray shows no \n",
            "pleural effusion or \n",
            "pneumothorax.\n",
            "Here is the\n",
            "chest X-ray\n",
            "image for\n",
            "you.\n",
            "Gen. Perf.\n",
            "Figure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\n",
            "unified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\n",
            "plex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\n",
            "Abstract\n",
            "We present HealthGPT, a powerful Medical Large Vision-\n",
            "Language Model (Med-LVLM) that integrates medical vi-\n",
            "sual comprehension and generation capabilities within a uni-\n",
            "fied autoregressive paradigm. Our bootstrapping philosophy\n",
            "is to progressively adapt heterogeneous comprehension and\n",
            "generation knowledge to pre-trained large language mod-\n",
            "els (LLMs). This is achieved through a novel heterogeneous\n",
            "low-rank adaptation (H-LoRA) technique, which is com-\n",
            "plemented by a tailored hierarchical visual perception ap-\n",
            "proach and a three-stage learning strategy. To effectively\n",
            "learn the HealthGPT, we devise a comprehensive medi-\n",
            "cal domain-specific comprehension and generation dataset\n",
            "called VL-Health. Experimental results demonstrate ex-\n",
            "ceptional performance and scalability of HealthGPT in\n",
            "medical visual unified tasks. Our project can be accessed at\n",
            "https://github.com/DCDmllm/HealthGPT.\n",
            "1 Introduction\n",
            "Large Vision-Language Models (LVLMs) (Liu et al. 2023;\n",
            "OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\n",
            "demonstrated outstanding open-world visual comprehension\n",
            "and reasoning abilities through language-based interactive\n",
            "dialogue over the past years, simultaneously opening up\n",
            "new opportunities for applications in specialized domains.\n",
            "1\n",
            "arXiv:2502.09838v3  [cs.CV]  21 Feb 2025\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.schema import MetadataMode\n",
        "\n",
        "# print(docs[0].get_content(metadata_mode=MetadataMode.LLM))   # what the llm sees\n",
        "print(docs[0].get_content(metadata_mode=MetadataMode.EMBED)) # what embeddings see. in this case, same thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pM3gBXe8TUj3"
      },
      "outputs": [],
      "source": [
        "for doc in docs:\n",
        "    # define the content/metadata template\n",
        "    doc.text_template = \"Metadata:\\n{metadata_str}\\n---\\nContent:\\n{content}\"\n",
        "\n",
        "    # exclude page label from embedding\n",
        "    if \"page_label\" not in doc.excluded_embed_metadata_keys:\n",
        "        doc.excluded_embed_metadata_keys.append(\"page_label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c57tSsfpTUmt",
        "outputId": "30df49bf-6864-4859-d1f2-4e8fd4504296"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata:\n",
            "file_path: /content/data/2502.09838v3.pdf\n",
            "---\n",
            "Content:\n",
            "HealthGPT: A Medical Large Vision-Language Model for Unifying\n",
            "Comprehension and Generation via Heterogeneous Knowledge Adaptation\n",
            "Tianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\n",
            "Mengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\n",
            "1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\n",
            "4The Hong Kong University of Science and Technology,5National University of Singapore\n",
            "Project Page\n",
            " Code\n",
            "1. X-Ray \n",
            "Comprehension\n",
            "2. CT\n",
            "Comprehension\n",
            "3. MRI\n",
            "Comprehension\n",
            "Comp. Perf.\n",
            "7 Medical Multi-Modal Comprehension Tasks\n",
            "Gen. \n",
            "Performance\n",
            "5 Medical Multi-Modal Generation Tasks\n",
            "List all anatomical locations showing \n",
            "pulmonary edema, hazy opacity, or \n",
            "mediastinal displacement.\n",
            "Left hilar structures, left lung,\n",
            "right hilar structures, right lung.\n",
            "Which abdominal organ shows any \n",
            "indication of a lesion or abnormality \n",
            "in the CT image?\n",
            "No abdominal organs show\n",
            "any clear indications of lesions\n",
            "or abnormalities.\n",
            "Could you explain what this mass\n",
            "in the MRI means for my health? \n",
            "Is it very serious?\n",
            "Certainly, the MRI shows a\n",
            "defined mass in your left nasal\n",
            "cavity. There is no sign of the ...\n",
            "4. Microsopy Comprehension\n",
            " What is the purpose of the different \n",
            "membrane treatments used in this \n",
            "study?\n",
            "The purpose of the different\n",
            "membrane treatments used…\n",
            "5. OCT Comprehension\n",
            " What is the purpose of comparing \n",
            "the OCT structure image and OCTA \n",
            "image with H&E histology?\n",
            "To confirm the histological position\n",
            "of the obtained OCT brain images.\n",
            "6. Fundus\n",
            "Comprehension\n",
            " What specific findings or pathological \n",
            "changes can be observed in this \n",
            "fundus image?\n",
            "The fundus image appears normal with\n",
            "no noticeable signs of pathology…\n",
            "7. Ultrasound\n",
            "Comprehension\n",
            " What type of imaging technique \n",
            "is used in this image?\n",
            "The image is a sagittal gray-\n",
            "scale ultrasonographic…\n",
            "1. CT2MRI\n",
            "Generation\n",
            "I need a version of this CT representation \n",
            "in MRI.\n",
            "The image has\n",
            "been transformed\n",
            "into MRI.\n",
            "2. MRI2CT\n",
            "Generation\n",
            "Transform the MRI display into a \n",
            "CT image.\n",
            "Here is the CT\n",
            "version of the\n",
            "MRI image.\n",
            "3. Image Reconstruction\n",
            "Reconstruct the following \n",
            "medical images.\n",
            "Here is the reconstructed\n",
            "medical image you need.\n",
            "4. Super Resolution\n",
            "Could you improve the quality\n",
            "of this MRI image?\n",
            "Here is the image with\n",
            "improved resolution.\n",
            "5. Report-to-CXR\n",
            "The X-ray shows no \n",
            "pleural effusion or \n",
            "pneumothorax.\n",
            "Here is the\n",
            "chest X-ray\n",
            "image for\n",
            "you.\n",
            "Gen. Perf.\n",
            "Figure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\n",
            "unified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\n",
            "plex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\n",
            "Abstract\n",
            "We present HealthGPT, a powerful Medical Large Vision-\n",
            "Language Model (Med-LVLM) that integrates medical vi-\n",
            "sual comprehension and generation capabilities within a uni-\n",
            "fied autoregressive paradigm. Our bootstrapping philosophy\n",
            "is to progressively adapt heterogeneous comprehension and\n",
            "generation knowledge to pre-trained large language mod-\n",
            "els (LLMs). This is achieved through a novel heterogeneous\n",
            "low-rank adaptation (H-LoRA) technique, which is com-\n",
            "plemented by a tailored hierarchical visual perception ap-\n",
            "proach and a three-stage learning strategy. To effectively\n",
            "learn the HealthGPT, we devise a comprehensive medi-\n",
            "cal domain-specific comprehension and generation dataset\n",
            "called VL-Health. Experimental results demonstrate ex-\n",
            "ceptional performance and scalability of HealthGPT in\n",
            "medical visual unified tasks. Our project can be accessed at\n",
            "https://github.com/DCDmllm/HealthGPT.\n",
            "1 Introduction\n",
            "Large Vision-Language Models (LVLMs) (Liu et al. 2023;\n",
            "OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\n",
            "demonstrated outstanding open-world visual comprehension\n",
            "and reasoning abilities through language-based interactive\n",
            "dialogue over the past years, simultaneously opening up\n",
            "new opportunities for applications in specialized domains.\n",
            "1\n",
            "arXiv:2502.09838v3  [cs.CV]  21 Feb 2025\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].get_content(metadata_mode=MetadataMode.EMBED))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9B4ZMJ2TUpR"
      },
      "outputs": [],
      "source": [
        "%pip install -Uq llama-index-llms-openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lncTp4leTUsH",
        "outputId": "7fb15f9b-a9e5-4c59-afa1-e259297ff469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "import os\n",
        "import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "K6n6eYz5TUu2"
      },
      "outputs": [],
      "source": [
        "llm_transformations = OpenAI(model=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "f2800b59d087429991dcbce631b5b368",
            "44588a93f7744c4297533d51b56faade",
            "a7f2685485b94f34959998b0bfed8a92",
            "f05e497a43a34c889b671d2b1cf13ce6",
            "7c4818679d764e1989bff99c0fb6ea81",
            "128566737cdb4a2f903f0e4e09d2ab9b",
            "8abbcaf169fc442cbe75c8e4a80cb9d7",
            "53375159ab5a4d49a0c8516222fcd996",
            "fcfc6f87549b4c91a09cc0b48ea29393",
            "45f58ec4d3dd425d93202e88eb2e01b6",
            "6b89590db5124cdf9081e8f279ad16d8"
          ]
        },
        "id": "1Bkha-xdTUxz",
        "outputId": "b2cffb71-5f30-46a1-8e9b-e6b7308df0b8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2800b59d087429991dcbce631b5b368",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:18<00:00,  1.71it/s]\n",
            "100%|██████████| 50/50 [01:05<00:00,  1.30s/it]\n"
          ]
        }
      ],
      "source": [
        "# other transformations\n",
        "\n",
        "from llama_index.core.extractors import (\n",
        "    TitleExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "text_splitter = SentenceSplitter(\n",
        "    separator=\" \", chunk_size=1024, chunk_overlap=128\n",
        ")\n",
        "title_extractor = TitleExtractor(llm=llm_transformations, nodes=5)\n",
        "qa_extractor = QuestionsAnsweredExtractor(llm=llm_transformations, questions=3)\n",
        "\n",
        "\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=[\n",
        "        text_splitter,\n",
        "        title_extractor,\n",
        "        qa_extractor\n",
        "    ]\n",
        ")\n",
        "\n",
        "nodes = pipeline.run(\n",
        "    documents=docs,\n",
        "    in_place=True,\n",
        "    show_progress=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDlCGdXkTU0k",
        "outputId": "2bf55dc9-d005-432b-f700-5aad2dca4047"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHTSo8qHTU3T",
        "outputId": "1b65ba62-79fa-41dd-da14-4cf0ba0b2374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TextNode(id_='a13bb56f-dedd-4a24-8a80-24203d73bffb', embedding=None, metadata={'page_label': '1', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"HealthGPT: A Unified Vision-Language Model for Enhanced Comprehension and Generation in Healthcare Applications\"', 'questions_this_excerpt_can_answer': 'Based on the provided context about HealthGPT, here are three specific questions that can be answered using the information in the excerpt:\\n\\n1. **What is the novel technique introduced in HealthGPT for adapting heterogeneous knowledge to pre-trained large language models?**\\n   - Answer: The novel technique introduced in HealthGPT for adapting heterogeneous knowledge is called heterogeneous low-rank adaptation (H-LoRA).\\n\\n2. **What types of medical imaging tasks does HealthGPT excel in, according to the experimental results?**\\n   - Answer: HealthGPT excels in various medical multi-modal comprehension tasks, such as X-Ray, CT, MRI, microscopy, OCT, fundus, and ultrasound comprehension, as well as generation tasks like CT2MRI, MRI2CT, image reconstruction, super resolution, and report-to-CXR.\\n\\n3. **What is the purpose of the VL-Health dataset mentioned in the context of HealthGPT?**\\n   - Answer: The VL-Health dataset is devised to effectively learn the HealthGPT model by providing a comprehensive medical domain-specific comprehension and generation dataset.\\n\\nThese questions focus on specific aspects of HealthGPT that are detailed in the excerpt and are unlikely to be found in other sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='487b9093-1ad3-4685-ab72-3428a9365856', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='afce7e3d8c0798d4ac08319b358679811e93662be69476f75dfdaf5712d628b2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='HealthGPT: A Medical Large Vision-Language Model for Unifying\\nComprehension and Generation via Heterogeneous Knowledge Adaptation\\nTianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\\nMengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\\n1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\\n4The Hong Kong University of Science and Technology,5National University of Singapore\\nProject Page\\n Code\\n1. X-Ray \\nComprehension\\n2. CT\\nComprehension\\n3. MRI\\nComprehension\\nComp. Perf.\\n7 Medical Multi-Modal Comprehension Tasks\\nGen. \\nPerformance\\n5 Medical Multi-Modal Generation Tasks\\nList all anatomical locations showing \\npulmonary edema, hazy opacity, or \\nmediastinal displacement.\\nLeft hilar structures, left lung,\\nright hilar structures, right lung.\\nWhich abdominal organ shows any \\nindication of a lesion or abnormality \\nin the CT image?\\nNo abdominal organs show\\nany clear indications of lesions\\nor abnormalities.\\nCould you explain what this mass\\nin the MRI means for my health? \\nIs it very serious?\\nCertainly, the MRI shows a\\ndefined mass in your left nasal\\ncavity. There is no sign of the ...\\n4. Microsopy Comprehension\\n What is the purpose of the different \\nmembrane treatments used in this \\nstudy?\\nThe purpose of the different\\nmembrane treatments used…\\n5. OCT Comprehension\\n What is the purpose of comparing \\nthe OCT structure image and OCTA \\nimage with H&E histology?\\nTo confirm the histological position\\nof the obtained OCT brain images.\\n6. Fundus\\nComprehension\\n What specific findings or pathological \\nchanges can be observed in this \\nfundus image?\\nThe fundus image appears normal with\\nno noticeable signs of pathology…\\n7. Ultrasound\\nComprehension\\n What type of imaging technique \\nis used in this image?\\nThe image is a sagittal gray-\\nscale ultrasonographic…\\n1. CT2MRI\\nGeneration\\nI need a version of this CT representation \\nin MRI.\\nThe image has\\nbeen transformed\\ninto MRI.\\n2. MRI2CT\\nGeneration\\nTransform the MRI display into a \\nCT image.\\nHere is the CT\\nversion of the\\nMRI image.\\n3. Image Reconstruction\\nReconstruct the following \\nmedical images.\\nHere is the reconstructed\\nmedical image you need.\\n4. Super Resolution\\nCould you improve the quality\\nof this MRI image?\\nHere is the image with\\nimproved resolution.\\n5. Report-to-CXR\\nThe X-ray shows no \\npleural effusion or \\npneumothorax.\\nHere is the\\nchest X-ray\\nimage for\\nyou.\\nGen. Perf.\\nFigure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\\nunified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\\nplex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\\nAbstract\\nWe present HealthGPT, a powerful Medical Large Vision-\\nLanguage Model (Med-LVLM) that integrates medical vi-\\nsual comprehension and generation capabilities within a uni-\\nfied autoregressive paradigm. Our bootstrapping philosophy\\nis to progressively adapt heterogeneous comprehension and\\ngeneration knowledge to pre-trained large language mod-\\nels (LLMs). This is achieved through a novel heterogeneous\\nlow-rank adaptation (H-LoRA) technique, which is com-\\nplemented by a tailored hierarchical visual perception ap-\\nproach and a three-stage learning strategy. To effectively\\nlearn the HealthGPT, we devise a comprehensive medi-\\ncal domain-specific comprehension and generation dataset\\ncalled VL-Health. Experimental results demonstrate ex-\\nceptional performance and scalability of HealthGPT in\\nmedical visual unified tasks. Our project can be accessed at\\nhttps://github.com/DCDmllm/HealthGPT.\\n1 Introduction\\nLarge Vision-Language Models (LVLMs) (Liu et al. 2023;\\nOpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\\ndemonstrated outstanding open-world visual comprehension\\nand reasoning abilities through language-based interactive\\ndialogue over the past years, simultaneously opening up\\nnew opportunities for applications in specialized domains.\\n1\\narXiv:2502.09838v3  [cs.CV]  21 Feb 2025', mimetype='text/plain', start_char_idx=0, end_char_idx=4119, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='028fb837-59e6-4562-a53d-f123fdaa579f', embedding=None, metadata={'page_label': '2', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Optimizing HealthGPT: Overcoming Data Limitations and Enhancing Task Adaptability through Heterogeneous Low-Rank Adaptation and Hierarchical Visual Perception\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What are the two major challenges faced when adapting unified large visual language models (LVLMs) to the medical domain?**\\n   - The excerpt identifies high-scale and quality data limitations and conflicts between comprehension and generation as the two major challenges.\\n\\n2. **How does the proposed Heterogeneous Low-Rank Adaptation (H-LoRA) approach address the issues of comprehension and generation in LVLMs?**\\n   - H-LoRA decouples the learning process for comprehension and generation tasks by storing knowledge in independent \"plug-ins,\" thus avoiding joint optimization issues.\\n\\n3. **What is the significance of using discrete visual tokens in the context of LVLMs, as mentioned in the excerpt?**\\n   - The use of discrete visual tokens enhances controllability and demonstrates early success in open-world, any-to-any tasks, indicating the potential of a unified autoregressive learning paradigm in multi-modal tasks.\\n\\nThese questions focus on specific details and concepts introduced in the excerpt, providing insights that are not readily available in other sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9582fe07-8d83-4ad4-a92e-e9058d05b328', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='d0cdacf8ee0ab919a97950abca10fa9fe7c4b3fe0fc8a07ee274730ac1ed4bcb'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='337ed99a-8782-4463-8207-d2fdf17da6bf', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bc0cc03adf94cf0ddb591626fbd484e90363a92c90aab733669abbe856f147e2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Specifically, recent studies (Li et al. 2024a; Tu et al. 2024)\\nhave utilized pre-trained large language models (LLMs) and\\nvisual instruction data to build interactive diagnostic tools\\nand treatment planning systems, revealing the immense po-\\ntential of LVLMs in medical scenarios. However, these stud-\\nies primarily concentrate on visual comprehension tasks that\\nproduce text-based outputs, such as medical visual ques-\\ntion answering (Li et al. 2024a) or report generation (Nath\\net al. 2024), and deficient the “drawing” capability needed\\nfor medical visual generation. In practice, integrating visual\\ncomprehension and generation can significantly enhance the\\nmultifunctionality of medical LVLMs.\\nRecent studies have increasingly focused on developing\\nunified LVLMs capable of comprehending and generating\\ncontent across diverse visual modalities. Earlier approaches\\npredominantly utilized continuous visual tokens fed into\\nLLMs, using the LLMs themselves as conditional genera-\\ntors for external generative models (Ge et al. 2024; Wu et al.\\n2023; Dong et al. 2023). More recent research has explored\\nthe use of discrete visual tokens for image representation and\\ngeneration within a fully autoregressive framework (Team\\n2024; Wang et al. 2024a; Xie et al. 2024). These meth-\\nods not only enhance controllability but also demonstrate\\nearly success in open-world, any-to-any tasks, highlighting\\nthe preliminary potential of a unified autoregressive learning\\nparadigm in multi-modal tasks.\\nWhile unified LVLMs have achieved initial success in\\ngeneral scenarios, such a unified framework remains under-\\nexplored in the medical domain. Adapting the aforemen-\\ntioned general unified model paradigm to the medical do-\\nmain presents two major challenges: (i) High-scale and\\n-quality Data Limitations . Open-world models necessi-\\ntate extensive pre-training on billions or even more diverse,\\nmulti-modal data samples for comprehension and genera-\\ntion tasks (Lu et al. 2024; Team 2024). However, the ac-\\ncessible medical data significantly lacks in scale and qual-\\nity compared to natural multi-modal datasets. Its special-\\nized and domain-specific characteristics make it challenging\\nto develop a unified medical model from scratch. (ii) Con-\\nflicts between Comprehension and Generation . Compre-\\nhension tasks often strip away visual details to focus on\\nabstraction, while generation tasks require detailed preser-\\nvation, making tokens sensitive to all visual alterations. As\\nshown in Figure 2, which features experiments conducted on\\nmedical images, the performance in comprehension (or gen-\\neration) tasks steadily decreases as the proportion of genera-\\ntion (or comprehension) data increases, and vice versa. This\\nhighlights a dilemma in autoregressive multi-modal training,\\nstemming from the need to maintain consistency between\\npre- and post-LVLMs. While some methods have explored\\nmutual enhancement between comprehension and genera-\\ntion (Pan et al. 2024; Tong et al. 2024), improvements still\\nexhibit diminishing returns, with performance degradation\\nremaining a significant issue.\\n(a) (b)\\nFigure 2: With a fixed amount of comprehension (genera-\\ntion) data, increasing the proportion of the other type leads\\nto significant performance degradation.\\nTo tackle the aforementioned challenges, we propose\\nHealthGPT (see Figure 1) , which progressively adapts a\\npre-trained LLM as an unified medical multi-modal model\\nwith a small amount of visual instruction data. We de-\\nvise innovative Parameter-Efficient Fine-Tuning (PEFT) ap-\\nproach (Ding et al. 2023), calledHeterogeneous Low-Rank\\nAdaptation (H-LoRA), which decouples the learning pro-\\ncess of LVLMs for comprehension and generation tasks. In-\\nspired by the plug-and-play nature of LoRA (Hu et al. 2021),\\nH-LoRA enables the model to store heterogeneous compre-\\nhension and generation knowledge in independent “plug-\\nins”, thus avoiding joint optimization issues caused by con-\\nflicts between comprehension and generation tasks. In addi-\\ntion, we also consider the variety of sub-tasks among com-\\nprehension or generation tasks. Qualitative research high-\\nlights the limitations of a single LoRA in handling multi-\\ndimensional task scenarios, mainly due to catastrophic for-\\ngetting and interference (Liu et al. 2024d; Lin et al. 2024).', mimetype='text/plain', start_char_idx=0, end_char_idx=4291, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='337ed99a-8782-4463-8207-d2fdf17da6bf', embedding=None, metadata={'page_label': '2', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Optimizing HealthGPT: Overcoming Data Limitations and Enhancing Task Adaptability through Heterogeneous Low-Rank Adaptation and Hierarchical Visual Perception\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt, here are three specific questions that can be answered using the context, along with higher-level summaries to enhance understanding:\\n\\n1. **What is the primary advantage of using H-LoRA over traditional LoRA in handling multi-dimensional task scenarios?**\\n   - *Summary: The excerpt discusses the limitations of a single LoRA in managing various comprehension and generation tasks due to issues like catastrophic forgetting and interference. H-LoRA addresses these challenges by enabling the model to store knowledge in independent \"plugins\" and introducing LoRA experts for dynamic task adaptation.*\\n\\n2. **How does H-LoRA reduce the training time compared to MoELoRA when using multiple experts?**\\n   - *Summary: The text highlights that H-LoRA employs reversible matrix block multiplication to combine LoRA experts, which significantly cuts down the overhead associated with multiple matrix multiplications. Specifically, it mentions that using four experts requires only 67% of the training time compared to MoELoRA.*\\n\\n3. **What role does Hierarchical Visual Perception (HVP) play in the implementation of H-LoRA within HealthGPT?**\\n   - *Summary: The excerpt explains that HVP separates the learning of visual details from the Vision Transformer (ViT) for both comprehension and generation tasks. It emphasizes the importance of maintaining visual features from different layers of the ViT to meet the varying requirements for visual granularity in these tasks.*'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9582fe07-8d83-4ad4-a92e-e9058d05b328', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='d0cdacf8ee0ab919a97950abca10fa9fe7c4b3fe0fc8a07ee274730ac1ed4bcb'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='028fb837-59e6-4562-a53d-f123fdaa579f', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '2', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='1ab5fe5892d6701cadb74568bd914b50762b18e42641ad8a4269cd8242286cd5')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2021),\\nH-LoRA enables the model to store heterogeneous compre-\\nhension and generation knowledge in independent “plug-\\nins”, thus avoiding joint optimization issues caused by con-\\nflicts between comprehension and generation tasks. In addi-\\ntion, we also consider the variety of sub-tasks among com-\\nprehension or generation tasks. Qualitative research high-\\nlights the limitations of a single LoRA in handling multi-\\ndimensional task scenarios, mainly due to catastrophic for-\\ngetting and interference (Liu et al. 2024d; Lin et al. 2024).\\nTo address this, we draw on the concept of Mixture of Ex-\\nperts (MoE) (Masoudnia and Ebrahimpour 2014) and in-\\ntroduce LoRA experts. The aim is to dynamically transfer\\ntask-shared knowledge to adapt to downstream tasks. Unlike\\nMoELoRA (Luo et al. 2024a), H-LoRA employs reversible\\nmatrix block multiplication to combine LoRA experts, sig-\\nnificantly reducing the overhead of multiple matrix multi-\\nplications. Notably, when using four experts, it requires\\nonly 67% of the MoELoRA training time.\\nTo effectively leverage H-LoRA inHealthGPT, we fur-\\nther introduce a Hierarchical Visual Perception (HVP)\\nand devise a correspondingThree-stage Learning Strategy\\n(TLS). HVP: we separate visual details learning from Vi-\\nsion transformer (ViT) for comprehension and generation.\\nAs is widely recognized, the ViT encodes visual concepts\\nwith increasing abstraction, generally, becoming finer as we\\nprogress over levels (Vig 2019). Thus, we maintain the vi-\\nsual features of the anterior and posterior layers to accom-\\nmodate the differing requirements for visual granularity in\\ncomprehension and generation tasks while preventing po-\\n2', mimetype='text/plain', start_char_idx=3754, end_char_idx=5418, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='4cdf94e4-fe14-4f74-aa66-eb17627ecab4', embedding=None, metadata={'page_label': '3', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"HealthGPT: Advancements in Unified Medical Vision-Language Comprehension and Generation through H-LoRA and the VL-Health Dataset\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document on HealthGPT and its advancements in medical vision-language comprehension and generation, here are three specific questions that can be answered using the context:\\n\\n1. **What are the main contributions of the HealthGPT framework as outlined in the document?**\\n   - This question targets the specific contributions mentioned in the excerpt, such as the introduction of the unified Med-LVLM, the effective learning paradigm (H-LoRA), the holistic training dataset (VL-Health), and the demonstrated downstream improvements.\\n\\n2. **How does the H-LoRA architecture address data conflict issues in training HealthGPT?**\\n   - This question focuses on the specific design and purpose of the H-LoRA architecture, which is described as an optimized multi-LoRA PEFT architecture based on task-gated decoupling, aimed at mitigating data conflict issues during training.\\n\\n3. **What types of tasks are included in the VL-Health dataset curated for training unified medical LVLMs?**\\n   - This question seeks to identify the specific tasks included in the VL-Health dataset, which comprises seven comprehension tasks and five generation tasks, as mentioned in the excerpt.\\n\\nThese questions are tailored to extract detailed information that is specific to the content of the document and may not be readily available in other sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='009972f7-4fc4-436e-82c5-cbb05e68c6fa', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='481d8a7657352c6809354a39c6ed7fc63367682dcbccc5ca5c026af40adb0f46'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d9e07243-7496-49b3-bf04-b0d4e7715e64', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='993e9d88be405aba644a12a9ee95491469de247182abe6be525ad536fa38cbe3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='tential task interference. TLS: In the first and second stages,\\ngiven the heterogeneity between comprehension and gener-\\nation tasks, we first train H-LoRA plugins for HealthGPT\\nto incorporate both medical comprehension and generation\\nknowledge, thus endowing the LLMs with capabilities for\\nvision-language alignment and vision-to-vision reconstruc-\\ntion. Additionally, through minimal mixed-task training, we\\nbuilt fusion embedding layers and output heads that merge\\ntext and visual tokens, establishing a unified LVLM founda-\\ntion for visual instruction fine-tuning. In the third stage, by\\nonly training the H-LoRA plugins, HealthGPT is able to\\nrapidly adapt to a wide range of downstream medical tasks,\\ncovering various types of medical comprehension and gen-\\neration tasks.\\nTo effectively implement our approach, we have cu-\\nrated a dataset for training unified medical LVLMs, called\\nVL-Health, including seven comprehension tasks and five\\ngeneration tasks (Figure 1). Through quantitative analysis\\nand validation on multi-modal tasks, the results demonstrate\\nthat HealthGPT is capable of unifying medical multi-\\nmodal abilities in data-constrained scenarios, achieving per-\\nformance comparable to or better than existing state-of-the-\\nart (SOTA) models across multiple metrics. Overall, the\\nmain contributions of this paper are summarized as follows:\\n• Unified Med-LVLM. We introduce HealthGPT,\\nwhich, to the best of our knowledge, is the first unified\\nframework for multi-modal comprehension and genera-\\ntion in complex medical scenarios.\\n• Effective Learning Paradigm. We present H-LoRA, an\\noptimized multi-LoRA PEFT architecture based on task-\\ngated decoupling, is designed to effectively mitigate data\\nconflict issues.\\n• Holistic Training Dataset. We curated VL-Health, a\\ncomprehensive dataset designed for both comprehension\\nand generation tasks.\\n• Superior Downstream Improvements : Extensive ex-\\nperiments are conducted and the results confirm\\nHealthGPT’s effectiveness in medical vision-language\\ncomprehension and generation.\\n2 Related Work\\nMedical Vision Large Language Models. Recently, medi-\\ncal vision large language models (Med-VLLMs) have made\\nsignificant progress, demonstrating excellent performance\\nin understanding medical images and responding to human\\nqueries based on these images (Zhou et al. 2023; Tian et al.\\n2023). XrayGPT (Thawkar et al. 2023) combines a med-\\nical visual encoder (MedClip) (Wang et al. 2022) with a\\nfine-tuned LLM , using a simple linear transformation layer\\nto achieve alignment between visual and textual informa-\\ntion, significantly enhancing the understanding of medical\\nimages. On this basis, LLaV A-Med (Li et al. 2024b) fur-\\nther enhances visual-text alignment in medical contexts by\\nselecting high-quality image-text pairs from PubMed pa-\\npers and synthesized VQA datasets. BiomedGPT (Luo et al.\\n2024b) employs a BERT-style encoder and GPT-style de-\\ncoder architecture, pre-trained on interdisciplinary datasets.\\nCompared to commercial models like Med-PaLM (Singhal\\net al. 2023), BiomedGPT significantly reduces model size\\nwhile maintaining superior performance. However, issues\\nof language adaptability and dataset specificity still remain.\\nTo address these, HuatuoGPT-Vision (Chen et al. 2024a)\\nintroduces the PubMedVision dataset, which contains 1.3\\nmillion high-quality medical samples, significantly improv-\\ning the model’s adaptability across diverse medical applica-\\ntions. However, current Med-VLLMs mainly focus on med-\\nical comprehension and lack the capability for the medical\\nvision-language generation.\\nUnified Visual Comprehension and Generation Mod-\\nels. Recent research has increasingly concentrated on cre-\\nating unified LVLMs that are adept at understanding and\\nproducing content across various visual modalities. NExT-\\nGPT (Wu et al. 2023) achieves perception and generation for\\narbitrary combinations of multi-modal inputs and outputs by\\naligning LLMs. Similarly, SEED (Ge et al. 2023), SEED-\\nX (Ge et al. 2024), and DreamLLM (Dong et al. 2023) em-\\nploy learnable queries and leverage next-token prediction to\\ngenerate visual tokens, providing conditional inputs to exter-\\nnal generation modules. Unlike these methods, which func-\\ntion as external conditioners, Unified-IO (Lu et al. 2022),\\nUnified-IO 2 (Lu et al.', mimetype='text/plain', start_char_idx=0, end_char_idx=4290, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='d9e07243-7496-49b3-bf04-b0d4e7715e64', embedding=None, metadata={'page_label': '3', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"HealthGPT: Advancements in Unified Medical Vision-Language Comprehension and Generation through H-LoRA and the VL-Health Dataset\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"HealthGPT: Advancements in Unified Medical Vision-Language Comprehension and Generation through H-LoRA and the VL-Health Dataset,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are the key differences between the methods employed by SEED, SEED-X, and DreamLLM compared to Unified-IO and its successors?**\\n   - This question targets the distinctions in methodology and architecture between the various models mentioned, specifically focusing on how they handle multi-modal generation tasks.\\n\\n2. **How do Large Vision-Language Models (LVLMs) process input images and text sequences to generate responses?**\\n   - This question seeks to understand the technical process of how LVLMs convert images and text into token sequences and how they generate responses based on these inputs.\\n\\n3. **What is the optimization objective for Large Vision-Language Models as described in the excerpt?**\\n   - This question focuses on the specific goal of LVLMs in terms of their training and performance metrics, particularly regarding the cross-entropy loss of the generated responses.\\n\\nThese questions are tailored to extract detailed information from the excerpt that may not be readily available in other sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='009972f7-4fc4-436e-82c5-cbb05e68c6fa', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='481d8a7657352c6809354a39c6ed7fc63367682dcbccc5ca5c026af40adb0f46'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4cdf94e4-fe14-4f74-aa66-eb17627ecab4', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '3', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='e7181fed2aa9952211dcf547447b2649449b9798efcc3143d71714871fe64044')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Similarly, SEED (Ge et al. 2023), SEED-\\nX (Ge et al. 2024), and DreamLLM (Dong et al. 2023) em-\\nploy learnable queries and leverage next-token prediction to\\ngenerate visual tokens, providing conditional inputs to exter-\\nnal generation modules. Unlike these methods, which func-\\ntion as external conditioners, Unified-IO (Lu et al. 2022),\\nUnified-IO 2 (Lu et al. 2024), and Chameleon (Team 2024)\\ninternalize multi-modal generation tasks within a unified\\nTransformer architecture by extending multi-modal vocab-\\nularies, enabling direct generation based on next-token pre-\\ndiction. Building on this concept, Lumina-mGPT (Liu et al.\\n2024a) and ANOLE (Chern et al. 2024) further enhance the\\ngeneration capabilities of unified models using high-quality\\ndata, particularly improving the quality and flexibility of im-\\nage generation.\\n3 Preliminaries\\nLarge Vision-Language Models.The input to a LVLM typ-\\nically consists of an image ximg and a discrete text sequence\\nxtxt. The visual encoder Eimg converts the input image ximg\\ninto a sequence of visual tokens V = [ vi]Nv\\ni=1, while the\\ntext sequence xtxt is mapped into a sequence of text to-\\nkens T = [ ti]Nt\\ni=1 using an embedding function Etxt. The\\nLLM MLLM(·|θ) models the joint probability of the token\\nsequence U = {V, T }, which is expressed as:\\nPθ(R|U) =\\nNrY\\ni=1\\nPθ(ri|{U, r<i}), (1)\\nwhere R = [ri]Nr\\ni=1 is the text response sequence. The LVLM\\niteratively generates the next token ri based on r<i. The op-\\ntimization objective is to minimize the cross-entropy loss of\\nthe response R. It is worth noting that most LVLMs adopt\\n3', mimetype='text/plain', start_char_idx=3929, end_char_idx=5508, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='667e2e57-2c5c-404e-a1de-97a9464508f6', embedding=None, metadata={'page_label': '4', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"HealthGPT: A Unified Framework for Autoregressive Generation of Text and Images through Hierarchical Visual Perception and Low-Rank Adaptation\"', 'questions_this_excerpt_can_answer': \"Based on the provided excerpt from the document on HealthGPT, here are three specific questions that can be answered using the context, along with brief explanations of their relevance:\\n\\n1. **What is the role of VQGAN in the HealthGPT architecture, and how does it contribute to the generation of images?**\\n   - This question focuses on the specific function of VQGAN within the HealthGPT framework, detailing its process of mapping input images to latent representations and how it utilizes quantized index sequences for image generation.\\n\\n2. **How does Low-Rank Adaptation (LoRA) enhance the performance of HealthGPT in downstream tasks?**\\n   - This question seeks to understand the mechanism of LoRA in the context of HealthGPT, particularly how it reduces the number of learnable parameters while maintaining the model's ability to adapt to various tasks.\\n\\n3. **What distinguishes the comprehension and generation tasks in HealthGPT's hierarchical visual perception approach?**\\n   - This question addresses the differences in visual perception strategies employed by HealthGPT for comprehension versus generation, highlighting the use of ViT to process images at multiple hierarchical levels and the implications for semantic understanding.\\n\\nThese questions are tailored to extract detailed insights from the excerpt that are specific to the HealthGPT framework and its components, which may not be readily available in other sources.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9d24e4e3-4794-421f-87d8-932b3548a4a9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='cc3a09baf3d6cdd410ab99958bdc1e6af97abc52b697440662a85a9d57195e27')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 3: The HealthGPT architecture integrates hierarchical visual perception and H-LoRA, employing a task-specific hard\\nrouter to select visual features and H-LoRA plugins, ultimately generating outputs with an autoregressive manner.\\na design paradigm based on ViT, alignment adapters, and\\npre-trained LLMs(Liu et al. 2023, 2024b), enabling quick\\nadaptation to downstream tasks.\\nVQGAN. VQGAN (Esser, Rombach, and Ommer 2021)\\nemploys latent space compression and indexing mechanisms\\nto effectively learn a complete discrete representation of im-\\nages. VQGAN first maps the input imageximg to a latent rep-\\nresentation z = E(x) through a encoder E. Then, the latent\\nrepresentation is quantized using a codebookZ = {zk}K\\nk=1,\\ngenerating a discrete index sequence I = [im]N\\nm=1, where\\nim ∈ Zrepresents the quantized code index:\\nI = Quantize(z|Z) = arg min\\nzk∈Z\\n∥z − zk∥2. (2)\\nIn our approach, the discrete index sequence I serves as\\na supervisory signal for the generation task, enabling the\\nmodel to predict the index sequence ˆI from input conditions\\nsuch as text or other modality signals. Finally, the predicted\\nindex sequence ˆI is upsampled by the VQGAN decoder G,\\ngenerating the high-quality image ˆximg = G(ˆI).\\nLow Rank Adaptation. LoRA(Hu et al. 2021) effectively\\ncaptures the characteristics of downstream tasks by intro-\\nducing low-rank adapters. The core idea is to decompose\\nthe bypass weight matrix ∆W ∈ Rdin×dout\\ninto two low-\\nrank matrices {A ∈ Rdin×r, B ∈ Rr×dout\\n}, where r ≪\\nmin{din, dout}, significantly reducing learnable parameters.\\nThe output with the LoRA adapter for the input x is then\\ngiven by:\\nh = xW0 + αx∆W/r = xW0 + αxAB/r, (3)\\nwhere matrix A is initialized with a Gaussian distribution,\\nwhile the matrixB is initialized as a zero matrix. The scaling\\nfactor α/r controls the impact of ∆W on the model.\\n4 HealthGPT\\n4.1 Unified Autoregressive Generation.\\nHealthGPT (Figure 3) utilizes a discrete token representa-\\ntion that covers both text and visual outputs, unifying visual\\ncomprehension and generation as an autoregressive task. For\\ncomprehension, Mllm receives the input joint sequence U\\nand outputs a series of text token R = [ r1, r2, . . . , rNr ],\\nwhere ri ∈ Vtxt, and Vtxt represents the LLM’s vocabulary:\\nPθ(R | U) =\\nNrY\\ni=1\\nPθ(ri | U, r<i). (4)\\nFor generation, Mllm first receives a special start token\\n⟨START IMG⟩, then generates a series of tokens corre-\\nsponding to the VQGAN indices I = [ i1, i2, . . . , iNi ],\\nwhere ij ∈ Vvq, and Vvq represents the index range of VQ-\\nGAN. Upon completion of generation, the LLM outputs an\\nend token ⟨END IMG⟩:\\nPθ(I | U) =\\nNiY\\nj=1\\nPθ(ij | U, i<j). (5)\\nFinally, the generated index sequence I is fed into the de-\\ncoder G, which reconstructs the target image ˆximg = G(I).\\n4.2 Hierarchical Visual Perception\\nGiven the differences in visual perception between compre-\\nhension and generation tasks—where the former focuses on\\nabstract semantics and the latter emphasizes complete se-\\nmantics—we employ ViT to compress the image into dis-\\ncrete visual tokens at multiple hierarchical levels. Specif-\\nically, the image is converted into a series of features\\n{f1, f2, . . . , fL} as it passes through L ViT blocks.\\n4', mimetype='text/plain', start_char_idx=0, end_char_idx=3188, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='9121c0fe-7dab-4813-a053-5f0c944d85fb', embedding=None, metadata={'page_label': '5', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Adaptive Multi-Modal Learning: Dynamic Feature Selection and Fine-Tuning Strategies for Enhanced Medical Image Reconstruction and Visual Comprehension\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What are the two types of hidden states defined in the context of the adaptive multi-modal learning framework, and how do they differ in terms of their suitability for different tasks?**\\n   - This question targets the specific definitions and characteristics of Concrete-grained features and Abstract-grained features as described in the excerpt.\\n\\n2. **How does the H-LoRA mechanism integrate the concepts of Mixture of Experts (MoE) and what is the significance of the matrix merging and routing weight allocation in this context?**\\n   - This question focuses on the technical details of the H-LoRA mechanism, particularly its innovative approach to integrating MoE and the implications of its design choices.\\n\\n3. **What are the key stages in the training pipeline for the adaptive multi-modal learning model, and how do they differ in terms of the tasks they address?**\\n   - This question seeks to clarify the two distinct stages of the training pipeline (Multi-modal Alignment and Heterogeneous H-LoRA Plugin Adaptation) and their specific objectives related to medical comprehension and generation tasks.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b8f18ed1-d6d6-45a9-b396-528ddbe7acea', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='209681d49a86169e8f8f51274df8e2ce8c8fb3f7965c969c29cb1f87a873d119'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d9c76f69-16b1-4e00-8464-3726b15474f0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3103c6d1dddda66e534af5a9006f04354b2d696bc9d84e3855f140f966b84712')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='To address the needs of various tasks, the hidden states\\nare divided into two types: (i) Concrete-grained features\\nFCon = {f1, f2, . . . , fk}, k < L, derived from the shal-\\nlower layers of ViT, containing sufficient global features,\\nsuitable for generation tasks; (ii) Abstract-grained features\\nFAbs = {fk+1, fk+2, . . . , fL}, derived from the deeper\\nlayers of ViT, which contain abstract semantic information\\ncloser to the text space, suitable for comprehension tasks.\\nThe task type T (comprehension or generation) deter-\\nmines which set of features is selected as the input for the\\ndownstream large language model:\\nFimg\\nT =\\n(\\nFCon, if T = generation task\\nFAbs, if T = comprehension task (6)\\nWe integrate the image featuresFimg\\nT and text featuresT into\\na joint sequence through simple concatenation, which is then\\nfed into the LLM Mllm for autoregressive generation.\\n4.3 Heterogeneous Knowledge Adaptation\\nWe devise H-LoRA, which stores heterogeneous knowledge\\nfrom comprehension and generation tasks in separate mod-\\nules and dynamically routes to extract task-relevant knowl-\\nedge from these modules. At the task level, for each task type\\nT, we dynamically assign a dedicated H-LoRA submodule\\nθT , which is expressed as:\\nR = MLLM(U|θ, θT ), θ T = {AT , BT , RT\\nouter}. (7)\\nAt the feature level for a single task, H-LoRA integrates the\\nidea of Mixture of Experts (MoE) (Masoudnia and Ebrahim-\\npour 2014) and designs an efficient matrix merging and rout-\\ning weight allocation mechanism, thus avoiding the signif-\\nicant computational delay introduced by matrix splitting in\\nexisting MoELoRA (Luo et al. 2024a). Specifically, we first\\nmerge the low-rank matrices (rank = r) of k LoRA experts\\ninto a unified matrix:\\nAmerged, Bmerged = Concat({Ai}k\\n1 ), Concat({Bi}k\\n1 ), (8)\\nwhere Amerged ∈ Rdin×rk and Bmerged ∈ Rrk×dout\\n. The\\nk-dimension routing layer generates expert weights W ∈\\nRtoken num×k based on the input hidden state x, and these are\\nexpanded to Rtoken num×rk as follows:\\nWexpanded = αkW/r ⊗ 1r, (9)\\nwhere ⊗ denotes the replication operation. The overall out-\\nput of H-LoRA is computed as:\\nOH-LoRA = (xAmerged ⊙ Wexpanded)Bmerged, (10)\\nwhere ⊙ represents element-wise multiplication. Finally, the\\noutput of H-LoRA is added to the frozen pre-trained weights\\nto produce the final output:\\nO = xW0 + OH-LoRA. (11)\\n900\\n800\\n700\\n600\\n500\\n400\\n300\\n200\\n100\\n0\\nComp. Gen.\\n(a) (b)\\n783K765K\\n（K）\\nFigure 4: Data statistics of VL-Health.\\n4.4 Training Pipeline\\n1st Stage: Multi-modal Alignment. In the first stage, we\\ndesign separate visual adapters and H-LoRA submodules for\\nmedical unified tasks. For the medical comprehension task,\\nwe train abstract-grained visual adapters using high-quality\\nimage-text pairs to align visual embeddings with textual\\nembeddings, thereby enabling the model to accurately de-\\nscribe medical visual content. During this process, the pre-\\ntrained LLM and its corresponding H-LoRA submodules\\nremain frozen. In contrast, the medical generation task re-\\nquires training concrete-grained adapters and H-LoRA sub-\\nmodules while keeping the LLM frozen. Meanwhile, we ex-\\ntend the textual vocabulary to include multimodal tokens,\\nenabling the support of additional VQGAN vector quanti-\\nzation indices. The model trains on image-VQ pairs, en-\\ndowing the pre-trained LLM with the capability for image\\nreconstruction. This design ensures pixel-level consistency\\nof pre- and post-LVLM. The processes establish the initial\\nalignment between the LLM’s outputs and the visual inputs.\\n2nd Stage: Heterogeneous H-LoRA Plugin Adaptation.\\nThe submodules of H-LoRA share the word embedding\\nlayer and output head but may encounter issues such as\\nbias and scale inconsistencies during training across dif-\\nferent tasks.', mimetype='text/plain', start_char_idx=0, end_char_idx=3724, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='d9c76f69-16b1-4e00-8464-3726b15474f0', embedding=None, metadata={'page_label': '5', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Adaptive Multi-Modal Learning: Dynamic Feature Selection and Fine-Tuning Strategies for Enhanced Medical Image Reconstruction and Visual Comprehension\"', 'questions_this_excerpt_can_answer': \"Based on the provided excerpt, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What is the purpose of training the model on image-VQ pairs in the context of the adaptive multi-modal learning framework?**\\n   - This question targets the specific role of image-VQ pairs in ensuring pixel-level consistency between the pre-trained LLM and visual inputs, as mentioned in the excerpt.\\n\\n2. **How does the H-LoRA plugin adaptation process address issues of bias and scale inconsistencies during training?**\\n   - This question focuses on the strategies employed during the second stage of the model's training, particularly how the word embedding layer and output head are fine-tuned while keeping other submodules frozen.\\n\\n3. **What types of downstream tasks are enhanced through the visual instruction fine-tuning stage of the model?**\\n   - This question seeks to identify the specific applications, such as medical QA and report generation, that benefit from the additional task-specific data introduced in the third stage of the training process.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b8f18ed1-d6d6-45a9-b396-528ddbe7acea', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='209681d49a86169e8f8f51274df8e2ce8c8fb3f7965c969c29cb1f87a873d119'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='9121c0fe-7dab-4813-a053-5f0c944d85fb', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '5', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='36680544f8ad4c23393e92688cf3144f18812081fb3147f7b46c3cbec91428b9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The model trains on image-VQ pairs, en-\\ndowing the pre-trained LLM with the capability for image\\nreconstruction. This design ensures pixel-level consistency\\nof pre- and post-LVLM. The processes establish the initial\\nalignment between the LLM’s outputs and the visual inputs.\\n2nd Stage: Heterogeneous H-LoRA Plugin Adaptation.\\nThe submodules of H-LoRA share the word embedding\\nlayer and output head but may encounter issues such as\\nbias and scale inconsistencies during training across dif-\\nferent tasks. To ensure that the multiple H-LoRA plugins\\nseamlessly interface with the LLMs and form a unified base,\\nwe fine-tune the word embedding layer and output head us-\\ning a small amount of mixed data to maintain consistency\\nin the model weights. Specifically, during this stage, all H-\\nLoRA submodules for different tasks are kept frozen, with\\nonly the word embedding layer and output head being op-\\ntimized. Through this stage, the model accumulates foun-\\ndational knowledge for unified tasks by adapting H-LoRA\\nplugins.\\n3rd Stage: Visual Instruction Fine-Tuning. In the third\\nstage, we introduce additional task-specific data to fur-\\nther optimize the model and enhance its adaptability to\\ndownstream tasks such as medical visual comprehension\\n(e.g., medical QA, medical dialogues, and report generation)\\nor generation tasks (e.g., super-resolution, denoising, and\\n5', mimetype='text/plain', start_char_idx=3221, end_char_idx=4587, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='e4278e05-0df6-4c34-9579-fb3769d9e7eb', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comprehensive Evaluation of HealthGPT and Advanced Vision-Language Models in Medical Visual Comprehension and Modality Conversion Tasks\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt, here are three specific questions that can be answered using the information in the context:\\n\\n1. **Which model achieved the highest performance in the medical visual comprehension tasks according to Table 1, and what were its scores across the various metrics?**\\n   - This question targets the specific performance metrics of the top-performing model, HealthGPT-L14, as detailed in the table.\\n\\n2. **How does the performance of HealthGPT-M3 compare to other models in terms of the average score across the medical visual comprehension tasks?**\\n   - This question focuses on the comparative performance of HealthGPT-M3, allowing for an analysis of its average score relative to other models listed in the table.\\n\\n3. **What are the key differences in the number of parameters and performance metrics between the models HealthGPT-L14 and Llama-3.2?**\\n   - This question seeks to extract specific details about the two models, including their parameter counts and performance scores, highlighting the distinctions between them as presented in the table.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6d5a0570-973b-42f7-806f-dd3a7431788c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='41ef80bdf8bdf6da173d984639ed42defada78da75bf42b13d19e58704bcf6f0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='98215f8a-0f6f-4515-8c91-432fa232f3bc', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='83ab41811009bd1c39575779886063ac228c262009e4cf9abdbe9158f3875a7b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 1: Comparison of HealthGPT with other LVLMs and unified multi-modal models on medical visual comprehension\\ntasks. Bold and underlined text indicates the best performance and second-best performance, respectively.\\nVQA-RAD↑ SLAKE↑ PathVQA↑Type Model # Params Medical\\nLVLM close all close all close all\\nMMMU\\n-Med ↑ OMVQA↑ Avg.↑\\nComp. Only\\nMed-Flamingo 8.3B ✓ 58.6 43.0 47.0 25.5 61.9 31.3 28.7 34.9 41.4\\nLLaV A-Med 7B ✓ 60.2 48.1 58.4 44.8 62.3 35.7 30.0 41.3 47.6\\nHuatuoGPT-Vision 7B ✓ 66.9 53.0 59.8 49.1 52.9 32.0 42.0 50.0 50.7\\nBLIP-2 6.7B ✗ 43.4 36.8 41.6 35.3 48.5 28.8 27.3 26.9 36.1\\nLLaV A-v1.5 7B ✗ 51.8 42.8 37.1 37.7 53.5 31.4 32.7 44.7 41.5\\nInstructBLIP 7B ✗ 61.0 44.8 66.8 43.3 56.0 32.3 25.3 29.0 44.8\\nYi-VL 6B ✗ 52.6 42.1 52.4 38.4 54.9 30.9 38.0 50.2 44.9\\nInternVL2 8B ✗ 64.9 49.0 66.6 50.1 60.0 31.9 43.3 54.5 52.5\\nLlama-3.2 11B ✗ 68.9 45.5 72.4 52.1 62.8 33.6 39.3 63.2 54.7\\nComp. & Gen.\\nShow-o 1.3B ✗ 50.6 33.9 31.5 17.9 52.9 28.2 22.7 45.7 42.6\\nUnified-IO 2 7B ✗ 46.2 32.6 35.9 21.9 52.5 27.0 25.3 33.0 33.8\\nJanus 1.3B ✗ 70.9 52.8 34.7 26.9 51.9 27.9 30.0 26.8 33.5\\nHealthGPT-M3 3.8B ✓ 73.7 55.9 74.6 56.4 78.7 39.7 43.3 68.5 61.3\\nHealthGPT-L14 14B ✓ 77.7 58.3 76.4 64.5 85.9 44.4 49.2 74.4 66.4\\nTable 2: The experimental results for the four modality conversion tasks.', mimetype='text/plain', start_char_idx=0, end_char_idx=1292, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='98215f8a-0f6f-4515-8c91-432fa232f3bc', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comprehensive Evaluation of HealthGPT and Advanced Vision-Language Models in Medical Visual Comprehension and Modality Conversion Tasks\"', 'questions_this_excerpt_can_answer': \"Based on the provided excerpt, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What are the performance metrics (SSIM, PSNR, MSE) for HealthGPT-M3 and HealthGPT-L14 in the modality conversion tasks from CT to MRI for both brain and pelvis scans?**\\n   - This question targets the specific performance metrics of the HealthGPT models in a particular task, which is detailed in the table within the excerpt.\\n\\n2. **What datasets were utilized in the experiments for medical visual comprehension and modality conversion tasks in the study?**\\n   - This question seeks to identify the specific datasets mentioned in the context that were used for training and evaluation, which is crucial for understanding the experimental setup.\\n\\n3. **What model architecture and components were selected for the HealthGPT models, and how do they contribute to the model's adaptability across different tasks?**\\n   - This question focuses on the architectural choices and their implications for the model's performance, as described in the section about model details and the training strategy.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6d5a0570-973b-42f7-806f-dd3a7431788c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='41ef80bdf8bdf6da173d984639ed42defada78da75bf42b13d19e58704bcf6f0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e4278e05-0df6-4c34-9579-fb3769d9e7eb', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='232719ef691a12285c1222e0448ea1a9f6e9182c4c75e0b14a8c0e64dc2312ba'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='fcb4ec01-76d3-4cd6-ac08-38050e91056f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5111a362741ae96ee823a86637f1b0f39d8c50ecfd49992cfd7431625b5fd7d2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='CT to MRI (Brain) CT to MRI (Pelvis) MRI to CT (Brain) MRI to CT (Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\npix2pix 71.09 32.65 36.85 59.17 31.02 51.91 78.79 33.85 28.33 72.31 32.98 36.19\\nCycleGAN 54.76 32.23 40.56 54.54 30.77 55.00 63.75 31.02 52.78 50.54 29.89 67.78\\nBBDM 71.69 32.91 34.44 57.37 31.37 48.06 86.40 34.12 26.61 79.26 33.15 33.60\\nVmanba 69.54 32.67 36.42 63.01 31.47 46.99 79.63 34.12 26.49 77.45 33.53 31.85\\nDiffMa 71.47 32.74 35.77 62.56 31.43 47.38 79.00 34.13 26.45 78.53 33.68 30.51\\nHealthGPT-M3 79.38 33.03 33.48 71.81 31.83 43.45 85.06 34.40 25.49 84.23 34.29 27.99\\nHealthGPT-L14 79.73 33.10 32.96 71.92 31.87 43.09 85.31 34.29 26.20 84.96 34.14 28.13\\nmodality conversion). Notably, by this stage, the word em-\\nbedding layer and output head have been fine-tuned, only the\\nH-LoRA modules and adapter modules need to be trained.\\nThis strategy significantly improves the model’s adaptability\\nand flexibility across different tasks.\\n5 Experiments\\n5.1 Data and Experimental Setup\\nData Details. We curate VL-Health dataset (see Fig-\\nure 4). For medical visual comprehension, we leverage\\nmultiple medical-specific datasets, including PubMedVi-\\nsion (Chen et al. 2024a), LLaV A-Med (Li et al. 2024b),\\nPathVQA (He et al. 2020), MIMIC-CXR-VQA (Bae et al.\\n2024), SLAKE (Liu et al. 2021), and VQA-RAD (Lau\\net al. 2018). Additionally, we incorporate high-quality open-\\nworld data from LLaV A-1.5 (Liu et al. 2024b) to preserve\\nthe model’s general knowledge and instruction-following\\ncapabilities. For generation tasks, we construct a recon-\\nstruction dataset based on LLaV A-558k (Liu et al. 2024b),\\nand also explore two key tasks in personalized medical\\nimage enhancement—super-resolution and modality con-\\nversion—using the IXI (Davies et al. 2014) and Syn-\\nthRAD2023 (Thummerer et al. 2023) datasets. Detailed data\\nselection and instruction templates are in the Appendix.\\nModel Details. We select CLIP-L/14 (Radford et al. 2021)\\nas the visual encoder and used the hidden states of its\\nsecond and penultimate layers as concrete-grained and\\nabstract-grained features for model’s dynamic hierarchical\\nvisual perception. Drawing on the successful experiences of\\nLLaV A, we employ a MLP to align the multi-modal fea-\\nture embeddings. We choose the parameter-efficient phi-3-\\nmini (Abdin et al. 2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al.', mimetype='text/plain', start_char_idx=1293, end_char_idx=4042, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='fcb4ec01-76d3-4cd6-ac08-38050e91056f', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comprehensive Evaluation of HealthGPT and Advanced Vision-Language Models in Medical Visual Comprehension and Modality Conversion Tasks\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt and its context, here are three specific questions that can be answered using the information given:\\n\\n1. **What are the specific configurations used for the H-LoRA model in the visual comprehension and generation tasks?**\\n   - The excerpt mentions that the rank of H-LoRA is set to 16 and 64, with four experts involved in the configuration.\\n\\n2. **Which models are compared with HealthGPT in the main experiments for visual comprehension?**\\n   - The excerpt lists several models compared with HealthGPT, including medical-specific LVLMs like Med-Flamingo, LLaV A-Med, and HuatuoGPT-Vision, as well as open-world LVLMs such as BLIP-2, LLaV A-v1.5, InstructBLIP, Yi-VL, and InternVL2.\\n\\n3. **What version of VQGAN is utilized in the study for image indexing and upsampling?**\\n   - The excerpt specifies that the f8-8192 version of VQGAN is used as the image indexing and upsampling module in the experiments.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6d5a0570-973b-42f7-806f-dd3a7431788c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='41ef80bdf8bdf6da173d984639ed42defada78da75bf42b13d19e58704bcf6f0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='98215f8a-0f6f-4515-8c91-432fa232f3bc', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='a6fa9647f421bbc982925c868cb4c285d738f853b2a316d598ad876c94368ae5')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al. 2023), LLaV A-Med (Li et al.\\n2024b), HuatuoGPT-Vision (Chen et al. 2024a)) as well\\nas recent open-world LVLMs (e.g., BLIP-2 (Li et al.\\n2023b), LLaV A-v1.5 (Liu et al. 2024b), InstructBLIP (Dai\\net al. 2023), Yi-VL (Young et al. 2024), InternVL2 (Chen\\n6', mimetype='text/plain', start_char_idx=3634, end_char_idx=4294, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='f01ea275-a6f0-4b3b-8616-793aa8c8d440', embedding=None, metadata={'page_label': '7', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"HealthGPT Innovations: Enhancing Medical Imaging through Super-Resolution, Unified Visual Comprehension, and Multi-Task Learning with Heterogeneous Low-Rank Adaptation\"', 'questions_this_excerpt_can_answer': \"Based on the provided context, here are three specific questions that can be answered using the information in the excerpt:\\n\\n1. **How does HealthGPT-M3's performance in super-resolution tasks compare to other models, specifically in terms of SSIM, PSNR, MSE, and LPIPS metrics?**\\n   - This question can be answered by directly referencing the comparison results in Table 3, which lists the performance metrics for HealthGPT-M3 alongside other models.\\n\\n2. **What are the key observations regarding HealthGPT's performance in medical visual comprehension tasks compared to existing models?**\\n   - This question can be addressed by summarizing the findings from the experimental results mentioned in the excerpt, particularly the comparisons with medical-specific and general-purpose models, as well as the performance of unified models.\\n\\n3. **What advantages does HealthGPT offer in the modality conversion task compared to traditional methods like Pix2Pix and DiffMa?**\\n   - This question can be answered by discussing the results from the CT2MRI-Brain task and highlighting how HealthGPT's unified training approach leads to superior performance metrics compared to the traditional methods mentioned.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='de4841da-cca7-4622-af87-3588becbeae1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='2903bac9b6d5d2bdb5d2790fb3d950a2e50e196c2c7d32dda3ef7c94cb9c9100'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3e28aa58-8cad-4e0a-9270-dc13484d852a', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='205fbdb3f12b9bd9ff5204d3266adc4cecf22d7875187ca72cc3ea0c44957794')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 3: Comparison results of super-resolution task.\\nModel SSIM↑ PSNR↑ MSE↓ LPIPS↓\\nSRGAN 71.34 32.01 41.27 24.50\\nDASR 71.57 32.34 38.25 19.17\\nReal-ESRGAN 67.30 31.87 42.57 20.64\\nLIIF 73.27 32.13 40.14 22.93\\nBSRGAN 69.97 31.97 41.52 28.72\\nHealthGPT-M3 78.19 32.76 34.47 12.02\\nHealthGPT-L14 77.94 32.71 35.19 12.43\\nFigure 5: Performance comparison of LoRA, MoELoRA,\\nand H-LoRA under different rank settings.\\net al. 2024b), Llama-3.2 (Dubey et al. 2024)). Addition-\\nally, we test several SOTA unified visual comprehension\\nand generation models, including Show-o (Xie et al. 2024),\\nUnified-IO 2 (Lu et al. 2024), and Janus (Wu et al.\\n2024). The experimental results are shown in Table 1, with\\nthe following key observations: (i) SOTA Results Com-\\npared with LVLMs: In medical visual comprehension\\ntasks, HealthGPT demonstrates superior performance,\\nsignificantly outperforming both medical-specific models\\n(e.g., HuatuoGPT-Vision) and general-purpose models (e.g.,\\nLlama-3.2). (ii) Surpassing Current Unified LVLMs: De-\\nspite being trained on billions of data points, unified mod-\\nels still exhibit poor generalization performance in medi-\\ncal visual comprehension. For instance, Unified-IO 2 scored\\nonly 33.8. In contrast,HealthGPT-M3, with only 3.8B pa-\\nrameters, scored 61.3 on the medical multi-modal unified\\ntask, significantly outperforming existing unified models in\\nmedical downstream scenarios. (iii) Stable Improvement\\nwith Large Base Model: Our method demonstrates excel-\\nlent scalability, with HealthGPT-L14 achieving a score\\nof 66.4 in the larger model configuration. This result signif-\\nicantly outperforms all other models, highlighting the effec-\\ntiveness of scaling up the base model for enhanced perfor-\\nmance in medical tasks.\\nGeneration. We study three key tasks in medical imag-\\ning. (i) Modality Conversion: In this task, we focus on\\nthe conversion between CT and MRI modalities for the\\nbrain and pelvic regions, designing four specific sub-tasks.\\nAll comparative models (Pix2Pix (Isola et al. 2017), Cy-\\ncleGAN (Zhu et al. 2017), BBDM (Li et al. 2023a),\\n(%)\\n(%)\\n（a） （b）\\nFigure 6: The loss visualization (a) and performance com-\\nparison (b) with respect to different visual perceptions.\\nVmamba (Liu et al. 2024e), and DiffMa (Wang et al.\\n2024b)) trained a separate model for each sub-task, while\\nHealthGPT unify all tasks into a single training process.\\nThe experimental results, shown in Table 11, demonstrate\\nthat our approach outperforms other methods across multi-\\nple evaluation metrics. For instance, in the CT2MRI-Brain\\ntask, HealthGPT-M3 achieves an SSIM of 79.38, signif-\\nicantly surpassing traditional methods like Pix2Pix (71.09)\\nand the recent DiffMa (71.47). (ii) Super-Resolution: We\\nconduct 4× super-resolution experiments on the IXI dataset,\\nwith the results presented in Table 3. Notably, most exist-\\ning methods fail to fully leverage the prior knowledge of key\\nstructures in medical images, resulting in significant short-\\ncomings in detail recovery. In contrast, our method signif-\\nicantly mitigates this issue. Specifically, HealthGPT-M3\\nexcels in key metrics such as SSIM, PSNR, and ISE, achiev-\\ning scores of 78.19, 32.76, and 34.47, respectively. Ad-\\nditionally, HealthGPT-M3 achieves the lowest score of\\n12.34, further validating its exceptional performance in hu-\\nman visual perception.', mimetype='text/plain', start_char_idx=0, end_char_idx=3331, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='3e28aa58-8cad-4e0a-9270-dc13484d852a', embedding=None, metadata={'page_label': '7', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"HealthGPT Innovations: Enhancing Medical Imaging through Super-Resolution, Unified Visual Comprehension, and Multi-Task Learning with Heterogeneous Low-Rank Adaptation\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"HealthGPT Innovations: Enhancing Medical Imaging through Super-Resolution, Unified Visual Comprehension, and Multi-Task Learning with Heterogeneous Low-Rank Adaptation,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are the key performance metrics achieved by HealthGPT-M3, and how do they compare to other models?**\\n   - The excerpt mentions that HealthGPT-M3 excels in key metrics such as SSIM (78.19), PSNR (32.76), and ISE (34.47), and it achieves the lowest score of 12.34 in human visual perception, indicating its exceptional performance compared to other models.\\n\\n2. **How does the performance of H-LoRA compare to LoRA and MoELoRA in medical unified comprehension and generation tasks?**\\n   - The excerpt states that H-LoRA demonstrates superior performance in the majority of comprehension tasks and all generation tasks, particularly improving from 64.90 to 68.50 in the OmniMedVQA benchmark, while MoELoRA does not show advantages in this task.\\n\\n3. **What is the significance of the training parameters in the comparison between HealthGPT-M3 and HealthGPT-L14?**\\n   - The excerpt notes that HealthGPT-L14 was trained with a similar number of trainable parameters as the M3 version, leading to similar performance outcomes, which aligns with the expectations set by the researchers. \\n\\nThese questions focus on specific details and comparisons that are unique to the content of the excerpt and are unlikely to be found in other sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='de4841da-cca7-4622-af87-3588becbeae1', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='2903bac9b6d5d2bdb5d2790fb3d950a2e50e196c2c7d32dda3ef7c94cb9c9100'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='f01ea275-a6f0-4b3b-8616-793aa8c8d440', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '7', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='ff9676711c2ce65bc8c1d2d8f3b887128888a3c9df09c9554baf82de40675ff5')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='In contrast, our method signif-\\nicantly mitigates this issue. Specifically, HealthGPT-M3\\nexcels in key metrics such as SSIM, PSNR, and ISE, achiev-\\ning scores of 78.19, 32.76, and 34.47, respectively. Ad-\\nditionally, HealthGPT-M3 achieves the lowest score of\\n12.34, further validating its exceptional performance in hu-\\nman visual perception. (iii) Reconstruction: We compare\\nHealthGPT-M3 with unified models with reconstruction\\ncapabilities, such as Unified-IO 2 and SEED-X. The results\\nshow that our approach performs better controllability for vi-\\nsual reconstruction. We also trainHealthGPT-L14 with a\\nsimilar number of trainable parameters to the M3 version.\\nHence, the similar performance between the two models\\nmeets our expectations. Details are in the Appendix.\\n5.3 In-Depth Study\\nEffect of Heterogeneous Low-Rank Adaptation.H-LoRA\\nprovides an optimized multi-LoRA architecture for multi-\\ntask learning. We conduct extensive validation of this struc-\\nture, with results presented in Table 4, comparing the per-\\nformance of LoRA, MoELoRA, and H-LoRA in medical\\nunified comprehension and generation tasks. In the majority\\nof comprehension tasks and all generation tasks, H-LoRA\\ndemonstrates superior performance, particularly in the Om-\\nniMedVQA benchmark, where it improved from 64.90 to\\n68.50. Notably, despite some applications of MoELoRA in\\ncertain scenarios, it do not show advantages in this task and\\n7', mimetype='text/plain', start_char_idx=2989, end_char_idx=4404, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='9f50d180-1771-4a99-8eb5-508f9ca3bb0f', embedding=None, metadata={'page_label': '8', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"HealthGPT: Enhancing Medical Visual Comprehension and Generation through Heterogeneous Knowledge Adaptation and Three-Stage Learning Strategies\"', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are three specific questions that can be answered using the information in the excerpt:\\n\\n1. **What are the performance metrics of HealthGPT when using different training strategies, specifically comparing the three-stage training strategy with mixed-training?**\\n   - The excerpt provides specific performance metrics for HealthGPT using both the three-stage training strategy and mixed-training across various medical visual comprehension and generation tasks, highlighting the advantages of the three-stage approach.\\n\\n2. **How does the training time of HealthGPT with H-LoRA compare to that of LoRA and MoELoRA in medical visual comprehension and generation tasks?**\\n   - The excerpt includes a comparison of training times for HealthGPT with different methods, indicating that H-LoRA has a training time approximately 50% longer than LoRA, while also providing the training time for MoELoRA.\\n\\n3. **What insights does the hierarchical visual perception analysis provide regarding the efficiency of comprehension and generation tasks in HealthGPT?**\\n   - The excerpt discusses the results of an ablation analysis on visual perceptual inputs, indicating that comprehension tasks converge more efficiently with abstract-grained inputs, while generation tasks perform better with concrete-grained inputs, emphasizing the importance of tailoring visual inputs for specific tasks.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f3f31dff-a830-4267-9c9a-cf4c96410395', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='398e344fe39994e2f6c13cd9739c897713194fd50b57f784b58e37dc38dd5eba')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 4: We present the performance and speed differences of LoRA, MoELoRA (n=4), and H-LoRA (n=4) on medical visual\\ncomprehension and generation tasks.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQAModel\\nclose all close all close all\\nMMMU\\n-Med OMVQA RECOM MTRANS SR\\nTraining\\nTime\\nHealthGPT w/\\n+LoRA 71.3 57.2 70.0 53.4 76.4 38.6 41.30 65.10 62.67 59.99 65.88 1.00×\\n+MoELoRA 72.5 57.2 66.4 52.4 73.2 36.0 39.30 64.90 67.31 59.76 65.91 1.49×\\n+H-LoRA 73.7 55.9 74.6 56.4 78.7 39.7 43.30 68.50 67.69 60.30 66.14 1.00×\\nTable 5: Comparison between the H-LoRA-based Three-Stage Learning Strategy and the mixed-training approach.\\nComp. Gen.\\nVQA-RAD SLAKE PathVQA CT MRITraining Strategy\\nclose all close all close all\\nMMMU\\n-Med OMVQA Brain Pelvis Brain Pelvis\\nMixed-Training 56.6 37.9 45.0 32.9 65.7 33.6 44.0 48.9 65.64 62.75 56.61 50.77HealthGPT w/ 3-stage-Training 72.5 55.2 77.9 59.6 79.7 49.0 42.7 68.5 70.84 72.99 65.26 61.33\\nFigure 7: Case study of report-to-CXR under different instructions. (a) shows a normal CXR image for comparison. (b) and (c)\\nillustrate generated cases with varying severity and affected regions. The graffiti areas indicate abnormal conditions.\\nhad a training time approximately 50% longer than LoRA.\\nFigure 5 illustrates the performance of the three PEFT meth-\\nods in medical visual comprehension and generation tasks\\nacross different ranks, with H-LoRA consistently outper-\\nforming the other methods in all scenarios, demonstrating\\nsignificant advantages in handling diverse tasks.\\nDifferent Learning Strategy. We propose a three-stage\\nlearning strategy for H-LoRA that decouples comprehension\\nand generation tasks. Unlike methods that train both tasks\\nsimultaneously, our approach reduces performance degra-\\ndation from task conflicts (see Table 5). In the medical vi-\\nsual comprehension task, mixed training causes catastrophic\\nforgetting and degrades visual reconstruction, whereas our\\nstrategy effectively uses the medical embedding knowledge\\nin pre-trained LLMs to mitigate these conflicts. Meanwhile,\\nwe examine how fusing heterogeneous H-LoRA plugins in\\nthe second training stage results in minimal performance\\ndegradation. Detailed results are in the Appendix.\\nHierarchical Visual Perception Analysis. We conduct an\\nablation analysis on visual perceptual inputs for comprehen-\\nsion and generation tasks. Figure 6 shows that comprehen-\\nsion tasks converge more efficiently with abstract-grained\\ninputs, while generation tasks perform better with concrete-\\ngrained inputs. This highlights the importance of the hier-\\narchical visual perception we propose, suggesting that tai-\\nloring visual inputs for specific tasks at different hierarchies\\ncan significantly improve efficiency.\\nReport-to-CXR Task. We further explore the medical im-\\nage generation task without reference images, using a small\\namount of MIMIC-CXR data (Johnson et al. 2019) for in-\\nstruction fine-tuning. Figure 7 annotates images with vary-\\ning injury degrees and locations, comparing them to healthy\\nCXR images. We observe thatHealthGPT effectively gen-\\nerates CXR images based on the instructions, showcasing its\\npotential in healthcare education and auxiliary diagnosis.\\n6 Conclusion\\nIn this paper, we introduceHealthGPT, a Med-LVLM that\\nunifies medical vision-language comprehension and gen-\\neration through a novel heterogeneous knowledge adap-\\ntation approach. Experimental results demonstrate that\\nHealthGPT achieves significant performance improve-\\nments across multiple medical comprehension and genera-\\ntion tasks, showcasing its potential for healthcare applica-\\n8', mimetype='text/plain', start_char_idx=0, end_char_idx=3562, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='158d0193-abc4-4935-ab97-0e1034453a3a', embedding=None, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Recent Advances in Multimodal Models: Integrating Vision and Language through Innovative Techniques in Pre-Training, Medical Applications, and Image Synthesis\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"Recent Advances in Multimodal Models: Integrating Vision and Language through Innovative Techniques in Pre-Training, Medical Applications, and Image Synthesis,\" here are three specific questions that can be answered using the context:\\n\\n1. **What is the focus of the paper titled \"Recent Advances in Multimodal Models\"?**\\n   - The paper discusses innovative techniques in pre-training, medical applications, and image synthesis related to the integration of vision and language in multimodal models.\\n\\n2. **What are some of the notable datasets and models referenced in the document that contribute to the field of multimodal learning?**\\n   - The document references several datasets and models, including the MIMIC-Ext-MIMIC-CXR-VQA dataset for visual question answering in chest X-ray images and the Huatuogpt-vision model, which aims to inject medical visual knowledge into multimodal large language models.\\n\\n3. **Which authors contributed to the technical report titled \"Phi-4,\" and what is its significance in the context of multimodal models?**\\n   - The authors of the \"Phi-4\" technical report include Abdin, M.; Aneja, J.; Behl, H.; and others. This report is significant as it contributes to the ongoing research and development in the field of multimodal models, as indicated by its citation in the document.\\n\\nThese questions are tailored to extract specific information from the excerpt that may not be readily available in other sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d62a7ebd-46e5-4e42-b68a-23024ada525f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='10945dcb05c8dd55d43b50dc88f6364dc5c09552326a97ed13b0e42c1018a0d9'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1cb3b9b3-ebfc-4574-9075-178f4a5730f9', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6790d4990eea4140aea245ccbbe5d7e8500343a260f0594174102fcaf3da98eb')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='tions.\\nReferences\\nAbdin, M.; Aneja, J.; Behl, H.; Bubeck, S.; Eldan, R.; Gu-\\nnasekar, S.; Harrison, M.; Hewett, R. J.; Javaheripi, M.;\\nKauffmann, P.; et al. 2024. Phi-4 technical report. arXiv\\npreprint arXiv:2412.08905.\\nBae, S.; Kyung, D.; Ryu, J.; Cho, E.; Lee, G.; Kweon, S.;\\nOh, J.; JI, L.; Chang, E.; Kim, T.; et al. 2024. MIMIC-Ext-\\nMIMIC-CXR-VQA: A Complex, Diverse, And Large-Scale\\nVisual Question Answering Dataset for Chest X-ray Images.\\nChen, J.; Gui, C.; Ouyang, R.; Gao, A.; Chen, S.; Chen,\\nG. H.; Wang, X.; Zhang, R.; Cai, Z.; Ji, K.; et al.\\n2024a. Huatuogpt-vision, towards injecting medical visual\\nknowledge into multimodal llms at scale. arXiv preprint\\narXiv:2406.19280.\\nChen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.; Tong,\\nW.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024b. How far are we to\\ngpt-4v? closing the gap to commercial multimodal models\\nwith open-source suites. arXiv preprint arXiv:2404.16821.\\nChern, E.; Su, J.; Ma, Y .; and Liu, P. 2024. Anole:\\nAn open, autoregressive, native large multimodal mod-\\nels for interleaved image-text generation. arXiv preprint\\narXiv:2407.06135.\\nDai, W.; Li, J.; Li, D.; Tiong, A. M. H.; Zhao, J.; Wang, W.;\\nLi, B.; Fung, P.; and Hoi, S. 2023. InstructBLIP: Towards\\nGeneral-purpose Vision-Language Models with Instruction\\nTuning. arXiv:2305.06500.\\nDavies, R. L.; Royston, P. A.; Leung, M. S.; Haider, M. E.\\nA. M. J.; Barkhof, S. G. A. L.; and B., P. E. T. M. 2014. The\\nIXI Dataset. Accessed: 2025-01-30.\\nDing, N.; Qin, Y .; Yang, G.; Wei, F.; Yang, Z.; Su, Y .;\\nHu, S.; Chen, Y .; Chan, C.-M.; Chen, W.; et al. 2023.\\nParameter-efficient fine-tuning of large-scale pre-trained\\nlanguage models. Nature Machine Intelligence, 5(3): 220–\\n235.\\nDong, R.; Han, C.; Peng, Y .; Qi, Z.; Ge, Z.; Yang, J.; Zhao,\\nL.; Sun, J.; Zhou, H.; Wei, H.; et al. 2023. Dreamllm:\\nSynergistic multimodal comprehension and creation. arXiv\\npreprint arXiv:2309.11499.\\nDubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.;\\nLetman, A.; Mathur, A.; Schelten, A.; Yang, A.; Fan, A.;\\net al. 2024. The llama 3 herd of models. arXiv preprint\\narXiv:2407.21783.\\nEsser, P.; Rombach, R.; and Ommer, B. 2021. Taming trans-\\nformers for high-resolution image synthesis. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern\\nrecognition, 12873–12883.\\nGe, Y .; Ge, Y .; Zeng, Z.; Wang, X.; and Shan, Y . 2023. Plant-\\ning a seed of vision in large language model. arXiv preprint\\narXiv:2307.08041.\\nGe, Y .; Zhao, S.; Zhu, J.; Ge, Y .', mimetype='text/plain', start_char_idx=0, end_char_idx=2473, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='1cb3b9b3-ebfc-4574-9075-178f4a5730f9', embedding=None, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Recent Advances in Multimodal Models: Integrating Vision and Language through Innovative Techniques in Pre-Training, Medical Applications, and Image Synthesis\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"Recent Advances in Multimodal Models: Integrating Vision and Language through Innovative Techniques in Pre-Training, Medical Applications, and Image Synthesis,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are some notable contributions to the field of medical visual question answering as mentioned in the excerpt?**\\n   - The excerpt references \"Pathvqa: 30000+ questions for medical visual question answering\" by He et al. (2020) and \"Omnimedvqa: A new large-scale comprehensive evaluation benchmark for medical lvlm\" by Hu et al. (2024), highlighting significant advancements in this area.\\n\\n2. **Which recent works focus on integrating vision and language in the context of large language models, and what are their contributions?**\\n   - The excerpt mentions \"Planting a seed of vision in large language model\" by Ge et al. (2023) and \"Llava-med: Training a large language-and-vision assistant for biomedicine in one day\" by Li et al. (2024a, 2024b), indicating efforts to enhance multimodal understanding and applications in biomedicine.\\n\\n3. **What innovative techniques in pre-training multimodal models are discussed in the context of image synthesis?**\\n   - The excerpt cites \"Taming transformers for high-resolution image synthesis\" by Esser et al. (2021) and \"Bbdm: Image-to-image translation with brownian bridge diffusion models\" by Li et al. (2023a), showcasing advancements in image synthesis techniques that leverage multimodal models.\\n\\nThese questions are tailored to extract specific insights from the excerpt that may not be readily available in other sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d62a7ebd-46e5-4e42-b68a-23024ada525f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='10945dcb05c8dd55d43b50dc88f6364dc5c09552326a97ed13b0e42c1018a0d9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='158d0193-abc4-4935-ab97-0e1034453a3a', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='c8a35225f5c56084aa4486342fed46633f2d37e6696ac7cc416f34f32e5ad342'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='58a251e3-1ff9-44bf-b6fb-0cdfce33d708', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='54ec272bf8ade7f392cb232cad95d712d9f7fb92f4a6ce8cb3b5b55b0df2fd13')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2021. Taming trans-\\nformers for high-resolution image synthesis. In Proceedings\\nof the IEEE/CVF conference on computer vision and pattern\\nrecognition, 12873–12883.\\nGe, Y .; Ge, Y .; Zeng, Z.; Wang, X.; and Shan, Y . 2023. Plant-\\ning a seed of vision in large language model. arXiv preprint\\narXiv:2307.08041.\\nGe, Y .; Zhao, S.; Zhu, J.; Ge, Y .; Yi, K.; Song, L.; Li, C.;\\nDing, X.; and Shan, Y . 2024. Seed-x: Multimodal models\\nwith unified multi-granularity comprehension and genera-\\ntion. arXiv preprint arXiv:2404.14396.\\nHe, X.; Zhang, Y .; Mou, L.; Xing, E.; and Xie, P. 2020.\\nPathvqa: 30000+ questions for medical visual question an-\\nswering. arXiv preprint arXiv:2003.10286.\\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\\nS.; Wang, L.; and Chen, W. 2021. Lora: Low-rank adaptation\\nof large language models. arXiv preprint arXiv:2106.09685.\\nHu, Y .; Li, T.; Lu, Q.; Shao, W.; He, J.; Qiao, Y .; and Luo,\\nP. 2024. Omnimedvqa: A new large-scale comprehensive\\nevaluation benchmark for medical lvlm. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 22170–22183.\\nIsola, P.; Zhu, J.-Y .; Zhou, T.; and Efros, A. A. 2017. Image-\\nto-image translation with conditional adversarial networks.\\nIn Proceedings of the IEEE conference on computer vision\\nand pattern recognition, 1125–1134.\\nJohnson, A. E.; Pollard, T. J.; Greenbaum, N. R.; Lungren,\\nM. P.; Deng, C.-y.; Peng, Y .; Lu, Z.; Mark, R. G.; Berkowitz,\\nS. J.; and Horng, S. 2019. MIMIC-CXR-JPG, a large pub-\\nlicly available database of labeled chest radiographs. arXiv\\npreprint arXiv:1901.07042.\\nLau, J. J.; Gayen, S.; Ben Abacha, A.; and Demner-\\nFushman, D. 2018. A dataset of clinically generated visual\\nquestions and answers about radiology images. Scientific\\ndata, 5(1): 1–10.\\nLi, B.; Xue, K.; Liu, B.; and Lai, Y .-K. 2023a. Bbdm: Image-\\nto-image translation with brownian bridge diffusion models.\\nIn Proceedings of the IEEE/CVF conference on computer\\nvision and pattern Recognition, 1952–1961.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024a. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, C.; Wong, C.; Zhang, S.; Usuyama, N.; Liu, H.; Yang,\\nJ.; Naumann, T.; Poon, H.; and Gao, J. 2024b. Llava-\\nmed: Training a large language-and-vision assistant for\\nbiomedicine in one day. Advances in Neural Information\\nProcessing Systems, 36.\\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023b. Blip-2:\\nBootstrapping language-image pre-training with frozen im-\\nage encoders and large language models. In International\\nconference on machine learning, 19730–19742. PMLR.\\nLin, T.; Liu, J.; Zhang, W.; Li, Z.; Dai, Y .; Li, H.; Yu, Z.; He,\\nW.; Li, J.; Jiang, H.; et al. 2024. Teamlora: Boosting low-\\nrank adaptation with expert collaboration and competition.', mimetype='text/plain', start_char_idx=2130, end_char_idx=5030, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='58a251e3-1ff9-44bf-b6fb-0cdfce33d708', embedding=None, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Recent Advances in Multimodal Models: Integrating Vision and Language through Innovative Techniques in Pre-Training, Medical Applications, and Image Synthesis\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt and context, here are three specific questions that can be answered using the information given:\\n\\n1. **What is the title of the document referenced in the excerpt, and what are its main themes?**\\n   - The document is titled \"Recent Advances in Multimodal Models: Integrating Vision and Language through Innovative Techniques in Pre-Training, Medical Applications, and Image Synthesis,\" focusing on advancements in multimodal models that combine vision and language.\\n\\n2. **What are the key contributions of the paper by Lin et al. (2024) mentioned in the excerpt?**\\n   - The paper by Lin et al. (2024) introduces \"Teamlora,\" which aims to enhance low-rank adaptation through expert collaboration and competition, as indicated in the excerpt.\\n\\n3. **What is the significance of the Blip-2 model as referenced in the excerpt?**\\n   - The Blip-2 model is significant for its approach to bootstrapping language-image pre-training by utilizing frozen image encoders alongside large language models, as discussed in the context of its presentation at the International Conference on Machine Learning.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d62a7ebd-46e5-4e42-b68a-23024ada525f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='10945dcb05c8dd55d43b50dc88f6364dc5c09552326a97ed13b0e42c1018a0d9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1cb3b9b3-ebfc-4574-9075-178f4a5730f9', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '9', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='b0f0562abdcb8443339b4128e2b840da5ff59a4c2143360cabc5df9a3682425c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2023b. Blip-2:\\nBootstrapping language-image pre-training with frozen im-\\nage encoders and large language models. In International\\nconference on machine learning, 19730–19742. PMLR.\\nLin, T.; Liu, J.; Zhang, W.; Li, Z.; Dai, Y .; Li, H.; Yu, Z.; He,\\nW.; Li, J.; Jiang, H.; et al. 2024. Teamlora: Boosting low-\\nrank adaptation with expert collaboration and competition.\\narXiv preprint arXiv:2408.09856.\\n9', mimetype='text/plain', start_char_idx=4664, end_char_idx=5065, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='1f0342e5-32ae-465e-911a-6b95664b9760', embedding=None, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Integrating Multimodal Approaches in Biomedical AI: A Comprehensive Survey of Innovations in Vision-Language Models, Clinical Knowledge, and Transformer Architectures for Enhanced Healthcare Applications\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What is the title of the document that includes a comprehensive survey of innovations in biomedical AI, particularly focusing on multimodal approaches?**\\n   - Answer: \"Integrating Multimodal Approaches in Biomedical AI: A Comprehensive Survey of Innovations in Vision-Language Models, Clinical Knowledge, and Transformer Architectures for Enhanced Healthcare Applications\"\\n\\n2. **Which authors contributed to the paper titled \"Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering,\" and in which conference was it presented?**\\n   - Answer: The authors are Liu, B.; Zhan, L.-M.; Xu, L.; Ma, L.; Yang, Y.; and Wu, X.-M., and it was presented at the 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI).\\n\\n3. **What is the focus of the paper \"Biomedgpt: An open multimodal large language model for biomedicine,\" and which journal published it?**\\n   - Answer: The focus of the paper is on a multimodal large language model specifically designed for biomedicine, and it was published in the IEEE Journal of Biomedical and Health Informatics.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f8441ba4-0487-4eb3-a2e0-19131d9ace1b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='559e70903051a9340844697f4b220e89647b593db0106aa23085d70f87fb3fdb'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='2d0ff875-b46d-41e2-8546-6cff575b40a5', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='5f386a8a56f528a56e45b3f1312c33fdc5fb08ca6ac6c6376bc0114be248e15a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Liu, B.; Zhan, L.-M.; Xu, L.; Ma, L.; Yang, Y .; and Wu,\\nX.-M. 2021. Slake: A semantically-labeled knowledge-\\nenhanced dataset for medical visual question answering. In\\n2021 IEEE 18th International Symposium on Biomedical\\nImaging (ISBI), 1650–1654. IEEE.\\nLiu, D.; Zhao, S.; Zhuo, L.; Lin, W.; Qiao, Y .; Li, H.; and\\nGao, P. 2024a. Lumina-mgpt: Illuminate flexible photore-\\nalistic text-to-image generation with multimodal generative\\npretraining. arXiv preprint arXiv:2408.02657.\\nLiu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2024b. Improved\\nbaselines with visual instruction tuning. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 26296–26306.\\nLiu, H.; Li, C.; Li, Y .; Li, B.; Zhang, Y .; Shen, S.; and Lee,\\nY . J. 2024c. LLaV A-NeXT: Improved reasoning, OCR, and\\nworld knowledge. https://llava-vl.github.io/blog/2024-01-\\n30-llava-next/.\\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruc-\\ntion Tuning. In NeurIPS.\\nLiu, Q.; Wu, X.; Zhao, X.; Zhu, Y .; Xu, D.; Tian, F.; and\\nZheng, Y . 2024d. When moe meets llms: Parameter efficient\\nfine-tuning for multi-task medical applications. In Proceed-\\nings of the 47th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval, 1104–\\n1114.\\nLiu, Y .; Tian, Y .; Zhao, Y .; Yu, H.; Xie, L.; Wang, Y .; Ye,\\nQ.; and Liu, Y . 2024e. VMamba: Visual State Space Model.\\narXiv preprint arXiv:2401.10166.\\nLu, J.; Clark, C.; Lee, S.; Zhang, Z.; Khosla, S.; Marten, R.;\\nHoiem, D.; and Kembhavi, A. 2024. Unified-IO 2: Scaling\\nAutoregressive Multimodal Models with Vision Language\\nAudio and Action. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition , 26439–\\n26455.\\nLu, J.; Clark, C.; Zellers, R.; Mottaghi, R.; and Kembhavi,\\nA. 2022. Unified-io: A unified model for vision, language,\\nand multi-modal tasks. In The Eleventh International Con-\\nference on Learning Representations.\\nLuo, T.; Lei, J.; Lei, F.; Liu, W.; He, S.; Zhao, J.; and Liu,\\nK. 2024a. Moelora: Contrastive learning guided mixture of\\nexperts on parameter-efficient fine-tuning for large language\\nmodels. arXiv preprint arXiv:2402.12851.\\nLuo, Y .; Zhang, J.; Fan, S.; Yang, K.; Hong, M.; Wu, Y .;\\nQiao, M.; and Nie, Z. 2024b. Biomedgpt: An open multi-\\nmodal large language model for biomedicine. IEEE Journal\\nof Biomedical and Health Informatics.\\nMasoudnia, S.; and Ebrahimpour, R. 2014. Mixture of ex-\\nperts: a literature survey. Artificial Intelligence Review, 42:\\n275–293.\\nMoor, M.; Huang, Q.; Wu, S.; Yasunaga, M.; Dalmia, Y .;\\nLeskovec, J.; Zakka, C.; Reis, E. P.; and Rajpurkar, P. 2023.\\nMed-flamingo: a multimodal medical few-shot learner. In\\nMachine Learning for Health (ML4H), 353–367. PMLR.\\nNath, V .; Li, W.; Yang, D.; Myronenko, A.; Zheng, M.; Lu,\\nY .; Liu, Z.; Yin, H.; Law, Y .', mimetype='text/plain', start_char_idx=0, end_char_idx=2798, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='2d0ff875-b46d-41e2-8546-6cff575b40a5', embedding=None, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Integrating Multimodal Approaches in Biomedical AI: A Comprehensive Survey of Innovations in Vision-Language Models, Clinical Knowledge, and Transformer Architectures for Enhanced Healthcare Applications\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What are some recent advancements in multimodal medical AI as highlighted in the document?**\\n   - The excerpt lists several recent works, such as \"Med-flamingo,\" \"Vila-m3,\" and \"Xraygpt,\" which represent advancements in integrating vision-language models with medical expertise and applications in healthcare.\\n\\n2. **Which specific studies or papers are referenced in the context of enhancing vision-language models with clinical knowledge?**\\n   - The excerpt mentions studies like \"Vila-m3: Enhancing vision-language models with medical expert knowledge\" and \"Large language models encode clinical knowledge,\" indicating a focus on the integration of clinical knowledge into AI models.\\n\\n3. **What is the significance of the \"SynthRAD2023 Grand Challenge dataset\" as mentioned in the document?**\\n   - The excerpt describes the \"SynthRAD2023 Grand Challenge dataset\" as a resource for generating synthetic CT for radiotherapy, highlighting its importance in advancing medical imaging and treatment methodologies.\\n\\nThese questions focus on specific contributions and findings within the context of the document, providing insights that are not readily available in other sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f8441ba4-0487-4eb3-a2e0-19131d9ace1b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='559e70903051a9340844697f4b220e89647b593db0106aa23085d70f87fb3fdb'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1f0342e5-32ae-465e-911a-6b95664b9760', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='1bfbd5edc3c63d36a2c80d84c4cc8aa97ec689fc903f1dfcc14710b43e360dae'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='67debe6a-461b-4fb5-93ee-cdbd3776e851', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7209e129c0f024361cb14ccda92742066c0f8db4fa1dcc4ca319db3bdf0a91ed')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=';\\nLeskovec, J.; Zakka, C.; Reis, E. P.; and Rajpurkar, P. 2023.\\nMed-flamingo: a multimodal medical few-shot learner. In\\nMachine Learning for Health (ML4H), 353–367. PMLR.\\nNath, V .; Li, W.; Yang, D.; Myronenko, A.; Zheng, M.; Lu,\\nY .; Liu, Z.; Yin, H.; Law, Y . M.; Tang, Y .; et al. 2024. Vila-\\nm3: Enhancing vision-language models with medical expert\\nknowledge. arXiv preprint arXiv:2411.12915.\\nOpenAI. 2023. GPT-4V(ision) System Card. https://cdn.\\nopenai.com/papers/GPTV System Card.pdf.\\nPan, K.; Tang, S.; Li, J.; Fan, Z.; Chow, W.; Yan, S.;\\nChua, T.-S.; Zhuang, Y .; and Zhang, H. 2024. Auto-\\nEncoding Morph-Tokens for Multimodal LLM. arXiv\\npreprint arXiv:2405.01926.\\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\\net al. 2021. Learning transferable visual models from nat-\\nural language supervision. In International conference on\\nmachine learning, 8748–8763. PMLR.\\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\\net al. 2023. Large language models encode clinical knowl-\\nedge. Nature, 620(7972): 172–180.\\nTeam, C. 2024. Chameleon: Mixed-modal early-fusion\\nfoundation models. arXiv preprint arXiv:2405.09818.\\nThawkar, O.; Shaker, A.; Mullappilly, S. S.; Cholakkal,\\nH.; Anwer, R. M.; Khan, S.; Laaksonen, J.; and Khan,\\nF. S. 2023. Xraygpt: Chest radiographs summarization\\nusing medical vision-language models. arXiv preprint\\narXiv:2306.07971.\\nThummerer, A.; van der Bijl, E.; Galapon Jr, A.; Verhoeff,\\nJ. J.; Langendijk, J. A.; Both, S.; van den Berg, C. N. A.;\\nand Maspero, M. 2023. SynthRAD2023 Grand Challenge\\ndataset: Generating synthetic CT for radiotherapy. Medical\\nphysics, 50(7): 4664–4674.\\nTian, D.; Jiang, S.; Zhang, L.; Lu, X.; and Xu, Y . 2023. The\\nrole of large language models in medical image processing:\\na narrative review. Quantitative Imaging in Medicine and\\nSurgery, 14(1): 1108.\\nTong, S.; Fan, D.; Zhu, J.; Xiong, Y .; Chen, X.; Sinha, K.;\\nRabbat, M.; LeCun, Y .; Xie, S.; and Liu, Z. 2024. Meta-\\nMorph: Multimodal Understanding and Generation via In-\\nstruction Tuning. arXiv preprint arXiv:2412.14164.\\nTu, T.; Azizi, S.; Driess, D.; Schaekermann, M.; Amin, M.;\\nChang, P.-C.; Carroll, A.; Lau, C.; Tanno, R.; Ktena, I.; et al.\\n2024. Towards generalist biomedical AI. NEJM AI, 1(3):\\nAIoa2300138.\\nVig, J. 2019. A multiscale visualization of attention in the\\ntransformer model. arXiv preprint arXiv:1906.05714.\\nWang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y .; Wang, J.;\\nZhang, F.; Wang, Y .; Li, Z.; Yu, Q.; et al. 2024a.', mimetype='text/plain', start_char_idx=2537, end_char_idx=5128, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='67debe6a-461b-4fb5-93ee-cdbd3776e851', embedding=None, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Integrating Multimodal Approaches in Biomedical AI: A Comprehensive Survey of Innovations in Vision-Language Models, Clinical Knowledge, and Transformer Architectures for Enhanced Healthcare Applications\"', 'questions_this_excerpt_can_answer': \"Based on the provided context, here are three specific questions that can be answered using the excerpt:\\n\\n1. **What is the title of the document referenced in the excerpt, and what is its primary focus?**\\n   - This question can be answered by identifying the document title and understanding its emphasis on integrating multimodal approaches in biomedical AI.\\n\\n2. **What are some of the key references cited in the document related to advancements in biomedical AI and transformer models?**\\n   - This question can be addressed by listing the authors and their works mentioned in the excerpt, such as the NEJM AI article and the arXiv preprints, which highlight significant contributions to the field.\\n\\n3. **What is the significance of the publication date of the document, and how does it relate to the advancements in biomedical AI mentioned in the excerpt?**\\n   - This question can be answered by discussing the implications of the document's creation and modification dates (September 1, 2025) in the context of ongoing research and developments in the field of biomedical AI as referenced in the excerpt.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f8441ba4-0487-4eb3-a2e0-19131d9ace1b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='559e70903051a9340844697f4b220e89647b593db0106aa23085d70f87fb3fdb'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2d0ff875-b46d-41e2-8546-6cff575b40a5', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '10', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='6cda3b691e8a02deefd13bbb11472d7a2729abe3d2bc76f9f0babf65f4da0367')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='2024. Towards generalist biomedical AI. NEJM AI, 1(3):\\nAIoa2300138.\\nVig, J. 2019. A multiscale visualization of attention in the\\ntransformer model. arXiv preprint arXiv:1906.05714.\\nWang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y .; Wang, J.;\\nZhang, F.; Wang, Y .; Li, Z.; Yu, Q.; et al. 2024a. Emu3:\\nNext-token prediction is all you need. arXiv preprint\\narXiv:2409.18869.\\n10', mimetype='text/plain', start_char_idx=4837, end_char_idx=5209, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='c4714f61-5ae7-4626-b04e-928d82e92cb0', embedding=None, metadata={'page_label': '11', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Recent Innovations and Research Trends in Multimodal Learning for Enhanced Medical Imaging\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"Recent Innovations and Research Trends in Multimodal Learning for Enhanced Medical Imaging,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are some recent advancements in multimodal learning techniques for medical imaging as highlighted in the document?**\\n   - This question can be answered by referencing the various studies and innovations mentioned in the excerpt, such as the Medclip model and the Soft Masked Mamba Diffusion Model.\\n\\n2. **Which authors contributed to the research on multimodal learning and what are the titles of their respective works mentioned in the excerpt?**\\n   - The excerpt lists several authors along with their works, such as Wang et al. (2022) with \"Medclip\" and Wu et al. (2024) with \"Janus.\" This question allows for a detailed identification of contributors and their contributions.\\n\\n3. **What are the publication years and arXiv identifiers for the studies referenced in the excerpt?**\\n   - The excerpt provides specific publication years and arXiv identifiers for each study, such as \"arXiv:2210.10163\" for Wang et al. (2022) and \"arXiv:2403.04652\" for Young et al. (2024). This question focuses on the bibliographic details of the cited works.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='eafae00c-fd06-4349-839e-280d6fccf61c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='60dd8be7762f1b431824c90627d5fc28bda712b7c19d9ff6a40f18bab7a35c0b')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Wang, Z.; Wu, Z.; Agarwal, D.; and Sun, J. 2022. Medclip:\\nContrastive learning from unpaired medical images and text.\\narXiv preprint arXiv:2210.10163.\\nWang, Z.; Zhang, L.; Wang, L.; and Zhang, Z. 2024b. Soft\\nMasked Mamba Diffusion Model for CT to MRI Conver-\\nsion. arXiv preprint arXiv:2406.15910.\\nWu, C.; Chen, X.; Wu, Z.; Ma, Y .; Liu, X.; Pan, Z.; Liu, W.;\\nXie, Z.; Yu, X.; Ruan, C.; and Luo, P. 2024. Janus: Decou-\\npling Visual Encoding for Unified Multimodal Understand-\\ning and Generation. arXiv:2410.13848.\\nWu, S.; Fei, H.; Qu, L.; Ji, W.; and Chua, T.-S. 2023.\\nNext-gpt: Any-to-any multimodal llm. arXiv preprint\\narXiv:2309.05519.\\nXie, J.; Mao, W.; Bai, Z.; Zhang, D. J.; Wang, W.; Lin, K. Q.;\\nGu, Y .; Chen, Z.; Yang, Z.; and Shou, M. Z. 2024. Show-o:\\nOne single transformer to unify multimodal understanding\\nand generation. arXiv preprint arXiv:2408.12528.\\nYoung, A.; Chen, B.; Li, C.; Huang, C.; Zhang, G.; Zhang,\\nG.; Li, H.; Zhu, J.; Chen, J.; Chang, J.; et al. 2024.\\nYi: Open foundation models by 01. ai. arXiv preprint\\narXiv:2403.04652.\\nZhou, H.; Liu, F.; Gu, B.; Zou, X.; Huang, J.; Wu, J.; Li,\\nY .; Chen, S. S.; Zhou, P.; Liu, J.; et al. 2023. A survey of\\nlarge language models in medicine: Progress, application,\\nand challenge. arXiv preprint arXiv:2311.05112.\\nZhu, J.-Y .; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-\\npaired image-to-image translation using cycle-consistent ad-\\nversarial networks. InProceedings of the IEEE international\\nconference on computer vision, 2223–2232.\\n11', mimetype='text/plain', start_char_idx=0, end_char_idx=1504, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='07728dda-e9ce-40fe-865b-3b39c127eb0e', embedding=None, metadata={'page_label': '12', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"HealthGPT: An In-Depth Exploration of Model Architecture, Training Strategies, and Hyperparameter Configurations for Medical Vision-Language Processing\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document \"HealthGPT: An In-Depth Exploration of Model Architecture, Training Strategies, and Hyperparameter Configurations for Medical Vision-Language Processing,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are the two versions of HealthGPT, and what are their respective pre-trained LLMs and parameter sizes?**\\n   - This question can be answered by referring to the details provided in the model overview section, which specifies the versions (HealthGPT-M3 and HealthGPT-L14), their associated pre-trained LLMs (Phi-3-mini and Phi-4), and their parameter sizes (3.8B and 14B).\\n\\n2. **What is the three-stage learning strategy proposed for training HealthGPT, and how do the hyperparameter settings differ between the two versions during these stages?**\\n   - The excerpt outlines a three-stage learning strategy and provides a detailed hyperparameter configuration for both HealthGPT-M3 and HealthGPT-L14 across different stages, including differences in learning rates, global batch sizes, and dropout rates.\\n\\n3. **What specific challenges were observed during the training of HealthGPT, and how did these challenges influence the hyperparameter configurations for the two model versions?**\\n   - The text mentions instances of loss spikes during training and notes that larger model parameters and learning rates contributed to this issue, leading to slight differences in hyperparameter settings between HealthGPT-M3 and HealthGPT-L14. This question allows for a deeper understanding of the training dynamics and adjustments made in response to observed challenges.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a271bd41-2149-4376-808e-f5dc9e5c8ed4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='cb73576cfc5593b46c2c5951c08379a55b1e2c742a64bcad9ceb29e224868cbb')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Appendix\\nThis is the Appendix for “HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation\\nvia Heterogeneous Knowledge Adaptation”. This Appendix is organized as follows:\\n• Section A presents the experimental implementation details, the training process of HealthGPT, and the specifics of\\nVL-Health.\\n• Section B systematically provides an analysis of Heterogeneous Low-Rank Adaptation.\\n• Section C shows supplementary experimental results to validate the effectiveness ofHealthGPT.\\nA Implementation Details\\nA.1 Model Details\\nWe employ CLIP-L/14 (Radford et al. 2021) as the visual feature extractor, extracting both shallow and deep features to serve as\\nvisual tokens. The model uses alignment adapters, implemented with two-layer MLPs, to align shallow features, representing\\nconcrete visual granularity, and deep features, representing abstract visual granularity. These visual tokens are concatenated\\nwith text tokens and input into the large language models (LLMs).\\nHealthGPT offers two versions: HealthGPT-M3 and HealthGPT-L14, which are based on Phi-3-mini (Abdin et al.\\n2024) and Phi-4 (Abdin et al. 2024) as the pre-trained LLMs, respectively. In addition, we expand the LLM vocabulary with\\n8192 VQ indices derived from VQGAN-f8-8192 (Esser, Rombach, and Ommer 2021), serving as multi-modal tokens to further\\naugment the model’s capacity for understanding both visual and textual input. Figure 6 shows the details.\\nTable 6: Overview of the Components of HealthGPT.\\nModel ViT Adapter MLP-dims Model dims LLM Params Vocab Size H-LoRA Rank\\nHealthGPT-M3 CLIP-L/14 2-layer MLP 1024 3072 Phi-3-mini 3.8B 40206 16(Comp.), 64(Gen.)\\nHealthGPT-L14 CLIP-L/14 2-layer MLP 1024 5120 Phi-4 14B 108547 8(Comp.), 32(Gen.)\\nA.2 Training Details\\nIn this study, we propose a three-stage learning strategy that is compatible with our innovative heterogeneous low-rank adapta-\\ntion (H-LoRA). We provide a detailed hyperparameter configuration for the model’s three-stage training process. The specific\\nhyperparameter settings used are listed in Table 7. These hyperparameters are crucial for ensuring the model’s learning efficacy\\nand final performance.\\nTable 7: Overview of Hyperparameter Configurations.\\nHealthGPT-M3 HealthGPT-L14\\nStage-1 Stage-2 Stage-3 Stage-1 Stage-2 Stage-3Hyperparameter\\nComp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen. Comp. Gen.\\nOptimizer AdamW AdamW AdamW AdamW AdamW AdamW\\nAdapter LR 1e-3 2e-5 2e-5 2e-5 1e-3 2e-5 2e-5 2e-5\\nLearning Rate / 2e-4 2e-4 2e-4 / 1e-4 2e-4 2e-4\\nGlobal Batch Size 256 64 32 128 64 256 64 32 128 64\\nWeight Decay 0 0 0 0 0 0\\nDropout Rate 0 0.05 0.05 0.05 0 0.05 0.05 0.05\\nLR Scheduler Warm Up Constant Warm Up Warm Up Constant Warm Up\\nMax Sequence Length 2048 2048 2048 2048 2048 2048\\nIt is worth noting that we sometimes observe instances of loss spikes during the training of medical visual comprehension\\nand generation tasks. Through repeated validation, we discovered that larger model parameters and learning rates tend to lead to\\nthis issue, which is the reason for the slight differences in hyperparameters betweenHealthGPT-M3 and HealthGPT-L14.\\n12', mimetype='text/plain', start_char_idx=0, end_char_idx=3134, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='e2b89757-ba49-4a2e-b091-b37ab4e607e0', embedding=None, metadata={'page_label': '13', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"VL-Health Dataset: A Comprehensive Collection and Processing Framework for Medical Imaging to Enhance Comprehension and Generation Tasks\"', 'questions_this_excerpt_can_answer': \"Based on the provided excerpt from the document about the VL-Health dataset, here are three specific questions that can be answered using the information in the context:\\n\\n1. **What are the two key steps involved in the construction of the VL-Health dataset, and what specific tasks are associated with each step?**\\n   - This question targets the detailed process of dataset construction, specifically focusing on the data collection and processing phases, as outlined in the excerpt.\\n\\n2. **How many visual question-answering (VQA) training samples and generation training samples were ultimately selected for the VL-Health dataset, and what is the significance of this selection?**\\n   - This question seeks to extract specific numerical data regarding the training samples and the rationale behind their selection, which is crucial for understanding the dataset's design.\\n\\n3. **What types of medical imaging modalities and diseases are represented in the VL-Health dataset, and how does this diversity contribute to the model's training?**\\n   - This question aims to explore the variety of modalities and diseases included in the dataset, as well as the implications of this diversity for enhancing the model's learning capabilities.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ba538698-7cc4-4ecb-afcb-6813a2391de6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='20d53bfbbd0511775a4a7ba95d390f821d64a0fdfc08fa769b1d34dc8a9a3be5')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='（a） （b）\\nFigure 8: VL-Health dataset collection distribution.\\nA.3 VL-Health\\nThe construction of theVL-Health dataset involves two key steps:(i) data collection, (ii) data processing, as detailed below:\\nData Collection: During the collection phase, we carefully considered the diversity of medical images and the complexity of\\nthe tasks, selecting appropriate subsets for comprehension and generation tasks. For comprehension tasks, we selected datasets\\nsuch as VQA-RAD (Lau et al. 2018), SLAKE (Liu et al. 2021), PathVQA (He et al. 2020), and MIMIC-CXR-VQA (Bae et al.\\n2024), which cover various medical imaging modalities like radiology and pathology, and include professional annotations\\nto assist the model in learning tasks such as lesion detection and disease diagnosis. Additionally, large-scale multi-modal\\ndatasets like LLaV A-Med (Li et al. 2024b) and PubMedVision (Chen et al. 2024a) were included to provide broader medical\\nknowledge support and facilitate the training of complex reasoning tasks. For generation tasks, we focused on four mainstream\\ntask categories: super-resolution image generation, modality conversion, text-to-image generation, and image reconstruction.\\nThe IXI (Davies et al. 2014) dataset, containing a large number of healthy brain MRI images, is suitable for training super-\\nresolution models; the MIMIC-CHEST-XRAY (Bae et al. 2024) dataset, with X-ray images and their corresponding textual\\nreports, is appropriate for text-to-image generation tasks; the SynthRAD2023 (Thummerer et al. 2023) dataset provides a large\\nnumber of paired CT and MRI images, supporting modality conversion model training; for image reconstruction tasks, we\\nrewrote and adjusted the LLaV A-558k (Liu et al. 2024b) dataset.\\nData Processing: After data collection, we performed filtering and processing of the raw data. For VisualQA tasks, we stan-\\ndardized the data entries into two forms: open-ended questions and single-choice questions, enabling flexible training and\\nevaluation. Additionally, considering that multi-image data has a minimal impact on performance but introduces extra padding\\nand training time, we excluded multi-image data. For the scanned image data in generation tasks, we applied slicing extrac-\\ntion, image registration, data augmentation, and normalization to treat 2D images as visual inputs for model training or used\\nVQGAN-generated indices to supervise the generation tasks.\\nData Statistics This section provides detailed statistical information about the VL-Health dataset to offer a more compre-\\nhensive understanding.\\nData Overview: To ensure a balanced development of the model’s comprehension and generation capabilities, in addition\\nto the LLaV A-558k and PubMedVision-PT datasets used for alignment, the VL-Health dataset ultimately selected 765,802\\nadditional visual question-answering (VQA) training samples (to endow the model with visual comprehension and instruction-\\nfollowing capabilities) and 783,045 generation training samples (to provide the model with reconstruction and visual generation\\ninstruction-following abilities). This contributes to the transfer of knowledge between comprehension and generation tasks, en-\\nhancing the model’s overall performance. For medical image comprehension tasks, images were selected from VQA-RAD (ap-\\nproximately 450 images), SLAKE (approximately 630 images), PathVQA (approximately 2,600 images), MIMIC-CXR-VQA\\n(approximately 52,000 images), LLaV A-Med (approximately 61,000 images), and PubMedVision (approximately 500,000 im-\\nages). Multiple question-answer pairs were retained for each image to enhance the model’s understanding and generalization\\nof the image content. Table 8 shows the data distribution ofVL-Health for three-stage learning strategy, where mixed-47k is\\nbased on the sampling of all data in stage-1.\\nDiversity and Quality Assessment: VL-Health covers 11 modalities, including CT, MRI, X-ray, microscopy, OCT, ultra-\\nsound, and fundus photography, which aids the model in learning features from various modalities. The dataset also encom-\\npasses a wide range of diseases, from common to rare, and from localized lesions to systemic diseases, including pulmonary\\ndiseases, skeletal abnormalities, brain lesions, tumors, cardiovascular diseases, and cellular abnormalities. This provides com-\\nprehensive training support to the model, enabling it to learn the characteristics and diagnosis of various diseases.\\n13', mimetype='text/plain', start_char_idx=0, end_char_idx=4428, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='832e70cf-3654-4416-8db3-8b0235b1488d', embedding=None, metadata={'page_label': '14', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"VL-Health Dataset: Leveraging Heterogeneous Low-Rank Adaptation (H-LoRA) for Enhanced Medical Task Learning\"', 'questions_this_excerpt_can_answer': \"Based on the provided excerpt from the document, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What are the key components of the VL-Health dataset used for training and evaluation in medical tasks, and how are they structured?**\\n   - This question focuses on the specific elements that make up the VL-Health dataset, including task type, task instruction, response, input image, and target image index, as detailed in the excerpt.\\n\\n2. **How does the H-LoRA algorithm differ from MoELoRA in terms of computational overhead, and what are the specific operational steps involved in MoELoRA?**\\n   - This question seeks to understand the differences in computational efficiency between the two algorithms, specifically looking at the operational steps that contribute to MoELoRA's time complexity.\\n\\n3. **What is the process for selecting task-specific image features in the H-LoRA algorithm, and how does it determine whether to use concrete-grained or abstract-grained visual features?**\\n   - This question addresses the decision-making process within the H-LoRA algorithm regarding the selection of image features based on the type of task (generation or comprehension), as outlined in the pseudocode provided.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='1687014f-48f1-44a0-81b3-fdc19b576877', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '14', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='38b6cbe2c8187edb0ecc47e65c4756aadc2286621de293b9e0d7673be0cd8cf6')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Table 8: Data distribution of VL-Health in three-stage learning strategy.\\nMedical Task Stage-1 Stage-2\\nComp. LLaV A-558k, PubMedVision-PT Mixed-47kGen. LLaV A-558k\\nMedical Task Stage-3\\nComp. LLaV A Med, MIMIC CXR VQA, PubMedVision-FT, LLaV A-665k, PathVQA, SLAKE, VQA-RAD\\nGen. IXI, SynthRAD2023, MIMIC-CHEST-XRAY\\nData Format. All data samples are converted into a unified instruction-response format for training and evaluation. Specifi-\\ncally, the VL-Health dataset consists of the following components:\\n• Task Type: Specifies the granularity of visual features output by the visual encoder and selects the corresponding H-\\nLoRA submodule. For generation tasks, the response also includes multi-modal tokens corresponding to VQ indices.\\n• Task Instruction: Guides the model to interpret the image and generate a response, covering various aspects of the\\nimage and specifying the output format.\\n• Response: The textual output generated based on the task instruction and input image, ensuring it meets the question and\\nformatting requirements.\\n• Input Image: Provides the visual signal for the model to process.\\n• Target Image Index: In generation tasks, this is added as a multi-modal token to the response for autoregressive\\ngeneration.\\nB Analysis of Heterogeneous Low-Rank Adaptation\\nWe propose H-LoRA, which utilizes hard routing selection to allocate plugins for knowledge learning and representation across\\ntasks, thereby preventing conflicts arising from heterogeneous knowledge. Furthermore, within each task, we optimized based\\non MoELoRA, enhancing performance while reducing computational overhead. The pseudocode is detailed Algorithm 1.\\nAlgorithm 1: H-LoRA Algorithm\\nInput: concrete-grained visual features FCon, abstract-grained visual featuresFAbs, comprehension-based H-LoRA modules\\n({AComp.\\ni }k\\ni=1, RComp.\\nouter ), generation-based H-LoRA modules({AGen.\\ni }k\\ni=1, RGen.\\nouter), task type T (comprehension or generation),\\nnumber of LoRA experts k, origin linear layer weights W0, text features T , hidden state h\\nOutput: final output O\\n// Select task-specific image features\\nif T = generation task then\\nFimg ← FCon\\nelse if T = comprehension task then\\nFimg ← FAbs\\nend if\\nU ←concat(Fimg, T ) // Concatenate image features and text features\\n{Ai}k\\ni=1, {Bi}k\\ni=1, Router ← {AT\\ni }k\\ni=1, {BT\\ni }k\\ni=1, RT\\nouter // Assign task-specific H-LoRA submodule\\n// Merge LoRA experts’ matrices\\nAmerged ← concat({Ai}k\\ni=1)\\nBmerged ← concat({Bi}k\\ni=1)\\nW ←R(h) // Generate routing weights based on input hidden state x\\nWexpanded ← α × W/r ⊗ 1r // Expand routing weights to match merged matrices\\nOH-LoRA ← (x · Amerged ⊙ Wexpanded) · Bmerged // Compute H-LoRA output using element-wise multiplication\\nO ←x · W0 + OH-LoRA // Add H-LoRA output to pre-trained weights to get final output\\nReturn O\\nWe further analyzed the computational overhead differences between MoELoRA and H-LoRA. Assuming that both methods\\nuse the same number of LoRA experts k, we can compare their time complexity from the perspective of the operational steps\\ninvolved.\\nComputational Overhead of MoELoRA. In MoELoRA, the operations involving the expert matrix mainly include the fol-\\nlowing steps: (i) Expert Multiplication : MoELoRA requires 2k multiplications with the LoRA experts. (ii) Router Multi-\\nplication: One multiplication with the Router is required. (iii) Router Output Expansion : MoELoRA needs to perform k\\n14', mimetype='text/plain', start_char_idx=0, end_char_idx=3392, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='87bb77f8-1a07-4dfc-9884-278bb59e47ab', embedding=None, metadata={'page_label': '15', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comparative Performance and Efficiency Analysis of MoELoRA and H-LoRA in Medical Visual Question Answering on the OmniMedVQA Benchmark\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What is the additional time complexity introduced by MoELoRA when equipping a fully connected layer, and how does it compare to H-LoRA?**\\n   - The additional time complexity introduced by MoELoRA is O(5k + 1), while H-LoRA has a fixed additional time complexity of O(6), independent of the number of experts k.\\n\\n2. **How does H-LoRA achieve computational efficiency compared to MoELoRA in terms of operations required?**\\n   - H-LoRA achieves computational efficiency by merging all LoRA experts into larger A and B matrices, requiring only 2 multiplications with the LoRA experts, one multiplication with the Router, one expansion operation, one dot product, and one addition operation, resulting in a total of O(6) operations.\\n\\n3. **What is the significance of the OmniMedVQA benchmark in the context of the experiments discussed in the document?**\\n   - The OmniMedVQA benchmark is a large-scale medical visual question answering benchmark designed to encompass various modalities and anatomical regions, providing a diverse set of images from multiple medical datasets, which is crucial for demonstrating the performance of models like HealthGPT in the experiments.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2176f658-0034-4503-9b51-710dc7a262dc', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '15', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='90f15d34b6950f804585e9248980858d68bcb24cfadc548a1dbca909e288c452'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='c212d7a2-f89c-4b0c-909d-31e54166f722', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='58d1ff3c3cfe023af0d26cd47f14ebf43ecc9baeef546ef02dd04217044974ac')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='expansion operations on the Router’s output weights to generate the appropriate shapes that match the dimensions of the in-\\nput and LoRA experts while iterating through the experts. (iv) Dot Product: For each expanded Router weight, a dot product\\nwith the intermediate state of the expert is required, resulting in k multiplications. (v) Addition: Finally, k addition operations\\nare required to accumulate the results from each LoRA expert into the final output. Assuming the time complexity of each\\noperation is the same, the additional time complexity introduced when equipping a fully connected layer with MoELoRA is:\\nO(2k+1+ k+k+k) = O(5k+1). Thus, MoELoRA introduces an additional time overhead ofO(5k+1) during computation.\\nH-LoRA. In contrast to MoELoRA, H-LoRA reduces the computational overhead by concatenating the LoRA expert matrices.\\nSpecifically: (i) Expert Multiplication: H-LoRA merges all LoRA experts by directly creating a larger A and B matrix, instead\\nof performing independent operations for each expert. This process can be implemented through matrix initialization without\\nadditional concatenation operations. Therefore, only 2 multiplications with the LoRA experts are required. (ii) Router Multi-\\nplication: H-LoRA still requires one multiplication with the Router. (iii) Router Output Expansion : H-LoRA only requires\\none expansion operation on the Router’s output weights. (iv) Dot Product: H-LoRA only requires one dot product between\\nthe Router’s output and the expert’s intermediate state. (v) Addition: Finally, H-LoRA only requires one addition operation\\nto accumulate the LoRA expert results into the intermediate state. Therefore, the additional time complexity introduced by\\nH-LoRA is: O(2 + 1 + 1 + 1 + 1) =O(6).\\nComparing the two, we see that MoELoRA introduces a linear increase in additional time complexity with respect to the\\nnumber of experts k, resulting in a complexity of O(5k + 1), while H-LoRA’s additional time complexity is fixed at O(6),\\nindependent of k. We observe that when k is small, the time complexity differences between MoELoRA and H-LoRA are\\nnegligible. However, as k increases, MoELoRA’s computational overhead grows linearly, while H-LoRA’s remains constant.\\nThis makes H-LoRA significantly more computationally efficient than MoELoRA, particularly in large-scale tasks. We will\\nfurther demonstrate the significant advantage of H-LoRA in training time in subsequent experiments, validating its efficiency\\nin practical applications.\\nC Supplemental Experimental Results\\nIn this section, we include additional experiments to demonstrate the superiority of HealthGPT and articulate our design\\nphilosophy.\\nC.1 Results: OmniMedVQA Benchmark\\nOmniMedVQA (Hu et al. 2024) is a novel, large-scale medical visual question answering (VQA) benchmark designed to\\nencompass various modalities and anatomical regions by collecting diverse images from multiple medical datasets. Our exper-\\nimental results are presented in Table 9.\\nTable 9: Performance comparison of OmniMedVQA Benchmark.\\nOmniMedVQA↑Type Model # Params Medical\\nLVLM CT X-ray FDM MiS OCT MRI USS Avg.\\nComp.', mimetype='text/plain', start_char_idx=0, end_char_idx=3118, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='c212d7a2-f89c-4b0c-909d-31e54166f722', embedding=None, metadata={'page_label': '15', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comparative Performance and Efficiency Analysis of MoELoRA and H-LoRA in Medical Visual Question Answering on the OmniMedVQA Benchmark\"', 'questions_this_excerpt_can_answer': \"Based on the provided excerpt, here are three specific questions that can be answered using the information contained within the context:\\n\\n1. **Which model achieved the highest average score in the OmniMedVQA Benchmark, and how many sub-tasks did it outperform other models in?**\\n   - Answer: HealthGPT-M3 achieved the highest average score in the OmniMedVQA Benchmark, outperforming other models in 4 out of 7 sub-tasks.\\n\\n2. **What are the performance scores of the model HealthGPT-M3 across the different medical imaging modalities listed in the OmniMedVQA Benchmark?**\\n   - Answer: The performance scores of HealthGPT-M3 across the different medical imaging modalities are as follows: CT - 35.3, X-ray - 81.9, FDM - 54.6, MiS - 88.2, OCT - 89.3, MRI - 78.5, USS - 51.4, with an average score of 68.5.\\n\\n3. **How does the performance of the model Llama-3.2 compare to that of HealthGPT-M3 in the OmniMedVQA Benchmark?**\\n   - Answer: Llama-3.2 has an average score of 63.2, which is lower than HealthGPT-M3's average score of 68.5, indicating that HealthGPT-M3 outperforms Llama-3.2 in the OmniMedVQA Benchmark.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2176f658-0034-4503-9b51-710dc7a262dc', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '15', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='90f15d34b6950f804585e9248980858d68bcb24cfadc548a1dbca909e288c452'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='87bb77f8-1a07-4dfc-9884-278bb59e47ab', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '15', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='fefe8ef4901963a84e562c4fdd08bc7b3558ab8aa41ad72db3388e9cbfcac599')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='C.1 Results: OmniMedVQA Benchmark\\nOmniMedVQA (Hu et al. 2024) is a novel, large-scale medical visual question answering (VQA) benchmark designed to\\nencompass various modalities and anatomical regions by collecting diverse images from multiple medical datasets. Our exper-\\nimental results are presented in Table 9.\\nTable 9: Performance comparison of OmniMedVQA Benchmark.\\nOmniMedVQA↑Type Model # Params Medical\\nLVLM CT X-ray FDM MiS OCT MRI USS Avg.\\nComp. Only\\nMed-Flamingo 8.3B ✓ 30.1 33.9 25.5 37.0 60.0 27.6 30.4 34.9\\nLLaV A-Med 7B ✓ 28.4 32.8 42.7 31.6 55.3 45.0 53.6 41.3\\nHuatuoGPT-Vision 7B ✓ 35.3 41.5 51.4 62.3 59.3 40.4 60.1 50.0\\nBLIP-2 6.7B ✗ 26.6 29.1 22.3 36.9 29.1 22.7 21.4 26.9\\nLLaV A-v1.5 7B ✗ 28.0 55.7 35.5 42.1 49.2 52.9 49.7 44.7\\nInstructBLIP 7B ✗ 20.1 22.2 34.1 30.6 38.6 31.9 25.5 29.0\\nYi-VL 6B ✗ 51.2 47.1 27.7 62.6 67.6 55.0 40.3 50.2\\nInternVL2 8B ✗ 40.2 57.9 53.2 64.0 59.1 58.1 49.1 54.5\\nLlama-3.2 11B ✗ 37.6 55.2 71.4 82.1 62.5 65.2 68.6 63.2\\nComp. & Gen.\\nShow-o 1.3B ✗ 29.0 50.4 30.9 22.0 30.8 34.2 33.8 33.0\\nUnified-IO 2 7B ✗ 10.8 37.7 12.3 25.3 32.6 30.9 37.7 26.8\\nJanus 1.3B ✗ 24.9 54.8 35.9 62.7 54.2 50.7 36.8 45.7\\nHealthGPT-M3 3.8B ✓ 35.3 81.9 54.6 88.2 89.3 78.5 51.4 68.5\\nHealthGPT-L14 14B ✓ 39.0 86.6 64.1 88.6 99.7 80.9 62.2 74.4\\nThrough our analysis, we make the following observations: (i) HealthGPT-M3 outperforms other models in 4 out of 7\\nsub-tasks, achieving an average score that exceeds cutting-edge medical Large Vision-Language Models (LVLMs) as well as\\n15', mimetype='text/plain', start_char_idx=2664, end_char_idx=4167, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='40620a2d-57c6-44a4-84de-26e07a24bae2', embedding=None, metadata={'page_label': '16', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comprehensive Performance Analysis and Evaluation of HealthGPT-L14 and H-LoRA in Medical Visual Language Models\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What are the training time implications of using MoELoRA compared to H-LoRA when increasing the number of LoRA experts?**\\n   - The excerpt details how the training time for MoELoRA significantly increases with the number of experts, particularly noting that at n = 8, the training time is twice that of LoRA, and at n = 32, it could reach eight times that of LoRA, while H-LoRA does not incur additional training delays.\\n\\n2. **How does the three-phase learning strategy proposed for H-LoRA address performance degradation in traditional unified models?**\\n   - The context explains that the three-phase learning strategy, particularly the second phase (Heterogeneous H-LoRA Plugin Adaptation), integrates LLMs with different H-LoRA plugins, effectively mitigating inter-task conflicts and unifying the model with minimal impact on overall performance.\\n\\n3. **What were the findings of the human evaluation conducted on the VQA-RAD, SLAKE, and PathVQA benchmarks regarding the performance of HealthGPT-L14 compared to other models?**\\n   - The excerpt mentions that during the human evaluation, HealthGPT was frequently selected as the best answer among the responses generated by various models, indicating its strong performance and potential application in medical care scenarios.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='28be5ce0-dc80-4d50-b31a-6d36ce4d47d8', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '16', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='1b500f052da9207a8bac34a9c2657baf51758b58294da27d606b111b4bb77f80')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='general LVLMs; (ii) the unified model demonstrates relatively weak performance on OmniMedVQA; however, our approach\\neffectively mitigates performance degradation caused by generation tasks, serving as a unified model; (iii) HealthGPT-L14\\nexcels across all sub-tasks, achieving optimal or near-optimal results with an average score of 74.4, significantly surpassing\\nother models.\\nC.2 Stability Analysis of Number of Experts\\nWe investigated the impact of the number of LoRA experts on model performance within a multi-LoRA architecture, conducting\\nextensive experiments on MoELoRA and H-LoRA with varying numbers of experts. The experimental results are presented\\nin Table 10. As the number of experts increases, the training time for MoELoRA is significantly prolonged. When n = 8 ,\\nthe training time for MoELoRA is twice that of LoRA, whereas H-LoRA incurs no additional training delay and performs\\nbetter. It is estimated that at n = 32, the training time for MoELoRA could reach eight times that of LoRA, preventing it from\\ncompleting training and inference. This result aligns with the analysis in Appendix B, indicating that H-LoRA not only avoids\\nintroducing additional training delays compared to LoRA but also outperforms MoELoRA.\\nTable 10: We explored the performance of MoELoRA and H-LoRA with different numbers of LoRA experts. At n = 32 ,\\nMoELoRA was unable to complete training.\\nn=2 n=4 n=8 n=32Model Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time Comp. Gen. Time\\n+MoELoRA 50.3 62.98 1.22 × 50.0 64.33 1.49 × 50.8 63.71 2.09 × / / 5.81 ×HealthGPT w/ +H-LoRA 51.5 63.48 0.99× 52.8 64.71 1.00× 53.6 64.98 0.99× 53.5 64.74 1.01×\\nC.3 Impact of Heterogeneous Knowledge Fusion on Performance\\nTraditional unified models often utilize mixed training methods, which may result in performance degradation due to variations\\nin task modes. To address this, we propose a three-phase learning strategy to support H-LoRA, effectively mitigating inter-task\\nconflicts. Specifically, the second phase (Heterogeneous H-LoRA Plugin Adaptation) integrates LLMs with different H-LoRA\\nplugins into a new unified foundation by mixing the training of the embedding layers and output heads for two tasks. Figure\\n9 illustrates the impact of this phase on the performance of medical comprehension and generation tasks. We observe that the\\nsecond phase effectively unifies the model with minimal impact on overall performance, significantly alleviating the conflict\\nissues arising from mixed training in medical scenarios.\\nC.4 Human Evaluation.\\n65.7 65.4 67.7 67.0\\nFigure 9: Performance changes before and after the\\nstage-2.\\nWe further conduct human evaluation on the VQA-RAD, SLAKE,\\nand PathVQA benchmarks, which contain 1,000 open-ended ques-\\ntions. Specifically, we recruit 5 clinicians to rank the randomly shuf-\\nfled responses from HealthGPT-L14, LLaV A-Med, HuatuoGPT-\\nVision, Llama-3.2, InternVL-2 and Show-o. During the evaluation,\\nquestions were randomly selected, and the model-generated responses\\nwere anonymized and ranked. The results, as shown in Figure 10, in-\\ndicate that HealthGPT was frequently selected as the best answer.\\nThis suggests that HealthGPT has further application potential in\\nmedical care scenarios.\\nC.5 Reconstruction Performance\\nCurrently, unified models that align visual features based on recon-\\nstruction tasks include pre-LVLMs, post-LVLMs, as well as Unified-\\nIO 2 (Lu et al. 2024) and SEED-X (Ge et al. 2024). To investigate the\\ncontrollability of visual generation in rigorous settings such as med-\\nical contexts, we evaluated the performance of these models in med-\\nical image reconstruction in Table 11. Experimental results demon-\\nstrate that HealthGPT exhibits the most stable reconstruction per-\\nformance with a small amount of data.\\n16', mimetype='text/plain', start_char_idx=0, end_char_idx=3766, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='5ecdc967-6fef-4807-895e-0d25ff7b70c7', embedding=None, metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comparative Analysis of Modality Transformation and Super-Resolution Reconstruction in Medical Imaging: Performance Metrics and Human Evaluation\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"Comparative Analysis of Modality Transformation and Super-Resolution Reconstruction in Medical Imaging,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are the performance metrics (SSIM, PSNR, MSE) for the HealthGPT-M3 model in the MRI (Brain) reconstruction task?**\\n   - This question targets the specific performance metrics for a particular model and task, which is detailed in the table provided in the excerpt.\\n\\n2. **How does the performance of the Unified-IO model compare to the SEED-X model in the CT (Pelvis) reconstruction task based on the provided metrics?**\\n   - This question focuses on a comparative analysis of two specific models for a particular task, which is explicitly mentioned in the experimental results.\\n\\n3. **What visual evidence is provided in Figures 11 and 12 to support the effectiveness of the proposed method in modality transformation and super-resolution reconstruction?**\\n   - This question seeks to understand the qualitative results illustrated in the figures referenced in the excerpt, which are crucial for evaluating the method\\'s effectiveness.\\n\\nThese questions are tailored to extract specific information from the excerpt that is unlikely to be found in other sections of the document or in different sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5544de1c-168b-4191-8aa5-cd5eb4cdd619', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='69b2a2d7bcc5d54d432fdb86523011f295e2dc2163aeb89b637895733be12461')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='C.6 Case Study\\nFigures 11 and 12 illustrate examples of modality transformation and super-resolution reconstruction. In Figure 11, the results\\ngenerated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively\\nguiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure 12\\ndemonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the\\nimage.\\nTable 11: The experimental results for the four reconstruction tasks.\\nCT(Brain) CT(Pelvis) MRI (Brain) MRI(Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\nSEED-X 20.18 27.66 112.11 21.53 28.02 102.87 4.90 27.62 112.86 6.31 27.89 106.21\\nUnified-IO 2 83.93 36.09 17.95 85.36 35.10 25.46 87.50 34.25 25.47 86.31 33.53 29.80\\nHealthGPT-M3 91.73 36.42 15.46 94.26 37.30 12.53 88.76 33.97 27.05 84.40 33.11 32.62\\n34.08\\n15.87\\n9.16\\n5.62\\n13.33\\n21.94\\nHealthGPT\\nLLaVA-Med\\nHuatuoGPT-Vision\\nLlama-3.2\\nInternVL-2\\nShow-o\\nHuman\\nEvaluation\\n(a) (b)\\nFigure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.\\n17', mimetype='text/plain', start_char_idx=0, end_char_idx=1208, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='273e50ab-728c-4eed-98ad-cc1856eb0845', embedding=None, metadata={'page_label': '18', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Exploring Modality Transfer: Key Entities, Themes, and Implications\"', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are three specific questions that can be answered using the excerpt:\\n\\n1. **What is the title of the document referenced in the excerpt?**\\n   - The document title is \"Exploring Modality Transfer: Key Entities, Themes, and Implications.\"\\n\\n2. **What is the significance of Figure 11 in the document?**\\n   - Figure 11 illustrates a case of modality transfer, which is a key concept discussed in the document.\\n\\n3. **What is the file size of the PDF document mentioned in the context?**\\n   - The file size of the document is 8,786,923 bytes.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0f6b5fc1-8392-44dd-84e8-b46c060254fc', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '18', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='6e9d9973d4bd846901886f09653cc74a85c513d5b60beaae5e50bcf6ce6b7e8a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 11: Case of modality transfer.\\n18', mimetype='text/plain', start_char_idx=0, end_char_idx=40, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='7e8d4f3a-51e7-44a0-8530-f3de276e94f5', embedding=None, metadata={'page_label': '19', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Enhancing MRI Image Quality Through Advanced Super-Resolution Techniques: A Comprehensive Study\"', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are three specific questions that can be answered using the information given:\\n\\n1. **What is the title of the document that includes Figure 12, which illustrates a case of MRI image super-resolution?**\\n   - Answer: The title of the document is \"Enhancing MRI Image Quality Through Advanced Super-Resolution Techniques: A Comprehensive Study.\"\\n\\n2. **What is the page number where Figure 12, depicting a case of MRI image super-resolution, can be found in the document?**\\n   - Answer: Figure 12 can be found on page 19 of the document.\\n\\n3. **What is the file size of the PDF document titled \"Enhancing MRI Image Quality Through Advanced Super-Resolution Techniques: A Comprehensive Study\"?**\\n   - Answer: The file size of the document is 8,786,923 bytes.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c9b83d92-ae76-40fe-8cd8-4fe94bd0b932', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '19', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='e2216e5b655d78cb78c4c3f348e96cf428dc61326b6da58c01cf05da93386acb')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Figure 12: Case of MRI image super-resolution.\\n19', mimetype='text/plain', start_char_idx=0, end_char_idx=49, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='cabfcff5-9ceb-49e3-9d56-18c60081fbab', embedding=None, metadata={'page_label': '1', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Exploring Diverse Themes and Unique Entities: A Comprehensive Analysis\"', 'questions_this_excerpt_can_answer': 'Based on the provided context, here are three specific questions that can be answered using the information given:\\n\\n1. **What is the file name and type of the document related to WSO2 LLC\\'s Anti-Corruption Policy?**\\n   - This question can be answered specifically by referencing the details in the context, which states the file name as \"WSO2 LLC - Anti-Corruption Policy .docx.pdf\" and the file type as \"application/pdf.\"\\n\\n2. **When was the document titled \"Exploring Diverse Themes and Unique Entities: A Comprehensive Analysis\" created and last modified?**\\n   - The context provides the creation date and last modified date as September 1, 2025, allowing for a precise answer to this question.\\n\\n3. **What is the file size of the WSO2 LLC Anti-Corruption Policy document?**\\n   - The context specifies that the file size is 503,440 bytes, which directly answers this question. \\n\\nThese questions focus on specific details that are unique to the provided context and are unlikely to be found in other documents or sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a34239c3-9cb7-4c92-91a7-112e1434a421', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='256d03a76cf23a22d64e10260c686ee5a6f4190572c36cb3a55f2afed78de4fc')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='1', mimetype='text/plain', start_char_idx=2, end_char_idx=3, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='2eb9f22b-1514-42ff-a51b-e0b5d28fae3e', embedding=None, metadata={'page_label': '2', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"WSO2 LLC Anti-Corruption Policy: Overview, Implementation Guidelines, and Document History\"', 'questions_this_excerpt_can_answer': 'Based on the provided context regarding the \"WSO2 LLC - Anti-Corruption Policy,\" here are three specific questions that can be answered using the information in the excerpt:\\n\\n1. **What is the classification level and asset owner of the WSO2 LLC Anti-Corruption Policy?**\\n   - Answer: The classification level is \"Internal,\" and the asset owner is the \"Legal\" team.\\n\\n2. **Who were the authors of the initial version of the Anti-Corruption Policy, and when was it created?**\\n   - Answer: The initial version (V1.0) was authored by the Legal Team and External Counsel (Cooley LLP) on 28/07/2022.\\n\\n3. **What was the outcome of the review conducted by the Legal Team on 19/10/2023 regarding the Anti-Corruption Policy?**\\n   - Answer: The Legal Team reviewed the policy on 19/10/2023 and determined that there were no changes needed.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d0b1409e-b4ad-40a9-9198-000eecc769f9', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '2', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='c0113287e78da8fcebdbffc3f2cd7cd3dc8402329fca70979a3c71917a1c97c0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only \\nVersion  control     Classification  Level:   Internal   Asset  Owner:   Legal     Asset:     WSO2  LLC  -  Anti-Corruption  Policy     Document  History:  \\nDate  Revision  Author(s)  Description  Reviewed  &  Approved  By  28/07/2022  V1.0  \\nLegal  Team  and  External  Counsel  (Cooley  LLP)   \\nInitial  Version  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n19/10/2023  V1.0  \\nLegal  Team  Reviewed.  No  changes  \\nPuny  Navaratne  (Vice  President  -  Legal  Affairs)  \\n \\n2', mimetype='text/plain', start_char_idx=0, end_char_idx=542, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='c86db92d-ffb4-47e0-b114-6de7ed6ba46a', embedding=None, metadata={'page_label': '3', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comprehensive Anti-Corruption Policy Framework: Guidelines, Prohibitions, and Legal Compliance Standards\"', 'questions_this_excerpt_can_answer': 'Based on the provided context from the \"Comprehensive Anti-Corruption Policy Framework,\" here are three specific questions that can be answered using the information in the excerpt:\\n\\n1. **What are the key components outlined in the Anti-Corruption Policy of WSO2 LLC?**\\n   - This question can be answered by referring to the contents listed in the excerpt, which includes sections such as \"Purpose,\" \"Policy Statements,\" \"Anti-Bribery Prohibitions,\" and others.\\n\\n2. **Which countries\\' anti-corruption laws are referenced in the attachment of the policy document?**\\n   - The excerpt mentions specific anti-corruption laws from the United Kingdom, Sri Lanka, and Brazil, including the UK Bribery Act 2010 and the Brazilian Anti-Corruption Act 2013.\\n\\n3. **What is the significance of the section on \"Violations and Consequences\" in the context of the Anti-Corruption Policy?**\\n   - This question can be addressed by exploring the implications and potential repercussions outlined in that specific section, which is crucial for understanding the enforcement of the policy and the seriousness of compliance.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='eb2ae63c-c316-4974-bb5f-9b7b9ca8097b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '3', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='63b62bf91a7dee393a337a76bb192153f845737b711dfde59d2d37744c6bf9c0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only  \\nContents  I.   PURPOSE \\nII.   POLICY  STATEMENTS \\nIII.   ANTI-BRIBERY  PROHIBITIONS \\nIV.   ACCOUNTING  REQUIREMENTS \\nV.   FACILITATION  PAYMENTS \\nVI.   INTERMEDIARIES  AND  BUSINESS  PARTNERS \\nVII.   GIFTS  AND  HOSPITALITIES \\nIX.   OTHER  ACTIVITIES \\nX.   VIOLATIONS  AND  CONSEQUENCES \\nXI.   TRAINING  AND  CERTIFICATION \\nXII.   STATUS \\nXIII.   REPORTING/QUESTIONS \\nIX.   ACKNOWLEDGEMENT   \\nATTACHMENT  1:  ANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL \\n●  UNITED  KINGDOM ●  THE  UK  BRIBERY  ACT  2010 ●  SRI  LANKA ●  BRAZIL o  THE  BRAZILIAN  ANTI  CORRUPTION  ACT  2013 o  THE  BRAZILIAN  IMPROBITY  ACT  1992 o  OTHER  POTENTIAL  LIABILITIES  \\n \\n3', mimetype='text/plain', start_char_idx=0, end_char_idx=739, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='2b2912cb-275c-44a6-ab01-97eb8d7bf8bf', embedding=None, metadata={'page_label': '4', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"WSO2 Anti-Corruption and Anti-Bribery Policy: Compliance Standards, Prohibitions, and Global Guidelines\"', 'questions_this_excerpt_can_answer': 'Based on the provided context from the \"WSO2 Anti-Corruption and Anti-Bribery Policy,\" here are three specific questions that can be answered:\\n\\n1. **What specific anti-corruption laws does WSO2 LLC\\'s Anti-Corruption Policy aim to ensure compliance with?**\\n   - The policy outlines compliance with several laws, including the U.S. Foreign Corrupt Practices Act, the UK Bribery Act 2010, and the Brazilian Anti-Corruption Act, among others.\\n\\n2. **Who is required to adhere to the Anti-Corruption Policy at WSO2 LLC?**\\n   - The policy applies to all worldwide directors, officers, employees, and individuals serving as independent contractors of WSO2, collectively referred to as \"WSO2 Personnel.\"\\n\\n3. **What actions are strictly prohibited for WSO2 Personnel under the Anti-Corruption Policy?**\\n   - WSO2 Personnel are strictly prohibited from promising, offering, providing, or authorizing cash payments or anything of value to achieve an improper purpose, as well as from requesting or accepting such payments from any person for the same reasons.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9aa353a0-cc99-4018-99e9-5d5d27bce189', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='3c238819097bd2b814dff417abedfd213f4dfdd3715ea64b2098f4bdccd9b851'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ebd3bda1-2adb-48ce-8433-7a11079d3c46', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='6e3f3467425301475fffeb4b16d8670d2190cd4e1dc528c12e816c2564155438')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only WSO2\\n \\nLLC   A\\nNTI\\n-C\\nORRUPTION\\n \\nP\\nOLICY\\n  \\nA\\nPPROVED\\n \\nBY\\n \\nTHE\\n \\nB\\nOARD\\n \\nOF\\n \\nD\\nIRECTORS\\n  JULY  12,  2022   \\n  \\nI.   P\\nURPOSE\\n \\n \\nWSO2  LLC  (together  with  its  worldwide  subsidiaries,  “ WSO2 ”  or  the  “ Company ”)  has  implemented  this  \\nAnti-Corruption\\n \\nPolicy\\n \\n(the\\n \\n“\\nPolicy\\n”)\\n \\nfor\\n \\nthe\\n \\npurpose\\n \\nof\\n \\nensuring\\n \\ncompliance\\n \\nwith\\n \\nthe\\n \\nU.S.\\n \\nForeign\\n \\nCorrupt\\n \\nPractices\\n \\nAct\\n \\nof\\n \\n1977,\\n \\nas\\n \\namended\\n \\n(the\\n \\n“\\nFCPA\\n”),\\n \\nthe\\n \\nU.S.\\n \\nTravel\\n \\nAct,\\n \\nthe\\n \\nU.S.\\n \\nDomestic\\n \\nBribery\\n \\nStatute,\\n \\nthe\\n \\nUK\\n \\nBribery\\n \\nAct\\n \\n2010,\\n \\nthe\\n \\nSri\\n \\nLankan\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments),\\n \\nthe\\n \\nBrazilian\\n \\nAnti-corruption\\n \\nAct\\n \\n(Law\\n \\nNo.\\n \\n12,846/2013),\\n \\nthe\\n \\nBrazilian\\n \\nImprobity\\n \\nAct\\n \\n1992\\n \\n(Law\\n \\nNo.\\n \\n8.429/1992),\\n \\nand\\n  \\nall\\n  \\nother\\n  \\nanti-corruption\\n  \\nlaws\\n  \\nand\\n  \\nregulations\\n  \\napplicable\\n  \\nto\\n  \\nWSO2’s\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(collectively,\\n \\n“\\nAnti-Corruption\\n \\nLaws\\n”).\\n  \\nThis\\n \\nPolicy\\n \\napplies\\n \\nto\\n \\nall\\n \\nworld-wide\\n \\ndirectors,\\n \\nofficers,\\n \\nemployees,\\n \\nand\\n \\nindividuals\\n \\nserving\\n \\nas\\n \\nindependent\\n \\ncontractors\\n \\nof\\n \\nWSO2\\n \\n(collectively,\\n \\n“\\nWSO2\\n \\nPersonnel\\n”)\\n \\nto\\n \\ncomply\\n \\nwith\\n \\nthe\\n \\nprinciples\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy.\\n  \\nPlease\\n \\nreport\\n \\nall\\n \\nquestions\\n \\nor\\n \\nconcerns\\n \\nto\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nwhose\\n \\ncontact\\n \\ninformation\\n \\nappears\\n \\nbelow.\\n \\n \\nII.   P\\nOLICY\\n \\nS\\nTATEMENTS\\n \\n \\nWSO2  Personnel  are  strictly  prohibited  from  promising,  offering,  providing,  or  authorizing  cash  \\npayments\\n \\n(such\\n \\nas\\n \\nbribes\\n \\nor\\n \\nkickbacks)\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nalso\\n \\nstrictly\\n \\nprohibited\\n \\nfrom\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nelse\\n \\nof\\n \\nvalue\\n \\nfrom\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose\\n \\nrelated\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n    \\n \\nWSO2  Personnel  must  comply  with  all  of  the  Company’s  internal  controls,  especially  those  designed  \\nto\\n \\nensure\\n \\naccurate\\n \\nand\\n \\ncomplete\\n \\nbooks\\n \\nand\\n \\nrecords,\\n \\nor\\n \\notherwise\\n \\nprevent\\n \\ncorruption,\\n \\nself-dealing,\\n \\nembezzlement,\\n \\nfraud,\\n \\nmoney\\n \\nlaundering,\\n \\nor\\n \\nother\\n \\nimproper\\n \\nactivities.\\n \\n \\nThere  are  no  exceptions  to  this  Policy,  even  if  our  competitors  engage  in  improper  behavior  or  corruption  \\nis\\n \\nan\\n \\naccepted\\n \\npractice\\n \\nin\\n \\na\\n \\ncountry\\n \\nwhere\\n \\nwe\\n \\noperate.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nrequired\\n \\nto\\n \\nadhere\\n \\nto\\n \\nboth\\n \\nthe\\n \\nspirit\\n \\nand\\n \\nthe\\n \\nletter\\n \\nof\\n \\nthis\\n \\nPolicy\\n \\nwith\\n \\nrespect\\n \\nto\\n \\nour\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld.\\n \\n \\nIII.', mimetype='text/plain', start_char_idx=0, end_char_idx=2829, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='ebd3bda1-2adb-48ce-8433-7a11079d3c46', embedding=None, metadata={'page_label': '4', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"WSO2 Anti-Corruption and Anti-Bribery Policy: Compliance Standards, Prohibitions, and Global Guidelines\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the \"WSO2 Anti-Corruption and Anti-Bribery Policy,\" here are three specific questions that can be answered using the context:\\n\\n1. **What is the stance of WSO2 regarding compliance with anti-corruption policies in relation to competitors\\' behaviors?**\\n   - The excerpt clearly states that there are no exceptions to the policy, even if competitors engage in improper behavior or if corruption is accepted in a country where WSO2 operates. WSO2 Personnel must adhere to the policy regardless of external practices.\\n\\n2. **What actions are prohibited under the FCPA and other Anti-Corruption Laws as outlined in WSO2\\'s policy?**\\n   - The policy prohibits WSO2 and its personnel from corruptly promising, offering, providing, or authorizing the provision of money or anything of value to government officials or certain other persons to achieve an improper purpose.\\n\\n3. **What constitutes \"improper purposes\" according to WSO2\\'s Anti-Corruption Policy?**\\n   - \"Improper purposes\" include influencing any act or decision of the recipient in their official capacity, inducing the recipient to violate their lawful duty, influencing government acts or decisions, or securing any improper advantage to obtain regulatory approvals, contracts, or other benefits.\\n\\nThese questions focus on the specific prohibitions and principles outlined in the policy, which may not be readily available in other documents or contexts.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='9aa353a0-cc99-4018-99e9-5d5d27bce189', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='3c238819097bd2b814dff417abedfd213f4dfdd3715ea64b2098f4bdccd9b851'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2b2912cb-275c-44a6-ab01-97eb8d7bf8bf', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '4', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='a4cf0786acba9c39ebe9ed090bc38ef606743a10da944a0a596e8117156a15a6')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='There  are  no  exceptions  to  this  Policy,  even  if  our  competitors  engage  in  improper  behavior  or  corruption  \\nis\\n \\nan\\n \\naccepted\\n \\npractice\\n \\nin\\n \\na\\n \\ncountry\\n \\nwhere\\n \\nwe\\n \\noperate.\\n  \\nWSO2\\n \\nPersonnel\\n \\nare\\n \\nrequired\\n \\nto\\n \\nadhere\\n \\nto\\n \\nboth\\n \\nthe\\n \\nspirit\\n \\nand\\n \\nthe\\n \\nletter\\n \\nof\\n \\nthis\\n \\nPolicy\\n \\nwith\\n \\nrespect\\n \\nto\\n \\nour\\n \\nbusiness\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld.\\n \\n \\nIII.   A\\nNTI\\n-B\\nRIBERY\\n \\nP\\nROHIBITIONS\\n \\n \\nThe  FCPA  and  other  Anti-Corruption  Laws  prohibit  WSO2  and  WSO2  Personnel  from  corruptly  \\npromising,\\n \\noffering,\\n \\nproviding,\\n \\nor\\n \\nauthorizing\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nmoney\\n \\nor\\n \\nanything\\n \\nof\\n \\nvalue\\n \\ndirectly\\n \\nor\\n \\nindirectly\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nand\\n \\ncertain\\n \\nother\\n \\npersons\\n \\nto\\n \\nachieve\\n \\nan\\n \\nimproper\\n \\npurpose.\\n \\n“Improper\\n \\npurposes”\\n \\ninclude\\n \\ninfluencing\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\nthe\\n \\nrecipient\\n \\nin\\n \\nhis/her\\n \\nofficial\\n \\ncapacity,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ndo\\n \\nor\\n \\nomit\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nin\\n \\nviolation\\n \\nof\\n \\nhis/her\\n \\nlawful\\n \\nduty,\\n \\ninducing\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\ninfluence\\n \\nany\\n \\nact\\n \\nor\\n \\ndecision\\n \\nof\\n \\na\\n \\ngovernment\\n \\nor\\n \\ninstrumentality\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nor\\n \\nsecuring\\n \\nany\\n \\nimproper\\n \\nadvantage,\\n \\nin\\n \\norder\\n \\nto\\n \\nobtain,\\n \\nretain,\\n \\nor\\n \\ndirect\\n \\nregulatory\\n \\napprovals,\\n \\ncontracts,\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nbenefits.\\n   \\n \\n4', mimetype='text/plain', start_char_idx=2434, end_char_idx=3801, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='c445545c-66da-4f20-8146-85bc0a3970fd', embedding=None, metadata={'page_label': '5', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Global Anti-Corruption and Anti-Bribery Policy: Prohibitions on Improper Payments and Benefits\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the \"Global Anti-Corruption and Anti-Bribery Policy,\" here are three specific questions that can be answered using the context:\\n\\n1. **What types of entities and individuals are prohibited from receiving bribes or improper benefits according to WSO2\\'s Anti-Corruption Policy?**\\n   - This question can be answered by referencing the specific categories listed in the policy, such as government officials, state-owned entities, public international organizations, political parties, non-governmental organizations, and private-sector companies.\\n\\n2. **What are some examples of situations where WSO2 personnel might be asked to provide a bribe or improper benefit?**\\n   - The excerpt provides specific scenarios, such as in exchange for contracts, licenses, favorable inspection results, or tax reductions, which can be directly cited in response to this question.\\n\\n3. **What are the potential legal implications for a company or individual engaging in acts of corruption under the Anti-Corruption Laws applicable to WSO2?**\\n   - The context mentions the possibility of facing liability in multiple countries for a single act of corruption, highlighting the broad reach of anti-bribery legislation and the risks involved. This can be elaborated upon to explain the legal consequences outlined in the policy.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6540bb39-89af-46cd-9aa5-8ece52852f2f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='65a5f110e0077cf0ea985c03049b539ba9c443b531c9a5157032aa1fe60c629f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='72759a5b-db49-44e3-9f5e-6758919b6995', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='59096fd7b2d877deb90909673494ac92ced2112452ff2a7b8f06173286a39655')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only The  FCPA  prohibits  improper  payments  provided  to  officials  of  governments,  state-affiliated  entities,  and  \\npolitical\\n \\nparties\\n \\noutside\\n \\nthe\\n \\nUnited\\n \\nStates.\\n \\nHowever,\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\ngovernment\\n \\nor\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\nthe\\n \\nUnited\\n \\nStates\\n \\nwill\\n \\nviolate\\n \\nU.S.\\n \\ndomestic\\n \\nbribery\\n \\nstatutes.\\n \\n \\nIn  addition  to  the  United  States,  almost  all  other  countries,  including  the  United  Kingdom,  Brazil,  and  Sri  \\nLanka,\\n \\nhave\\n \\npromulgated\\n \\ntheir\\n \\nown\\n \\nanti-bribery\\n \\nlegislation.\\n \\nMost\\n \\nof\\n \\nthose\\n \\ncountries\\n \\nprohibit\\n \\nmaking\\n \\nimproper\\n \\npayments\\n \\nto\\n \\ngovernment\\n \\nand\\n \\nprivate-sector\\n \\nrecipients\\n \\nwithin\\n \\ntheir\\n \\nborders.\\n \\nHowever,\\n \\nseveral\\n \\ncountries\\n \\nhave\\n \\nalso\\n \\nadopted\\n \\nlegislation\\n \\nsimilar\\n \\nto\\n \\nthe\\n \\nFCPA\\n \\nthat\\n \\nprohibit\\n \\nimproper\\n \\npayments\\n \\noutside\\n \\nthose\\n \\ncountries.\\n  \\nThe\\n \\nexistence\\n \\nof\\n \\nall\\n \\nof\\n \\nthese\\n \\nlaws\\n \\nmeans\\n \\nthat\\n \\nthere\\n \\nis\\n \\npotential\\n \\nfor\\n \\na\\n \\ncompany\\n \\nor\\n \\nan\\n \\nindividual\\n \\nto\\n \\nface\\n \\nliability\\n \\nin\\n \\nseveral\\n \\ncountries\\n \\nfor\\n \\nthe\\n \\nsame\\n \\nsingle\\n \\nact\\n \\nof\\n \\ncorruption.\\n  \\nAttachment\\n \\n1\\n \\ncontains\\n \\nan\\n \\noverview\\n \\nof\\n \\nthe\\n \\nAnti-Corruption\\n \\nLaws\\n \\nof\\n \\nother\\n \\njurisdictions\\n \\nwhich\\n \\nare\\n \\napplicable\\n \\nto\\n \\nWSO2.\\n \\n \\nGiven  the  broad  prohibitions  under  Anti-Corruption  Laws  applicable  to  WSO2,  this  Policy  \\nprohibits\\n \\nbribes,\\n \\nkickbacks,\\n \\nand\\n \\nthe\\n \\nprovision\\n \\nof\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nand\\n \\nadvantages\\n \\nto\\n \\nany\\n \\nperson,\\n \\nentity,\\n \\nor\\n \\norganization,\\n \\nincluding,\\n \\nbut\\n \\nnot\\n \\nlimited\\n \\nto,\\n \\nemployees,\\n \\nofficials,\\n \\nrepresentatives,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\n \\n(i)  government;   \\n(ii)  state-owned  or  affiliated  entity,  including,  but  not  limited  to,  a  state  hospital,  research  \\ninstitution,\\n \\nutility,\\n \\npublic\\n \\nuniversity,\\n \\nor\\n \\nsovereign\\n \\nwealth\\n \\nfund;\\n \\n \\n(iii)  public  international  organization  such  as  the  United  Nations  or  the  World  Bank;    \\n(iv)  political  party,  including  the  party  itself  as  well  as  candidates  for  public  office;    \\n(v)  non-governmental  organization;  or   (vi)   private-sector  company.      \\nOne  may  be  asked  by  certain  parties  to  provide  a  bribe  or  other  improper  benefit  in  exchange  for  the  \\naward\\n \\nof\\n \\na\\n \\ncontract,\\n \\nsponsorship\\n \\nopportunity,\\n \\nor\\n \\nother\\n \\nbusiness;\\n \\nthe\\n \\nissuance\\n \\nor\\n \\nrenewal\\n \\nof\\n \\na\\n \\nconcession,\\n \\nlicense,\\n \\nor\\n \\nbusiness,\\n \\nconstruction,\\n \\nor\\n \\nother\\n \\npermit\\n \\nor\\n \\nregistration;\\n \\nthe\\n \\nsuccessful\\n \\nfiling\\n \\nof\\n \\na\\n \\npatent\\n \\nor\\n \\ntrademark\\n \\napplication;\\n \\nan\\n \\nimpermissible\\n \\nreduction\\n \\nin\\n \\nduties\\n \\nor\\n \\nother\\n \\ntaxes;\\n \\nobtaining\\n \\na\\n \\nfavorable\\n \\ninspection\\n \\nresult\\n \\nor\\n \\ncourt\\n \\ndecision,\\n \\neven\\n \\nif\\n \\nthe\\n \\nfacts\\n \\nor\\n \\ncircumstances\\n \\ndo\\n \\nnot\\n \\nsupport\\n \\nsuch\\n \\na\\n \\nresult;\\n \\nor\\n \\nthe\\n \\ngrant\\n \\nof\\n \\nsome\\n \\nother\\n \\nimproper\\n \\nadvantage.\\n  \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nWSO2\\n \\nPersonnel\\n \\nfrom\\n \\nproviding\\n \\nbribes\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nany\\n \\nof\\n \\nthe\\n \\nabove\\n \\npurposes.\\n \\n \\nA  violation  of  this  Policy  can  occur  even  if  the  bribe  fails  to  achieve  the  purpose  for  which  it  was  \\nintended.', mimetype='text/plain', start_char_idx=0, end_char_idx=3268, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='72759a5b-db49-44e3-9f5e-6758919b6995', embedding=None, metadata={'page_label': '5', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Global Anti-Corruption and Anti-Bribery Policy: Prohibitions on Improper Payments and Benefits\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the \"Global Anti-Corruption and Anti-Bribery Policy,\" here are three specific questions that can be answered using the context:\\n\\n1. **What actions are prohibited by the WSO2 Anti-Corruption Policy regarding improper payments?**\\n   - The policy explicitly prohibits WSO2 Personnel from providing bribes or other improper benefits to any person, regardless of whether the intended purpose is achieved.\\n\\n2. **Can a violation of the Anti-Corruption Policy occur if a bribe does not result in any business advantage?**\\n   - Yes, a violation can occur even if the bribe fails to achieve the intended purpose, meaning that providing an improper payment or benefit can still constitute a violation regardless of the outcome.\\n\\n3. **Does the Anti-Corruption Policy apply to both WSO2 funds and personal funds when financing improper payments?**\\n   - Yes, the policy applies irrespective of whether WSO2 funds or personal funds are used to finance improper payments or benefits.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6540bb39-89af-46cd-9aa5-8ece52852f2f', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='65a5f110e0077cf0ea985c03049b539ba9c443b531c9a5157032aa1fe60c629f'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c445545c-66da-4f20-8146-85bc0a3970fd', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '5', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='686511eae965d18b54dc3fd31aebd4f82b8aedf14fcecd6a2dfaaea70e90955c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='This\\n \\nPolicy\\n \\nprohibits\\n \\nWSO2\\n \\nPersonnel\\n \\nfrom\\n \\nproviding\\n \\nbribes\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefits\\n \\nto\\n \\nany\\n \\nperson\\n \\nto\\n \\nachieve\\n \\nany\\n \\nof\\n \\nthe\\n \\nabove\\n \\npurposes.\\n \\n \\nA  violation  of  this  Policy  can  occur  even  if  the  bribe  fails  to  achieve  the  purpose  for  which  it  was  \\nintended.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\na\\n \\nperson\\n \\ncan\\n \\nviolate\\n \\nthis\\n \\nPolicy\\n \\nif\\n \\nthat\\n \\nperson\\n \\nprovides\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nbenefit\\n \\nto\\n \\na\\n \\nrecipient\\n \\nand\\n \\nthe\\n \\nrecipient\\n \\ndoes\\n \\nnot\\n \\ngrant\\n \\nany\\n \\nbusiness\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nin\\n \\nreturn.\\n   \\nIn\\n \\naddition,\\n \\nthe\\n \\nmere\\n \\noffer\\n \\nor\\n \\npromise\\n \\nof\\n \\na\\n \\nbribe\\n \\nor\\n \\nother\\n \\nimproper\\n \\nbenefit\\n \\nis\\n \\nsufficient\\n \\nto\\n \\ncause\\n \\na\\n \\nviolation.\\n  \\nAll\\n \\nof\\n \\nthe\\n \\nanti-bribery\\n \\nprohibitions\\n \\ncontained\\n \\nin\\n \\nthis\\n \\nPolicy\\n \\napply\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\none\\n \\nuses\\n \\nWSO2\\n \\nfunds\\n \\nor\\n \\npersonal\\n \\nfunds\\n \\nto\\n \\nfinance\\n \\nimproper\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits.\\n \\n \\nThis  Policy  also  prohibits  WSO2  Personnel  from  soliciting  or  accepting  bribes,  kickbacks,  or  other  \\nimproper\\n \\npayments/benefits\\n \\nfrom\\n \\nthe\\n \\nCompany’s\\n \\nvendors\\n \\nor\\n \\nother\\n \\npersons\\n \\nin\\n \\nrelation\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\nFor\\n \\n5', mimetype='text/plain', start_char_idx=2953, end_char_idx=4173, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='912c7a30-cd21-48bc-8586-637cafb6766c', embedding=None, metadata={'page_label': '6', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"WSO2 Anti-Corruption Policy: Ethical Standards, Accounting Requirements, and Regulations on Government Fees and Prohibition of Facilitation Payments in Business Practices\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the \"WSO2 Anti-Corruption Policy,\" here are three specific questions that can be answered using the context, along with higher-level summaries to enhance understanding:\\n\\n1. **What constitutes a violation of the WSO2 Anti-Corruption Policy regarding vendor payments?**\\n   - Summary: The policy outlines that a violation occurs if WSO2 personnel cause the company to overpay a vendor, and that vendor subsequently shares any portion of the overpayment with the personnel involved.\\n\\n2. **What are the accounting requirements that WSO2 personnel must adhere to in order to comply with the Anti-Corruption Policy?**\\n   - Summary: WSO2 is required to maintain accurate and fair records of transactions, expenses, and asset dispositions. Personnel must comply with internal controls and cooperate with audits, as violations can occur from concealing bribes or falsifying records, regardless of the amount.\\n\\n3. **How does the WSO2 Anti-Corruption Policy define and treat facilitation payments?**\\n   - Summary: The policy explicitly prohibits all facilitation payments, which are defined as corrupt payments made to government officials to expedite routine government actions. It emphasizes that such payments can violate anti-corruption laws and accounting requirements, although official fees imposed by government agencies may be permissible if documented properly.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5131d538-492d-4d7b-876a-4c7b00d3d5b8', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='81ba862f64b62d6a65b3706713bde8c2a433a858ae0c8cedc873eb53d7679d4b'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='029ec03e-30ad-42be-ade1-4d9fd0e0350e', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='11a93e917664a88870504847045b53e64570a60e45d98b6f6bac1565f75bb234')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only instance,  a  violation  of  this  Policy  will  occur  if  you  cause  WSO2  to  overpay  a  vendor  and  that  vendor  then  \\nshares\\n \\nall\\n \\nor\\n \\na\\n \\nportion\\n \\nof\\n \\nthat\\n \\noverpayment\\n \\nwith\\n \\nyou.\\n   \\n \\nThis  Policy  requires  WSO2  Personnel  to  adhere  to  high  ethical  standards  and  to  comply  with  all  \\napplicable\\n \\nlaws\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nthe\\n \\nCompany.\\n  \\nAnti-corruption\\n \\nviolations\\n \\ntypically\\n \\ninvolve\\n \\ncircumstances\\n \\nthat\\n \\nalso\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws,\\n \\nincluding\\n \\nthose\\n \\nthat\\n \\naddress\\n \\nmoney\\n \\nlaundering,\\n \\nembezzlement,\\n \\nfraud,\\n \\nexport\\n \\ncontrols,\\n \\nand\\n \\nsanctions/embargoes.\\n \\nGuilty\\n \\npersons\\n \\ncan\\n \\nface\\n \\nmultiple\\n \\ncharges\\n \\nbased\\n \\non\\n \\nthe\\n \\nsame\\n \\nset\\n \\nof\\n \\nfacts.\\n \\n \\nIV.   A\\nCCOUNTING\\n \\nR\\nEQUIREMENTS\\n \\n \\nWSO2  must  maintain  books,  records,  and  accounts,  which,  in  reasonable  detail,  accurately  and  fairly  \\nreflect\\n \\nthe\\n \\nCompany’s\\n \\ntransactions,\\n \\nexpenses,\\n \\nand\\n \\nasset\\n \\ndispositions.\\n \\nWSO2\\n \\nis\\n \\nalso\\n \\ncommitted\\n \\nto\\n \\nmaintaining\\n \\na\\n  \\nsystem\\n \\nof\\n \\ninternal\\n \\naccounting\\n \\ncontrols\\n \\nto\\n \\nprovide\\n \\nreasonable\\n \\nassurances\\n \\nthat\\n \\ntransactions\\n \\nare\\n \\nproperly\\n \\nauthorized\\n \\nby\\n \\nmanagement,\\n \\nexecuted,\\n \\nand\\n \\nrecorded.\\n  \\nThis\\n \\nmeans\\n \\nthat\\n \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\ncomply\\n \\nwith\\n  \\nour\\n \\ninternal\\n \\ncontrols\\n \\nand\\n \\navoid\\n \\nunauthorized\\n \\nactivities\\n \\nor\\n \\nexpenses.\\n  \\nWSO2\\n \\nPersonnel\\n \\nmust\\n \\nalso\\n \\ncooperate\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nperiodic\\n \\naudits\\n \\nand\\n \\nother\\n \\nefforts\\n \\nto\\n \\nensure\\n \\nthat\\n \\nour\\n \\ninternal\\n \\ncontrols\\n \\nare\\n \\nbeing\\n \\nobserved.\\n \\n \\nViolations  of  the  above  accounting  standards  can  occur  if  one  conceals  bribes  or  falsifies  other  \\ntransactions\\n \\nor\\n \\nexpenses,\\n \\neven\\n \\nif\\n \\nthey\\n \\nare\\n \\nnot\\n \\nrelated\\n \\nto\\n \\na\\n \\nbribe,\\n \\nin\\n \\nWSO2’s\\n \\nledgers\\n \\nor\\n \\nother\\n \\nrecords.\\n  \\nAlso,\\n \\nthere\\n \\nis\\n \\nno\\n \\nmateriality\\n \\nstandard.\\n \\nThis\\n \\nmeans\\n \\nthat\\n \\neven\\n \\nsmall\\n \\nmisreported\\n \\namounts\\n \\nmay\\n \\nresult\\n \\nin\\n \\nviolations.\\n   \\n \\nV.   F\\nACILITATION\\n \\nP\\nAYMENTS\\n \\n \\n \\nThis  Policy  prohibits  all  corrupt  payments  or  benefits,  including  so-called  grease  or  facilitation  payments  \\nprovided\\n \\nto\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\nto\\n \\nexpedite\\n \\nor\\n \\nsecure\\n \\nroutine\\n \\ngovernment\\n \\nactions\\n \\n(collectively,\\n \\n“\\nFacilitation\\n \\nPayments\\n”).\\n  \\nFacilitation\\n \\nPayments\\n \\ninclude\\n \\npayments\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nto\\n \\nexpedite\\n \\nroutine\\n \\nand\\n \\nnondiscretionary\\n \\nactivities,\\n \\nsuch\\n \\nas\\n \\nprocessing\\n \\npermit\\n \\nand\\n \\nlicense\\n \\napplications,\\n \\nscheduling\\n \\ninspections,\\n \\nand/or\\n \\nproviding\\n \\ninfrastructure\\n \\nservices\\n \\n(\\ne.g.\\n,\\n \\nwater,\\n \\nelectricity\\n \\nmail).\\n  \\nWSO2\\n \\nstrictly\\n \\nprohibits\\n \\nthe\\n \\noffer,\\n \\npromise,\\n \\nor\\n \\nprovision\\n \\nof\\n \\nFacilitation\\n \\nPayments\\n \\nto\\n \\nany\\n \\ndomestic\\n \\nor\\n \\nforeign\\n \\nlocal\\n \\nor\\n \\nfederal\\n \\ngovernment\\n \\nofficial,\\n \\nas\\n \\nthey\\n \\ncan\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws\\n \\nand\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndescribed\\n \\nabove.\\n \\n \\nPlease  note  that  in  some  cases,  government  agencies  may  impose  official  fees  that  may  be  paid  directly  in  \\nthe\\n \\nname\\n \\nof\\n \\na\\n \\ngovernmental\\n \\nentity\\n \\nor\\n \\nenterprise\\n \\nitself,\\n \\nas\\n \\nset\\n \\nout\\n \\nin\\n \\npublished\\n \\nfee\\n \\nschedules\\n \\nor\\n \\nother\\n \\nofficial\\n \\ndocuments.', mimetype='text/plain', start_char_idx=0, end_char_idx=3335, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='029ec03e-30ad-42be-ade1-4d9fd0e0350e', embedding=None, metadata={'page_label': '6', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"WSO2 Anti-Corruption Policy: Ethical Standards, Accounting Requirements, and Regulations on Government Fees and Prohibition of Facilitation Payments in Business Practices\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the \"WSO2 Anti-Corruption Policy,\" here are three specific questions that can be answered using the context:\\n\\n1. **What conditions must be met for government fees to be considered acceptable under the WSO2 Anti-Corruption Policy?**\\n   - The excerpt specifies that official government fees can be paid if they are deposited in the treasury of a government, an official government receipt is collected, and the expense is accurately recorded in the Company’s books.\\n\\n2. **What distinguishes an official government fee from a facilitation payment according to the WSO2 Anti-Corruption Policy?**\\n   - The policy distinguishes official government fees as those that are deposited in an official treasury account belonging to a government, while facilitation payments are defined as payments made for the benefit of government officials in their personal capacity, which do not meet the criteria for official fees.\\n\\n3. **What is the stance of WSO2 on the use of intermediaries and business partners in relation to bribery?**\\n   - The policy explicitly prohibits WSO2 Personnel from providing bribes or other improper benefits directly or indirectly through third parties, indicating a strict stance against corruption in all forms of business relationships.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5131d538-492d-4d7b-876a-4c7b00d3d5b8', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='81ba862f64b62d6a65b3706713bde8c2a433a858ae0c8cedc873eb53d7679d4b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='912c7a30-cd21-48bc-8586-637cafb6766c', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='39d483e0e16271b3ea98b4ac1ab464e7b8ee9bc75369207bbf6d3ddd23fcb199')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Please  note  that  in  some  cases,  government  agencies  may  impose  official  fees  that  may  be  paid  directly  in  \\nthe\\n \\nname\\n \\nof\\n \\na\\n \\ngovernmental\\n \\nentity\\n \\nor\\n \\nenterprise\\n \\nitself,\\n \\nas\\n \\nset\\n \\nout\\n \\nin\\n \\npublished\\n \\nfee\\n \\nschedules\\n \\nor\\n \\nother\\n \\nofficial\\n \\ndocuments.\\n  \\nThese\\n \\nofficial\\n \\ngovernment\\n \\nfees\\n \\ncan\\n \\nbe\\n \\npaid\\n \\nto\\n \\nexpedite\\n \\npassports,\\n \\nlicenses,\\n \\nor\\n \\nother\\n \\nservices,\\n \\nprovided\\n \\nthat\\n \\nthey\\n \\nare\\n \\ndeposited\\n \\nin\\n \\nthe\\n \\ntreasury\\n \\nof\\n \\na\\n \\ngovernment,\\n \\nan\\n \\nofficial\\n \\ngovernment\\n \\nreceipt\\n \\nis\\n \\ncollected,\\n \\nand\\n \\nthe\\n \\nexpense\\n \\nis\\n \\naccurately\\n \\nrecorded\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n  \\nHowever,\\n \\nFacilitation\\n \\nPayments\\n \\nprovided\\n \\nfor\\n \\nthe\\n \\nbenefit\\n \\nof\\n \\ngovernment\\n \\nofficials\\n \\nin\\n \\ntheir\\n \\npersonal\\n \\ncapacity\\n \\n(\\ni.e.\\n,\\n \\nare\\n \\nnot\\n \\ndeposited\\n \\nin\\n \\nan\\n \\nofficial\\n \\ntreasury\\n \\naccount\\n \\nbelonging\\n \\nto\\n \\na\\n \\ngovernment)\\n \\nwill\\n \\nviolate\\n \\nthis\\n \\nPolicy.\\n     \\n \\n \\nVI.   I\\nNTERMEDIARIES\\n \\nAND\\n \\nB\\nUSINESS\\n \\nP\\nARTNERS\\n \\n \\nThis  Policy  prohibits  WSO2  Personnel  from  providing  bribes  or  other  improper  benefits  directly  as  well  \\nas\\n \\nindirectly\\n \\nthrough\\n \\nthird\\n \\nparties.\\n \\nThis\\n \\nrisk\\n \\ncan\\n \\narise\\n \\nin\\n \\ncases\\n \\nwhere\\n \\nthe\\n \\nCompany\\n \\nworks\\n \\nwith\\n \\nagents,\\n 6', mimetype='text/plain', start_char_idx=3050, end_char_idx=4292, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='974ecb96-dc14-4e49-a55d-7098d63a16a7', embedding=None, metadata={'page_label': '7', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"WSO2 Anti-Corruption Compliance Framework: Guidelines for Intermediaries, Due Diligence, and Gifts\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the \"WSO2 Anti-Corruption Compliance Framework,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are the requirements for conducting due diligence on Intermediaries according to WSO2\\'s Anti-Corruption Policy?**\\n   - The policy requires that appropriate, risk-based anti-corruption due diligence is performed on Intermediaries to confirm that they do not have a history or reputation for corruption or similar wrongdoing, and that they have executed a written agreement containing anti-corruption compliance clauses.\\n\\n2. **What actions must WSO2 Personnel take if they learn of an Intermediary engaging in corrupt practices?**\\n   - WSO2 Personnel are required to notify the Compliance Officer if they learn of any Company Intermediary that engages in corrupt or other improper practices.\\n\\n3. **What types of gifts and benefits are permissible under WSO2\\'s Anti-Corruption Policy?**\\n   - Reasonably priced gifts, meals, entertainment, travel, and other benefits provided for non-corrupt business promotion or goodwill purposes may be permissible. Examples of acceptable items include a plastic pen, a t-shirt, a coffee mug, a paper weight, or a cap of moderate value embossed with the Company’s logo, which generally do not violate Anti-Corruption Laws.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='74c3e57c-f8e3-438c-b95b-bc17adf9d045', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='67fa141cf9352c57c5c5a89029d04ebc518acf2dca565aabae86c3327a6ed248'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d56da533-6411-438c-95fd-134ce55f4343', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='24c65075e4117db222b7dc9e89a25c590b087eeddf4b5020ccec4de0e18d8816')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only consultants,  representatives,  lobbyists,  suppliers/vendors,  resellers,  distributors,  customs  or  other  brokers,  \\ncontractors,\\n \\nadvisors,\\n \\nother\\n \\nbusiness\\n \\npartners,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nthat\\n \\nperforms\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\nWSO2\\n \\n(collectively\\n \\n“\\nIntermediaries\\n”).\\n   \\n \\nIn  certain  cases,  WSO2  and  WSO2  Personnel  can  be  held  liable  under  the  FCPA  and  other  laws  even  if  \\none\\n \\ndoes\\n \\nnot\\n \\nexpressly\\n \\nauthorize\\n \\nan\\n \\nIntermediary\\n \\nto\\n \\nengage\\n \\nin\\n \\ncorruption,\\n \\nbut\\n \\nthey\\n \\ndo\\n \\nso\\n \\nanyway.\\n \\nThis\\n \\ncan\\n \\noccur\\n \\nif\\n \\none\\n \\n(i)\\n \\nhas\\n \\nactual\\n \\nknowledge\\n \\nor\\n \\na\\n \\nfirm\\n \\nbelief\\n \\nthat\\n \\na\\n \\nperson\\n \\nwill\\n \\nengage\\n \\nin\\n \\ncorruption\\n \\nor\\n \\n(ii)\\n \\nconsciously\\n \\ndisregards,\\n \\ndeliberately\\n \\nignores,\\n \\nor\\n \\nis\\n \\nwillfully\\n \\nblind\\n \\nto\\n \\nthe\\n \\nIntermediary’s\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\npractices.\\n  \\n \\nGiven  these  risks,  this  Policy  requires  that  (i)  appropriate,  risk-based  anti-corruption  due  diligence  is  \\nperformed\\n \\non\\n \\nIntermediaries\\n \\nto\\n \\nconfirm\\n \\nthat\\n \\nsuch\\n \\nIntermediary\\n \\ndoes\\n \\nnot\\n \\nhave\\n \\na\\n \\nhistory\\n \\nor\\n \\nreputation\\n \\nfor\\n \\ncorruption\\n \\nor\\n \\nsimilar\\n \\nwrong\\n \\ndoing,\\n \\nand\\n \\n(ii)\\n \\nthe\\n \\nIntermediary\\n \\nhas\\n \\nexecuted\\n \\na\\n \\nwritten\\n \\nagreement\\n \\ncontaining\\n \\nanti-corruption\\n \\ncompliance\\n \\nclauses.\\n \\nPlease\\n \\nconsult\\n \\nWSO2’s\\n \\nCompliance\\n \\nOfficer\\n \\nfor\\n \\ninformation\\n \\nregarding\\n \\nWSO2’s\\n \\nIntermediary\\n \\ndue\\n \\ndiligence\\n \\nprocedures.\\n \\n \\nThroughout  any  relationship  with  an  Intermediary,  WSO2  Personnel  must  monitor  their  performance  to  \\nensure\\n \\nthat\\n \\nthey\\n \\ndo\\n \\nnot\\n \\nengage\\n \\nin\\n \\nactivities\\n \\nthat\\n \\nraise\\n \\ncorruption\\n \\nconcerns.\\n  \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nprovide\\n \\nguidance\\n \\non\\n \\nthe\\n \\ntypes\\n \\nof\\n \\nred\\n \\nflags\\n \\nthat\\n \\none\\n \\nshould\\n \\nmonitor\\n \\nbefore\\n \\nand\\n \\nafter\\n \\nengaging\\n \\nan\\n \\nIntermediary.\\n \\n \\nThis  Policy  requires  WSO2  Personnel  to  notify  the  Compliance  Officer  if  they  learn  of  any  Company  \\nIntermediary\\n \\nthat\\n \\nengages\\n \\nin\\n \\ncorrupt\\n \\nor\\n \\nother\\n \\nimproper\\n \\npractices.\\n \\nAlso,\\n \\nall\\n \\npayments\\n \\nto\\n \\nIntermediaries\\n \\nmust\\n \\nbe\\n \\naccurately\\n \\nreported\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks\\n \\nand\\n \\nrecords\\n \\nin\\n \\naccordance\\n \\nwith\\n \\nthe\\n \\naccounting\\n \\nrequirements\\n \\ndiscussed\\n \\nabove.\\n \\n \\nVII.   G\\nIFTS\\n \\nAND\\n \\nH\\nOSPITALITIES\\n \\n \\nAnti-Corruption  Laws  prohibit  the  provision  or  acceptance  of  money  or  things  of  value  for  corrupt  or  \\nimproper\\n \\npurposes.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthis\\n \\nprohibition\\n \\nis\\n \\nlikely\\n \\nin\\n \\ninstances\\n \\nwhere\\n \\npersonal\\n \\nbenefits\\n \\nare\\n \\ngiven\\n \\nor\\n \\naccepted\\n \\nin\\n \\nthe\\n \\ncourse\\n \\nof\\n \\nnegotiation\\n \\nor\\n \\ntender\\n \\nbid.\\n \\nHowever,\\n \\nreasonably\\n \\npriced\\n \\ngifts,\\n \\nmeals,\\n \\nentertainment,\\n \\ntravel,\\n \\nand\\n \\nother\\n \\nbenefits\\n \\nprovided\\n \\nfor\\n \\nnon-corrupt\\n \\nbusiness\\n \\npromotion\\n \\nor\\n \\ngoodwill\\n \\npurposes\\n \\nmay\\n \\nbe\\n \\npermissible\\n \\nunder\\n \\nAnti-Corruption\\n \\nLaws\\n \\nin\\n \\ncertain\\n \\ncases.\\n  \\nFor\\n \\ninstance,\\n \\na\\n \\nplastic\\n \\npen,\\n \\na\\n \\nt-shirt,\\n \\na\\n \\ncoffee\\n \\nmug,\\n \\na\\n \\npaper\\n \\nweight,\\n \\nor\\n \\na\\n \\ncap\\n \\nof\\n \\nmoderate\\n \\nvalue\\n \\nand\\n \\nembossed\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nlogo\\n \\nwill\\n \\ngenerally\\n \\nnot\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws.', mimetype='text/plain', start_char_idx=0, end_char_idx=3189, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='d56da533-6411-438c-95fd-134ce55f4343', embedding=None, metadata={'page_label': '7', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"WSO2 Anti-Corruption Compliance Framework: Guidelines for Intermediaries, Due Diligence, and Gifts\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the \"WSO2 Anti-Corruption Compliance Framework,\" here are three specific questions that can be answered using the context:\\n\\n1. **What types of gifts are generally considered acceptable under the Anti-Corruption Laws according to WSO2\\'s policy?**\\n   - The excerpt specifies that items of moderate value, such as a plastic pen, t-shirt, coffee mug, paper weight, or cap embossed with the Company’s logo, are generally acceptable and do not violate Anti-Corruption Laws.\\n\\n2. **What actions must WSO2 Personnel take before providing gifts or benefits to government officials or agents?**\\n   - WSO2 Personnel must obtain the approval of the Compliance Officer prior to providing gifts, meals, travel benefits, and other hospitalities to employees, officials, or agents of any government, political party, state-owned entity, or public international organization.\\n\\n3. **What considerations must WSO2 Personnel keep in mind regarding local laws when providing gifts or benefits?**\\n   - The excerpt indicates that WSO2 Personnel must ensure that the provision of a gift or benefit does not violate local laws or policies applicable in the recipient\\'s country, as some countries impose limits on the value of gifts or may ban such gifts altogether, regardless of intent.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='74c3e57c-f8e3-438c-b95b-bc17adf9d045', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='67fa141cf9352c57c5c5a89029d04ebc518acf2dca565aabae86c3327a6ed248'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='974ecb96-dc14-4e49-a55d-7098d63a16a7', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '7', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='eb900cda1c2c59e25e26ccc1ecf2fdb92e4b4e20b441dd03a127db6f608683d2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='For\\n \\ninstance,\\n \\na\\n \\nplastic\\n \\npen,\\n \\na\\n \\nt-shirt,\\n \\na\\n \\ncoffee\\n \\nmug,\\n \\na\\n \\npaper\\n \\nweight,\\n \\nor\\n \\na\\n \\ncap\\n \\nof\\n \\nmoderate\\n \\nvalue\\n \\nand\\n \\nembossed\\n \\nwith\\n \\nthe\\n \\nCompany’s\\n \\nlogo\\n \\nwill\\n \\ngenerally\\n \\nnot\\n \\nviolate\\n \\nAnti-Corruption\\n \\nLaws.\\n \\nHowever,\\n \\na\\n \\nfur\\n \\ncoat,\\n \\na\\n \\ncar,\\n \\nor\\n \\na\\n \\nvacation\\n \\nwill\\n \\nraise\\n \\nanticorruption\\n \\nconcerns,\\n \\nespecially\\n \\nif\\n \\nsuch\\n \\nbenefits\\n \\nare\\n \\nprovided\\n \\nto\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nother\\n \\nperson\\n \\nwho\\n \\nis\\n \\nresponsible\\n \\nfor\\n \\nmaking\\n \\ndecisions\\n \\nin\\n \\nrelation\\n \\nto\\n \\nthe\\n \\nCompany’s\\n \\nbusiness.\\n   \\n \\nWSO2  Personnel  must  also  ensure  that  the  provision  of  a  gift  or  other  benefit  does  not  violate  local  laws  \\nor\\n \\npolicies\\n \\nthat\\n \\napply\\n \\nin\\n \\nthe\\n \\ncountry\\n \\nwhere\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nthe\\n \\nbenefit\\n \\nis\\n \\nlocated.\\n  \\nSome\\n \\ncountries\\n \\nimpose\\n \\nexpress\\n \\nlimits\\n \\non\\n \\nthe\\n \\nvalue\\n \\nof\\n \\ngifts/benefits\\n \\nthat\\n \\na\\n \\nrecipient\\n \\ncan\\n \\naccept;\\n \\nother\\n \\ncountries\\n \\nban\\n \\nsuch\\n \\ngifts/benefits\\n \\naltogether\\n \\neven\\n \\nif\\n \\ngiven\\n \\nwith\\n \\nno\\n \\ncorrupt\\n \\nor\\n \\nimproper\\n \\nintention.\\n \\n \\nWSO2  Personnel  must  obtain  the  approval  of  the  Compliance  Officer  prior  to  providing  gifts,  meals,  \\ntravel\\n \\nbenefits,\\n \\nand\\n \\nother\\n \\nhospitalities\\n \\nto\\n \\nemployees,\\n \\nofficials,\\n \\nor\\n \\nagents\\n \\nof\\n \\nany\\n \\ngovernment,\\n \\npolitical\\n \\nparty,\\n \\nstateowned\\n \\nentity,\\n \\nor\\n \\npublic\\n \\ninternational\\n \\norganization.\\n \\nThe\\n \\nCompliance\\n \\nOfficer\\n \\ncan\\n \\nhelp\\n \\ndetermine\\n \\n7', mimetype='text/plain', start_char_idx=2947, end_char_idx=4397, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='42238f3e-d025-48a9-a3fb-81c6aab2537d', embedding=None, metadata={'page_label': '8', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"WSO2 Anti-Corruption Policy: Guidelines for Compliance, Training, Violations, and Consequences\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the \"WSO2 Anti-Corruption Policy,\" here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What types of activities should Company personnel consult the Compliance Officer about to ensure compliance with anti-corruption measures?**\\n   - The excerpt specifies that personnel should confer with the Compliance Officer before engaging in joint ventures, mergers and acquisitions, and charitable or political donations to ensure appropriate anti-corruption compliance measures are observed.\\n\\n2. **What are the potential consequences for violating the WSO2 Anti-Corruption Policy?**\\n   - The document outlines that violations can lead to disciplinary actions such as demotion, reassignment, additional training, probation, suspension, or termination. It also mentions that both the Company and personnel may face substantial fines, penalties, and even imprisonment in serious cases.\\n\\n3. **What is the requirement for anti-corruption training for designated personnel at WSO2?**\\n   - The excerpt states that all designated personnel must undergo anti-corruption training provided by WSO2, with the nature, content, and frequency of the training determined based on the risk profile. Additionally, certain personnel may be required to certify compliance with the policy periodically.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5e75fc0d-3e5e-4fae-b83d-7225f5dff1a7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='d5f706d3ed817f12832ee5750057ba9d7d0966eaa415a1259d6089a2fb00729e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only whether  the  provision  of  the  benefit  is  permissible  under  applicable  Anti-Corruption  Laws.   If  the  expense  \\nis\\n \\napproved,\\n \\nits\\n \\nvalue\\n \\nand\\n \\nbusiness\\n \\npurpose\\n \\nmust\\n \\nbe\\n \\nrecorded\\n \\naccurately\\n \\nin\\n \\nthe\\n \\nCompany’s\\n \\nbooks.\\n \\nThis\\n \\nPolicy\\n \\nprohibits\\n \\nCompany\\n \\npersonnel\\n \\nfrom\\n \\nproviding\\n \\ncash\\n \\nor\\n \\ngift\\n \\ncards\\n \\nor\\n \\ngift\\n \\ncertificates\\n \\nthat\\n \\ncan\\n \\neasily\\n \\nbe\\n \\nconverted\\n \\ninto\\n \\ncash.\\n \\n \\nIX.   O\\nTHER\\n \\nA\\nCTIVITIES\\n \\n \\nCorruption  concerns  can  arise  in  a  number  of  other  cases  including,  but  not  limited  to  (i)  joint  ventures  or  \\nteaming\\n \\narrangements\\n \\nwith\\n \\npublic\\n \\nor\\n \\nprivate-sector\\n \\npartners;\\n \\n(ii)\\n \\nmergers\\n \\nand\\n \\nacquisitions,\\n \\nespecially\\n \\nif\\n \\nthe\\n \\ntarget\\n \\nbusiness\\n \\nhas\\n \\nsignificant\\n \\ngovernment\\n \\ninteractions\\n \\nor\\n \\nan\\n \\ninternational\\n \\nprofile;\\n \\nand\\n \\n(iii)\\n \\ncharitable\\n \\nand\\n \\npolitical\\n \\ndonations.\\n \\nPlease\\n \\nconfer\\n \\nwith\\n \\nthe\\n \\nCompliance\\n \\nOfficer\\n \\nbefore\\n \\nengaging\\n \\nin\\n \\nthese\\n \\ntypes\\n \\nof\\n \\nactivities\\n \\nto\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nanti-corruption\\n \\ncompliance\\n \\nmeasures\\n \\nare\\n \\nobserved.\\n \\n \\n \\nX.   V\\nIOLATIONS\\n \\nAND\\n \\nC\\nONSEQUENCES\\n \\n \\nA  violation  of  this  Policy  will  result  in  appropriate  disciplinary  action,  including  demotion,  reassignment,  \\nadditional\\n \\ntraining,\\n \\nprobation,\\n \\nsuspension,\\n \\nor\\n \\neven\\n \\ntermination.\\n \\n \\nBoth  the  Company  and  Company  Personnel  may  be  subject  to  substantial  fines  and  penalties  for  violating  \\nAnti-Corruption\\n \\nLaws.\\n  \\nIn\\n \\nserious\\n \\ncases,\\n \\nindividuals\\n \\nmay\\n \\nface\\n \\nimprisonment,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\nassessment\\n \\nof\\n \\nmonetary\\n \\nfines\\n \\nand\\n \\npenalties.\\n  \\nIn\\n \\naddition,\\n \\nthe\\n \\nCompany\\n \\nmay\\n \\nface\\n \\nsuspension\\n \\nor\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts,\\n \\nthe\\n \\nloss\\n \\nof\\n \\nU.S.\\n \\nexport\\n \\nprivileges,\\n \\nand\\n \\ncertain\\n \\nother\\n \\nconsequences.\\n \\nThese\\n \\nresults\\n \\ncan\\n \\nbe\\n \\ndevastating\\n \\nto\\n \\nour\\n \\nbusiness.\\n \\n \\nXI.   T\\nRAINING\\n \\nAND\\n \\nC\\nERTIFICATION\\n \\n \\n \\nAll  designated  personnel  must  undergo  anti-corruption  training  provided  by  WSO2.   The  nature,  content,  \\nand\\n \\nfrequency\\n \\nof\\n \\nthat\\n \\ntraining\\n \\nwill\\n \\nbe\\n \\ndetermined\\n \\nby\\n \\nWSO2\\n \\nbased\\n \\non\\n \\nrisk\\n \\nprofile.\\n  \\n \\nWSO2  may  require  certain  WSO2  Personnel  to  certify  compliance  with  this  Policy  on  a  periodic  basis.   \\nXII.   S\\nTATUS\\n \\n \\n \\nThe  Compliance  Officer  and/or  outside  counsel  will  review  this  Policy  on  a  periodic  basis  and  update  it,  \\nas\\n \\nappropriate,\\n \\nto\\n \\nreflect\\n \\nany\\n \\nchanges.\\n   \\n \\nThis  Policy  does  not  form  part  of  any  employment  contract  with  you  and  may  be  amended  at  any  time.   \\nThis\\n \\nPolicy\\n \\nshould\\n \\nbe\\n \\nread\\n \\nin\\n \\nconjunction\\n \\nwith\\n \\nWSO2’s\\n \\nother\\n \\npolicies.\\n \\n \\n \\n8', mimetype='text/plain', start_char_idx=0, end_char_idx=2774, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='ff7c59f2-690c-4d3b-838b-66d4a625c880', embedding=None, metadata={'page_label': '9', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"WSO2 Anti-Corruption Policy: Comprehensive Reporting Obligations and Compliance Measures\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the \"WSO2 Anti-Corruption Policy,\" here are three specific questions that can be answered using the context:\\n\\n1. **Who is the designated Compliance Officer at WSO2, and how can personnel report violations of the Anti-Corruption Policy?**\\n   - The Compliance Officer is Puny Navaratne, and personnel can report violations via email at legal-compliance@wso2.com, anonymously through the hotline number (800) 461-9330, or online at whistleblower.wso2.com.\\n\\n2. **What assurances does WSO2 provide to individuals who report violations of the Anti-Corruption Policy?**\\n   - WSO2 assures that appropriate confidentiality measures will be taken and that there will be no retaliation against any individual for reporting violations in good faith.\\n\\n3. **What types of requests must WSO2 personnel report to the Compliance Officer?**\\n   - WSO2 personnel must report any corrupt, improper, illegal, or unusual requests for payments or other benefits made by customers, intermediaries, vendors, business partners, government officials, or company employees.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='43551bf5-0a46-45f8-becb-d4356dd4d8ec', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='8267da462dccea0cbdb5a76be47a8371237823ba4a25344a2cd9b30e3518122a')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only XIII.   R\\nEPORTING\\n/Q\\nUESTIONS\\n  \\n \\n \\nWSO2  Personnel  have  an  affirmative  obligation  to  report  all  violations  of  this  Policy  to  the  Compliance  \\nOfficer\\n \\nas\\n \\nfollows:\\n \\n \\nPuny  Navaratne  legal-compliance@wso2.com   \\nReports  may  also  be  submitted  anonymously  by  using  the  Company’s  hotline  number  \\n(800)\\n \\n461-9330\\n \\nor\\n \\nonline\\n \\nat\\n \\nwhistleblower.wso2.com.\\n    \\nHowever,\\n \\nwe\\n \\nencourage\\n \\nyou\\n \\nto\\n \\nconsider\\n \\nrevealing\\n \\nyour\\n \\nidentity\\n \\nso\\n \\nthat\\n \\nwe\\n \\ncan\\n \\nproperly\\n \\nfollow\\n \\nup\\n \\nand\\n \\ninvestigate\\n \\nalleged\\n \\nviolations.\\n \\nThe\\n \\nCompany\\n \\nwill\\n \\nensure\\n \\nthat\\n \\nappropriate\\n \\nconfidentiality\\n \\nmeasures\\n \\nare\\n \\ntaken\\n \\nand\\n \\nwill\\n \\nnot\\n \\nretaliate\\n \\nagainst\\n \\nany\\n \\nindividual\\n \\nfor\\n \\nreporting\\n \\nviolations\\n \\nin\\n \\ngood\\n \\nfaith.\\n \\n \\nWSO2  Personnel  must  also  notify  the  Compliance  Officer  of  any  corrupt,  improper,  illegal,  or  other  \\nunusual\\n \\nrequests\\n \\nfor\\n \\npayments\\n \\nor\\n \\nother\\n \\nbenefits\\n \\nmade\\n \\nby\\n \\ncustomers,\\n \\nIntermediaries,\\n \\nvendors,\\n \\nbusiness\\n \\npartners,\\n \\ngovernment\\n \\nofficials,\\n \\nor\\n \\nCompany\\n \\nemployees.\\n   \\nBy\\n \\nreporting\\n \\nsuch\\n \\nmatters,\\n \\nyou\\n \\nwill\\n \\nenable\\n \\nus\\n \\nto\\n \\nexplore\\n \\noptions\\n \\nto\\n \\nachieve\\n \\nour\\n \\nbusiness\\n \\ngoals\\n \\nwithout\\n \\nhaving\\n \\nto\\n \\ninteract\\n \\nwith\\n \\nsuch\\n \\npersons\\n \\nor\\n \\nprovide\\n \\nimproper\\n \\nbenefits.\\n \\n \\nIX.   ACKNOWLEDGEMENT\\n  \\n \\n \\nPlease  click here to  certify  and  acknowledge  that  you  have  read  and  understood  the  contents  of  this  Policy.  \\n9', mimetype='text/plain', start_char_idx=0, end_char_idx=1519, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='7418ce46-8310-4ef9-898f-e4dc17c4a365', embedding=None, metadata={'page_label': '10', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comparative Analysis of Bribery Legislation: An Overview of the UK Bribery Act 2010 and Sri Lanka\\'s Bribery Act\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document regarding the UK Bribery Act 2010 and its implications, here are three specific questions that can be answered using the context:\\n\\n1. **What are the four principal offenses outlined in the UK Bribery Act 2010?**\\n   - The context details the four principal offenses under the UKBA, which include offering or giving a bribe, requesting or accepting a bribe, bribery of foreign public officials, and failure to prevent bribery by associated persons.\\n\\n2. **What statutory defense is available to companies under Section 7 of the UK Bribery Act 2010?**\\n   - The excerpt explains that companies can demonstrate a statutory defense for a Section 7 violation if they can show that they had adequate systems and controls in place to prevent offenses under the UKBA.\\n\\n3. **What are the potential penalties for individuals and commercial organizations found guilty of bribery under the UK Bribery Act 2010?**\\n   - The context specifies that individuals guilty of bribery may face imprisonment for up to 10 years and/or unlimited fines, while commercial organizations may also incur unlimited fines and debarment from government contracts.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b63099a8-4fb0-4c39-ab1c-43ac45eff3aa', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='47af95b92b7796b612289156b749cefb7452e9067ee6353b0a18fe2a1a97f461'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ff4512b1-e39f-4d9e-bc01-9fb24e1a2202', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='ff477b70da7cba2d59a9f390d614c5b8a771531c4c4f6dc170f7b25379e94211')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only ATTACHMENT  1   \\nANTI-CORRUPTION  LAWS  OF  THE  UNITED  KINGDOM,  SRI  LANKA,  AND  BRAZIL   \\n  \\nUNITED  KINGDOM   \\nT\\nHE\\n \\nUK\\n \\nB\\nRIBERY\\n \\nA\\nCT\\n \\n2010  \\n \\n  Among  various  matters,  the  UK  Bribery  Act  2010  (the  “ UKBA ”)  prohibits  individuals  and  entities  from  \\noffering,\\n \\npromising,\\n \\nor\\n \\ngiving\\n \\n(directly\\n \\nor\\n \\nindirectly\\n \\nthrough\\n \\na\\n \\nthird\\n \\nparty)\\n \\na\\n \\nfinancial\\n \\nor\\n \\nother\\n \\nadvantage\\n \\nto\\n \\na\\n \\nrecipient\\n \\nwith\\n \\n(i)\\n \\nthe\\n \\nintention\\n \\nthat\\n \\nthe\\n \\nadvantage\\n \\ninduce\\n \\nthe\\n \\nrecipient\\n \\nto\\n \\nperform\\n \\nimproperly\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity\\n \\nor\\n \\nto\\n \\nreward\\n \\na\\n \\nperson\\n \\nfor\\n \\nthe\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\nsuch\\n \\nfunction\\n \\nor\\n \\nactivity,\\n \\nor\\n \\n(ii)\\n \\nthe\\n \\nknowledge\\n \\nor\\n \\nbelief\\n \\nthat\\n \\nthe\\n \\nacceptance\\n \\nof\\n \\nthe\\n \\nadvantage\\n \\nwould\\n \\nitself\\n \\nconstitute\\n \\nan\\n \\nimproper\\n \\nperformance\\n \\nof\\n \\na\\n \\nrelevant\\n \\nfunction\\n \\nor\\n \\nactivity.\\n \\nA\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nUKBA\\n \\nwill\\n \\noccur\\n \\nirrespective\\n \\nof\\n \\nwhether\\n \\nthe\\n \\nrecipient\\n \\nof\\n \\nan\\n \\nimproper\\n \\npayment\\n \\nor\\n \\nadvantage\\n \\nis\\n \\na\\n \\ngovernment\\n \\nofficial\\n \\nor\\n \\nan\\n \\nemployee\\n \\nof\\n \\na\\n \\nprivatesector\\n \\nentity.\\n \\n   The  UKBA  contains  four  principal  offenses  as  follows:  (i)  offering,  promising,  or  giving  of  a  bribe  to  \\nanother\\n \\nperson\\n \\n(Section\\n \\n1);\\n \\n(ii)\\n \\nrequesting,\\n \\nagreeing\\n \\nto\\n \\nreceive,\\n \\nor\\n \\naccepting\\n \\na\\n \\nbribe\\n \\n(Section\\n \\n2);\\n \\n(iii)\\n \\nbribery\\n \\nof\\n \\na\\n \\nforeign\\n \\n(non-UK)\\n \\npublic\\n \\nofficial\\n \\n(Section\\n \\n6);\\n \\nand\\n \\n(iv)\\n \\nfailure\\n \\nby\\n \\ncertain\\n \\ncommercial\\n \\norganizations\\n \\nto\\n \\nprevent\\n \\nSection\\n \\n1\\n \\nor\\n \\n6\\n \\nbribery\\n \\noffenses\\n \\nby\\n \\ntheir\\n \\nassociated\\n \\npersons\\n \\n(including\\n \\nemployees,\\n \\ncontractors,\\n \\nIntermediaries,\\n \\nor\\n \\nanyone\\n \\nelse\\n \\nperforming\\n \\nservices\\n \\nfor\\n \\nor\\n \\non\\n \\nbehalf\\n \\nof\\n \\na\\n \\ncompany)\\n \\nof\\n \\nany\\n \\nnationality\\n \\nanywhere\\n \\nin\\n \\nthe\\n \\nworld\\n \\n(Section\\n \\n7).\\n  \\nThe\\n \\nUKBA\\n \\nprovides\\n \\na\\n \\nstatutory\\n \\ndefense\\n \\nto\\n \\na\\n \\nSection\\n \\n7\\n \\nviolation\\n \\nfor\\n \\ncompanies\\n \\nthat\\n \\ncan\\n \\ndemonstrate\\n \\nthat\\n \\nthey\\n \\nhad\\n \\nin\\n \\nplace\\n \\nadequate\\n \\nsystems\\n \\nand\\n \\ncontrols\\n \\ndesigned\\n \\nto\\n \\nprevent\\n \\noffenses\\n \\nunder\\n \\nUKBA.\\n \\nThis\\n \\nPolicy\\n \\nis\\n \\npart\\n \\nof\\n \\nthe\\n \\nCompany’s\\n \\noverall\\n \\neffort\\n \\nto\\n \\nestablish\\n \\nsuch\\n \\nsystems\\n \\nand\\n \\ncontrols.\\n  \\n   Courts  in  the  United  Kingdom  exercise  broad  jurisdiction  over  UK  as  well  as  non-UK  persons  who  \\ncommit\\n \\nUKBA\\n \\noffenses.\\n  \\nThe\\n \\nCompany\\n \\nmaintains\\n \\na\\n \\nUK\\n \\nsubsidiary.\\n \\nIt\\n \\nis\\n \\nclear\\n \\nthat\\n \\nboth\\n \\nthis\\n \\nUK\\n \\nsubsidiary\\n \\nand\\n \\nmost\\n \\nof\\n \\nits\\n \\nemployees\\n \\nwill\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nthe\\n \\nUKBA.\\n  \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-UK\\n \\nentities\\n \\nand\\n \\nemployees\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nUKBA\\n \\njurisdiction.\\n \\n    Under  the  UKBA,  individuals  guilty  of  bribery  may  be  subject  to  imprisonment  for  up  to  10  years  and/or  \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount.\\n \\nCommercial\\n \\norganizations\\n \\nguilty\\n \\nof\\n \\nbribery\\n \\nor\\n \\nfailure\\n \\nto\\n \\nprevent\\n \\nbribery\\n \\nmay\\n \\nalso\\n \\nbe\\n \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount\\n \\nas\\n \\nwell\\n \\nas\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts.', mimetype='text/plain', start_char_idx=0, end_char_idx=3198, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='ff4512b1-e39f-4d9e-bc01-9fb24e1a2202', embedding=None, metadata={'page_label': '10', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comparative Analysis of Bribery Legislation: An Overview of the UK Bribery Act 2010 and Sri Lanka\\'s Bribery Act\"', 'questions_this_excerpt_can_answer': \"Based on the provided excerpt from the document regarding the UK Bribery Act 2010 and Sri Lanka's Bribery Act, here are three specific questions that can be answered using the context:\\n\\n1. **What are the potential penalties for individuals and commercial organizations found guilty of bribery under the UK Bribery Act 2010?**\\n   - This question can be answered by referencing the specific penalties mentioned in the excerpt, including imprisonment terms and fines.\\n\\n2. **How does the UK Bribery Act 2010 relate to other legal frameworks, such as the UK Proceeds of Crime Act 2002?**\\n   - The excerpt indicates that offenses under the UKBA could lead to violations of other laws, specifically mentioning the Proceeds of Crime Act, which provides a direct connection between these legal frameworks.\\n\\n3. **What is the primary legal framework for addressing corruption in Sri Lanka as mentioned in the document?**\\n   - The excerpt specifically references the Bribery Act (Chapter 26 of the Legislative Enactments) as the main legal framework for preventing and punishing corruption in Sri Lanka, providing a clear answer to this question.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b63099a8-4fb0-4c39-ab1c-43ac45eff3aa', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='47af95b92b7796b612289156b749cefb7452e9067ee6353b0a18fe2a1a97f461'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7418ce46-8310-4ef9-898f-e4dc17c4a365', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '10', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='8551cb9027e2914b4a69e40236b5e5af8d4a956ce5d71efea82f083c81003500')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Under  the  UKBA,  individuals  guilty  of  bribery  may  be  subject  to  imprisonment  for  up  to  10  years  and/or  \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount.\\n \\nCommercial\\n \\norganizations\\n \\nguilty\\n \\nof\\n \\nbribery\\n \\nor\\n \\nfailure\\n \\nto\\n \\nprevent\\n \\nbribery\\n \\nmay\\n \\nalso\\n \\nbe\\n \\nsubject\\n \\nto\\n \\na\\n \\nfine\\n \\nof\\n \\nan\\n \\nunlimited\\n \\namount\\n \\nas\\n \\nwell\\n \\nas\\n \\ndebarment\\n \\nfrom\\n \\ngovernment\\n \\ncontracts.\\n  \\nIn\\n \\naddition,\\n \\nUKBA\\n \\noffenses\\n \\ncould\\n \\nresult\\n \\nin\\n \\nviolations\\n \\nof\\n \\nother\\n \\nlaws\\n \\nsuch\\n \\nas\\n \\nthe\\n \\nUK\\n \\nProceeds\\n \\nof\\n \\nCrime\\n \\nAct\\n \\n2002,\\n \\nwhich\\n \\ncontains\\n \\nthe\\n \\nUK’s\\n \\nprincipal\\n \\nmoney\\n \\nlaundering\\n \\noffenses.\\n \\n    *   *   *   *   *     SRI  LANKA   \\nThe  legal  framework  for  the  prevention,  investigation  and  punishment  of  corruption  is  primarily  reflected  \\nin\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\n(Chapter\\n \\n26\\n \\nof\\n \\nthe\\n \\nLegislative\\n \\nEnactments).', mimetype='text/plain', start_char_idx=2791, end_char_idx=3671, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='4adfd606-adfb-4374-a65c-68ff90d0b860', embedding=None, metadata={'page_label': '11', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Combating Corruption in Sri Lanka: An In-Depth Analysis of Prohibited Inducements, Bribery Legislation, and Governance Mechanisms\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document regarding the Anti-Corruption Policy, here are three specific questions that can be answered using the context:\\n\\n1. **What categories of persons are prohibited from receiving gratifications and rewards as inducements under the Anti-Corruption Policy?**\\n   - This question can be answered by referencing the specific categories listed in the excerpt, such as judicial officers, police officers, public officers, and tenderers.\\n\\n2. **What conditions must be met for a gratification offered to a public officer to not be considered an offense under the Anti-Corruption Policy?**\\n   - The excerpt outlines specific conditions regarding the timing and intent behind the offering of gratifications, which can be directly referenced to answer this question.\\n\\n3. **What actions are public officers prohibited from performing in exchange for gratifications according to the Anti-Corruption Policy?**\\n   - The excerpt details various actions that public officers cannot perform or influence in exchange for gratifications, such as interfering with justice or promoting government contracts, which can be used to answer this question comprehensively.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4764dc2-e803-434c-8331-41fe495858ee', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='ac675350ba575089e2bfe08a8b94a60ed7ebcbcab9db4a8f700041e1cb5fc1f5'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='d33b181f-1a06-4f75-b969-440c6847b4f6', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='cc53bc02c61991770ccbc4cb7d47856e11183fdc4a15bf5504b8d6c214e569e6')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only   The  law  prohibits  persons  from  offering  gratifications  and  rewards  to  certain  categories  of  persons  as  \\ninducements\\n \\nand\\n \\nrewards\\n \\nfor\\n \\nthe\\n \\nperformance\\n \\nor\\n \\nnonperformance\\n \\nof\\n \\nspecified\\n \\nactivities.\\n \\nThey\\n \\nare\\n \\n(a)\\n \\njudicial\\n \\nofficers\\n \\nand\\n \\nMembers\\n \\nof\\n \\nParliament\\n \\nin\\n \\nrespect\\n \\nof\\n \\ntheir\\n \\nofficial\\n \\nduties;\\n \\n(b)\\n \\npolice\\n \\nofficers,\\n \\npeace\\n \\nofficers\\n \\nor\\n \\nother\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ninterfering\\n \\nwith\\n \\nthe\\n \\ndue\\n \\nadministration\\n \\nof\\n \\njustice,\\n \\nor\\n \\nprocuring\\n \\nor\\n \\nfacilitating\\n \\nthe\\n \\ncommission\\n \\nof\\n \\nany\\n \\noffence,\\n \\nor\\n \\nprotecting\\n \\noffenders\\n \\nfrom\\n \\ndetection\\n \\nor\\n \\npunishment,\\n \\nor\\n \\nabusing\\n \\nofficial\\n \\npowers\\n \\nto\\n \\nthe\\n \\ninjury\\n \\nor\\n \\ndetriment\\n \\nof\\n \\nany\\n \\nperson;\\n \\n(c)\\n \\npublic\\n \\nofficers\\n \\nfor\\n \\ntheir\\n \\nassistance\\n \\nor\\n \\ninfluence\\n \\nin\\n \\npromoting\\n \\nthe\\n \\nprocurement\\n \\nof\\n \\nany\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nfor\\n \\nany\\n \\nwork,\\n \\nservice\\n \\nor\\n \\nthe\\n \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial\\n \\nor\\n \\nsubstance,\\n \\nor\\n \\nin\\n \\nthe\\n \\nexecution\\n \\nof\\n \\nany\\n \\ncontract,\\n \\nor\\n \\nin\\n \\nthe\\n \\npayment\\n \\nof\\n \\nthe\\n \\nprice\\n \\nor\\n \\nconsideration\\n \\nor\\n \\nof\\n \\nany\\n \\nsubsidy\\n \\nin\\n \\nrespect\\n \\nthereof;\\n \\n(d)\\n  \\na\\n \\ntenderer\\n \\nfor\\n \\na\\n \\ncontract\\n \\nto\\n \\nwithdraw\\n \\nthe\\n \\ntender,\\n \\nor\\n \\nfor\\n \\nwithdrawing\\n \\na\\n \\ntender\\n \\nmade\\n \\nfor\\n \\na\\n \\ncontract\\n \\nwith\\n \\nthe\\n \\nintent\\n \\nof\\n \\nobtaining\\n \\nsuch\\n \\ncontract\\n \\nfor\\n \\nwork,\\n \\nservice\\n \\nor\\n  \\nsupply\\n \\nof\\n \\nany\\n \\narticle,\\n \\nmaterial,\\n \\nor\\n \\nsubstance;\\n \\n(e)\\n  \\npublic\\n \\nofficers\\n \\nto\\n \\nperform,\\n \\nabstain\\n \\nfrom\\n \\nperforming,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact\\n \\nwhether\\n \\nby\\n \\nthat\\n \\npublic\\n \\nofficer\\n \\nor\\n \\nby\\n \\nany\\n \\nother\\n \\npublic\\n \\nofficer,\\n \\nor\\n \\nassisting,\\n \\nfavoring,\\n \\nhindering\\n \\nor\\n \\ndelaying\\n \\nany\\n \\nperson\\n \\nin\\n \\nthe\\n \\ntransaction\\n \\nof\\n \\nany\\n \\nbusiness\\n \\nwith\\n \\nthe\\n \\nGovernment;\\n \\n(f)\\n \\npersons\\n \\nto\\n \\nprocure\\n \\nthe\\n \\nGovernment\\n \\nto\\n \\npay\\n \\nany\\n \\nclaim,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nprevent\\n \\nappointment\\n \\nto\\n \\nany\\n \\noffice,\\n \\nor\\n \\nprocure\\n \\nor\\n \\nsecure\\n \\nany\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nother\\n \\nbenefit\\n \\nfrom\\n \\nthe\\n \\nGovernment,\\n \\nor\\n \\nprevent\\n \\nthe\\n \\nsecuring\\n \\nof\\n \\nany\\n \\nsuch\\n \\ngrant,\\n \\nlease\\n \\nor\\n \\nbenefit\\n \\nby\\n \\nsuch\\n \\nother\\n \\nperson;\\n \\n(g)\\n \\npublic\\n \\nofficer\\n \\nemployed\\n \\nin\\n \\na\\n \\ngovernment\\n \\ndepartment,\\n \\noffice\\n \\nor\\n \\nestablishment\\n \\nwhile\\n \\nhaving\\n \\ndealings\\n \\nof\\n \\nany\\n \\nkind\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nthrough\\n \\nsuch\\n \\nan\\n \\nentity,\\n \\nor\\n \\nwithin\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nsuch\\n \\ndealings\\n \\n(provided\\n \\nthat\\n \\nif\\n \\nsuch\\n \\ngratification\\n \\nwas\\n \\npaid\\n \\none\\n \\nyear\\n \\nbefore\\n \\nor\\n \\nafter\\n \\nthe\\n \\ndealings\\n \\nit\\n \\nshall\\n \\nnot\\n \\nbe\\n \\nconsidered\\n \\nan\\n \\noffence\\n \\nif\\n \\nit\\n \\ncan\\n \\nbe\\n \\nproved\\n \\nthat\\n \\nit\\n \\nwas\\n \\noffered\\n \\nin\\n \\ngood\\n \\nfaith\\n \\nfor\\n \\na\\n \\npurpose\\n \\nnot\\n \\nconnected\\n \\nwith\\n \\nor\\n \\nunrelated\\n \\nto\\n \\nsuch\\n \\ndealings,\\n \\nand\\n \\nwhen\\n \\nit\\n \\nwas\\n \\noffered,\\n \\nthere\\n \\nwas\\n \\nno\\n \\nhope\\n \\nor\\n \\nexpectation\\n \\nof\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nit\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nfor\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nthe\\n \\ngratification\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nor\\n \\nreward\\n \\nfor\\n \\nthe\\n \\npublic\\n \\nofficer\\n \\ndoing\\n \\nor\\n \\nforbearing\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nconnected\\n \\nwith\\n \\nsuch\\n \\ndealings);\\n \\n(h)\\n \\nmembers,', mimetype='text/plain', start_char_idx=0, end_char_idx=3323, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='d33b181f-1a06-4f75-b969-440c6847b4f6', embedding=None, metadata={'page_label': '11', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Combating Corruption in Sri Lanka: An In-Depth Analysis of Prohibited Inducements, Bribery Legislation, and Governance Mechanisms\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document regarding anti-corruption legislation in Sri Lanka, here are three specific questions that can be answered using the context:\\n\\n1. **What are the potential penalties for individuals convicted under the Bribery Act in Sri Lanka?**\\n   - The excerpt specifies that the Bribery Act provides for imprisonment of up to seven years and fines of up to five thousand Sri Lankan rupees for the commission of offences.\\n\\n2. **What powers does the Commission to Investigate Allegations of Bribery or Corruption possess in relation to public officials?**\\n   - The context indicates that the Commission has wide powers of investigation, including requiring declarations of assets and liabilities from various public officials, including Members of Parliament and judges.\\n\\n3. **What legislation governs the prevention of corruption among members of public authorities in Sri Lanka?**\\n   - The excerpt mentions that similar offences have been created in respect of members of public authorities by the Public Bodies (Prevention of Corruption) Act No 13 of 1950, highlighting the legal framework for addressing corruption in this context.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b4764dc2-e803-434c-8331-41fe495858ee', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='ac675350ba575089e2bfe08a8b94a60ed7ebcbcab9db4a8f700041e1cb5fc1f5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4adfd606-adfb-4374-a65c-68ff90d0b860', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '11', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='51dd961d8c8fcf3293ddf2bc9e4eef5d4c51c21c0f0e5b6bdf527226cdcd2086')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='there\\n \\nwas\\n \\nno\\n \\nhope\\n \\nor\\n \\nexpectation\\n \\nof\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nit\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nfor\\n \\nhaving\\n \\nsuch\\n \\ndealings,\\n \\nor\\n \\nthat\\n \\nthe\\n \\ngratification\\n \\nwas\\n \\nnot\\n \\nintended\\n \\nto\\n \\nbe\\n \\nan\\n \\ninducement\\n \\nor\\n \\nreward\\n \\nfor\\n \\nthe\\n \\npublic\\n \\nofficer\\n \\ndoing\\n \\nor\\n \\nforbearing\\n \\nto\\n \\ndo\\n \\nany\\n \\nact\\n \\nconnected\\n \\nwith\\n \\nsuch\\n \\ndealings);\\n \\n(h)\\n \\nmembers,\\n \\nofficers\\n \\nor\\n \\nemployees\\n \\nof\\n \\nlocal\\n \\nauthorities\\n \\nor\\n \\nscheduled\\n \\ninstitutions\\n \\nfor\\n \\nvoting\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nvoting\\n \\nat\\n \\nmeetings\\n \\nof\\n \\nsuch\\n \\nbodies\\n \\nfor\\n \\nor\\n \\nagainst\\n \\nmatters\\n \\narising\\n \\nbefore\\n \\nthem,\\n \\nor\\n \\ntheir\\n \\nperformance,\\n \\nor\\n \\nabstaining\\n \\nfrom\\n \\nperforming,\\n \\nor\\n \\naiding\\n \\nin\\n \\nprocuring,\\n \\nexpediting,\\n \\ndelaying,\\n \\nhindering\\n \\nor\\n \\npreventing\\n \\nthe\\n \\nperformance\\n \\nof\\n \\nany\\n \\nofficial\\n \\nact,\\n \\nor\\n \\naiding,\\n \\nprocuring,\\n \\nor\\n \\npreventing\\n \\nthe\\n \\npassing\\n \\nof\\n \\nany\\n \\nvote\\n \\nor\\n \\ngranting\\n \\nof\\n \\nany\\n \\ncontract\\n \\nor\\n \\nadvantage\\n \\nin\\n \\nfavor\\n \\nof\\n \\nany\\n \\nperson.\\n  \\n \\nEach  of  the  above  provisions  have  corresponding  offence  in  respect  of  the  receipt  of  gratifications.      The  Bribery  Act  provides  for  imprisonment  of  up  to  seven  years  and  fines  of  up  to  five  thousand  Sri  \\nLankan\\n \\nrupees\\n \\nfor\\n \\nthe\\n \\ncommission\\n \\nof\\n \\noffences.\\n  \\nSri\\n \\nLankan\\n \\ncourts\\n \\nmay\\n \\nalso\\n \\nimpose\\n \\npenalties\\n \\namounting\\n \\nto\\n \\nthe\\n \\nvalue\\n \\nof\\n \\nthe\\n \\ngratification\\n \\nif\\n \\nthe\\n \\nconviction\\n \\nis\\n \\nentered\\n \\nby\\n \\na\\n \\nHigh\\n \\nCourt.\\n   \\n   Similar  offences  have  been  created  in  respect  of  members  of  public  authorities  by  the  Public  Bodies  \\n(Prevention\\n \\nof\\n \\nCorruption)\\n \\nAct\\n \\nNo\\n \\n13\\n \\nof\\n \\n1950.\\n \\n \\nThe  Bribery  Act  is  enforced  through  the  Commission  to  Investigate  Allegations  of  Bribery  or  Corruption  \\nAct\\n \\nNo.\\n \\n19\\n \\nof\\n \\n1994.\\n  \\nThe\\n \\nCommission\\n \\nhas\\n \\nwide\\n \\npowers\\n \\nof\\n \\ninvestigation\\n \\nincluding\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nas\\n \\nwell\\n \\nas\\n \\nof\\n \\nrequiring\\n \\ndeclaration\\n \\nof\\n \\nassets\\n \\nand\\n \\nliabilities\\n \\nby\\n \\nMembers\\n \\nof\\n \\nParliament,\\n \\njudges,\\n \\npublic\\n \\nofficials\\n \\nof\\n \\nGovernment\\n \\ndepartments,\\n \\nministries,\\n \\nand\\n \\nlocal\\n \\nauthorities,\\n \\nchairpersons\\n \\nand\\n \\nstaff\\n \\nof\\n \\npublic\\n \\ncorporations,\\n \\ncandidates\\n \\nfor\\n \\nelected\\n \\npublic\\n \\noffice\\n \\nand\\n \\nelected\\n \\nofficials\\n \\nunder\\n \\nthe\\n \\nDeclaration\\n \\nof\\n \\nAssets\\n \\nand\\n \\nLiabilities\\n \\nLaw,\\n \\nNo.\\n \\n1\\n \\nof\\n \\n1975.', mimetype='text/plain', start_char_idx=2912, end_char_idx=5311, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='e34d65a6-00cd-496e-90fd-a53230ecaa16', embedding=None, metadata={'page_label': '12', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comparative Analysis of Anti-Corruption Legal Frameworks: Implications for Legal Entities in Sri Lanka and Brazil\"', 'questions_this_excerpt_can_answer': 'Based on the provided context regarding the anti-corruption legal frameworks in Sri Lanka and Brazil, here are three specific questions that can be answered using the excerpt:\\n\\n1. **What are the main offenses outlined in the Brazilian Anti-Corruption Act (LAC) that can lead to liability for companies?**\\n   - The excerpt details specific offenses under the LAC, including promising or giving undue advantages to public agents, financing wrongdoing, using third parties to conceal interests, engaging in fraudulent acts in public tenders, and obstructing investigations.\\n\\n2. **What penalties can legal entities face under the Brazilian Anti-Corruption Act if found guilty of violations?**\\n   - The excerpt specifies that penalties can include fines ranging from 0.1% to 20% of gross revenue, seizure of assets, suspension of activities, compulsory termination, and prohibition from receiving public incentives for a period of 1 to 5 years.\\n\\n3. **How does the existence of a compliance program affect the penalties imposed on legal entities under the Brazilian Anti-Corruption Act?**\\n   - The excerpt mentions that having an effective compliance program may be considered a mitigating factor in evaluating the applicable fine for a legal entity found guilty under the LAC.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='195d34a8-a985-45cb-8cc6-2d2e1d480c51', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='9116da3b61d7a3c395dacc7b3001ac83940d7fb4097c459752d1a363ec9d1da2'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a02b3657-d1fa-4b00-ba43-322f42d32d97', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='78f78af3c3be6125a4817710452c9c5407a4abde16d99348784b391d4ac741b0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only Sri  Lanka  also  has  a  domestic  legal  regime  against  money-laundering  which  includes  the  Prevention  of  \\nMoney-\\n \\nLaundering\\n \\nAct,\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct\\n \\nand\\n \\nthe\\n \\nConvention\\n \\non\\n \\nthe\\n \\nSuppression\\n \\nof\\n \\nTerrorist\\n \\nFinancing\\n \\nAct.\\n  \\nOffences\\n \\nunder\\n \\nthe\\n \\nBribery\\n \\nAct\\n \\nand\\n \\nother\\n \\ncorruption-related\\n \\noffences\\n \\nunder\\n \\nthe\\n \\nPenal\\n \\nCode\\n \\nare\\n \\nconsidered\\n \\npredicate\\n \\noffences\\n \\nfor\\n \\nthe\\n \\npurposes\\n \\nof\\n \\nthe\\n \\nPrevention\\n \\nof\\n \\nMoney-Laundering\\n \\nAct\\n \\nand\\n \\nthe\\n \\nFinancial\\n \\nTransactions\\n \\nReporting\\n \\nAct.\\n  \\n \\n     *   *   *   *   *     B\\nRAZIL\\n \\n \\nTHE  BRAZILIAN  ANTICORRUPTION  ACT  2013   \\nLaw  No.  12,846/2013  (the  Brazilian  Anticorruption  Act  or  Lei  Anticorrupção  –  “ LAC ”)  provides  for  \\nstrict\\n \\nliability\\n \\nto\\n \\ncompanies,\\n1\\n \\nin\\n \\nthe\\n \\nadministrative\\n \\nand\\n \\ncivil\\n \\nspheres,\\n \\nfor\\n \\nwrongful\\n \\nacts\\n \\ncarried\\n \\nout\\n \\nin\\n \\ntheir\\n \\ninterest\\n \\nor\\n \\nfor\\n \\ntheir\\n \\nbenefit.\\n \\n   The  LAC  is  applicable  to  activities  after  January  2014  and  the  main  offenses  are:  (i)  promising,  offering  \\nor\\n \\ngiving,\\n \\ndirectly\\n \\nor\\n \\nindirectly,\\n \\nundue\\n \\nadvantage\\n \\nto\\n \\na\\n \\npublic\\n \\nagent\\n \\nor\\n \\nthird\\n \\nperson\\n \\nrelated\\n \\nto\\n \\nit;\\n \\n(ii)\\n \\nfinancing\\n \\nor\\n \\nin\\n \\nany\\n \\nway\\n \\nsponsoring\\n \\nthe\\n \\npractice\\n \\nof\\n \\nwrongdoings\\n \\ndescribed\\n \\nin\\n \\nthe\\n \\nAct;\\n \\n(iii)\\n \\nusing\\n \\na\\n \\nthird\\n \\nparty\\n \\nto\\n \\nconceal\\n \\nor\\n \\nsimulate\\n \\nits\\n \\nactual\\n \\ninterests\\n \\nor\\n \\nthe\\n \\nidentity\\n \\nof\\n \\nthe\\n \\nbeneficiaries\\n \\nof\\n \\nthe\\n \\nillegal\\n \\nacts\\n \\nagainst\\n \\nthe\\n \\npublic\\n \\nadministration;\\n \\n(iv)\\n \\nengaging\\n \\nin\\n \\nfraudulent\\n \\nacts\\n \\nin\\n \\npublic\\n \\ntenders,\\n \\nsuch\\n \\nas\\n \\nparticipating\\n \\nin\\n \\nbid\\n \\nrigging\\n \\nor\\n \\ndisturbing\\n \\nany\\n \\nstep\\n \\nof\\n \\nthe\\n \\npublic\\n \\ntender;\\n \\nand\\n \\n(v)\\n \\nobstructing\\n \\nor\\n \\nhampering\\n \\nthe\\n \\nsurveillance\\n \\nor\\n \\ninvestigations\\n \\nof\\n \\npublic\\n \\nentities.\\n \\n   In  the  administrative  sphere,  legal  entities  that  are  found  guilty  of  breaching  the  LAC  are  subject  to  a  fine  \\nof\\n \\n0.1%\\n \\nto\\n \\n20%\\n \\nof\\n \\nthe\\n \\ngross\\n \\nrevenue,\\n \\nless\\n \\ntaxes,\\n \\nregistered\\n \\nin\\n \\nthe\\n \\nyear\\n \\nprior\\n \\nto\\n \\nthe\\n \\ninitiation\\n \\nof\\n \\nthe\\n \\nadministrative\\n \\nproceedings,\\n \\nin\\n \\naddition\\n \\nto\\n \\nthe\\n \\npublication\\n \\nof\\n \\nthe\\n \\ndecision.\\n \\nMoreover,\\n \\nother\\n \\npenalties\\n \\nmay\\n \\nbe\\n \\nenforced\\n \\nin\\n \\nthe\\n \\ncivil\\n \\nsphere\\n \\nby\\n \\ncourts,\\n \\nsuch\\n \\nas:\\n \\n(i)\\n \\nseizure\\n \\nof\\n \\nassets\\n \\nobtained\\n \\nthrough\\n \\nillegal\\n \\npractice;\\n \\n(ii)\\n \\nsuspension\\n \\nor\\n \\npartial\\n \\nshutdown\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity’s\\n \\nactivities;\\n \\n(iii)\\n \\ncompulsory\\n \\ntermination\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity;\\n \\nand\\n \\n(iv)\\n \\nprohibition\\n \\nof\\n \\nreceiving\\n \\nincentives,\\n \\nsubsidies,\\n \\ngrants,\\n \\ndonations\\n \\nor\\n \\nloans\\n \\nfrom\\n \\npublic\\n \\nbodies\\n \\nor\\n \\nentities\\n \\nor\\n \\nfrom\\n \\npublic\\n \\nfinancial\\n \\ninstitutions\\n \\nor\\n \\npublicly-controlled\\n \\nfinancial\\n \\ninstitutions,\\n \\nfrom\\n \\n1\\n \\nto\\n \\n5\\n \\nyears.\\n \\n   The  existence  of  an  effective  compliance  program  may  be  considered  a  mitigating  factor  in  the  evaluation  \\nof\\n \\nthe\\n \\napplicable\\n \\nfine\\n \\nto\\n \\nbe\\n \\napplied\\n \\nto\\n \\nthe\\n \\nlegal\\n \\nentity.\\n \\n   The  Company  maintains  a  Brazilian  subsidiary.  It  is  clear  that  the  Brazilian  subsidiary  may  be  held  liable  \\nunder\\n \\nthe\\n \\nLAC.', mimetype='text/plain', start_char_idx=0, end_char_idx=3243, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='a02b3657-d1fa-4b00-ba43-322f42d32d97', embedding=None, metadata={'page_label': '12', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comparative Analysis of Anti-Corruption Legal Frameworks: Implications for Legal Entities in Sri Lanka and Brazil\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"Comparative Analysis of Anti-Corruption Legal Frameworks: Implications for Legal Entities in Sri Lanka and Brazil,\" here are three specific questions that can be answered using the context:\\n\\n1. **What factors may mitigate the fines applied to a legal entity under the Brazilian Anti-Corruption Law (LAC)?**\\n   - The excerpt mentions that the existence of an effective compliance program may be considered a mitigating factor in the evaluation of the applicable fine.\\n\\n2. **Under what circumstances can a non-Brazilian entity be subject to the jurisdiction of the Brazilian Anti-Corruption Law (LAC)?**\\n   - The text indicates that there could be circumstances where the Company’s non-Brazilian entities could be subject to LAC jurisdiction, even if they are not based in Brazil.\\n\\n3. **Is intent or fault required for a legal entity to be held liable under the Brazilian Anti-Corruption Law (LAC)?**\\n   - The excerpt clarifies that authorities are not required to show intent or fault of the legal entity; rather, the materiality of the violation and its occurrence in the interest of the legal entity are sufficient for a breach to be considered under the Act.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='195d34a8-a985-45cb-8cc6-2d2e1d480c51', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='9116da3b61d7a3c395dacc7b3001ac83940d7fb4097c459752d1a363ec9d1da2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e34d65a6-00cd-496e-90fd-a53230ecaa16', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '12', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='db280ec48912d80fdafc057db77db021117c968032b8a1736ffdea67bebcddf9')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The  existence  of  an  effective  compliance  program  may  be  considered  a  mitigating  factor  in  the  evaluation  \\nof\\n \\nthe\\n \\napplicable\\n \\nfine\\n \\nto\\n \\nbe\\n \\napplied\\n \\nto\\n \\nthe\\n \\nlegal\\n \\nentity.\\n \\n   The  Company  maintains  a  Brazilian  subsidiary.  It  is  clear  that  the  Brazilian  subsidiary  may  be  held  liable  \\nunder\\n \\nthe\\n \\nLAC.\\n \\nIn\\n \\naddition,\\n \\nthere\\n \\ncould\\n \\nbe\\n \\ncircumstances\\n \\nwhere\\n \\nthe\\n \\nCompany’s\\n \\nnon-Brazilian\\n \\nentities\\n \\ncould\\n \\nbe\\n \\nsubject\\n \\nto\\n \\nLAC\\n \\njurisdiction.\\n \\n   \\n1\\n  In  this  sense,  the  authorities  are  not  required  to  show  intent  or  fault  of  the  legal  entity.  The  mere  fact  that  there  is  \\nmateriality\\n \\nas\\n \\nto\\n \\nthe\\n \\nviolation\\n \\nand\\n \\nthat\\n \\nthe\\n \\nviolation\\n \\nhappened\\n \\nin\\n \\nthe\\n \\ninterest\\n \\nor\\n \\nbenefit\\n \\nof\\n \\nthe\\n \\nlegal\\n \\nentity\\n \\nis\\n \\nsufficient\\n \\nto\\n \\nconsider\\n \\nthat\\n \\nthere\\n \\nis\\n \\na\\n \\nbreach\\n \\nto\\n \\nthe\\n \\nAct.', mimetype='text/plain', start_char_idx=2895, end_char_idx=3787, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'),\n",
            " TextNode(id_='bf9c5f7e-b447-4b5d-99eb-bd4d63de30b1', embedding=None, metadata={'page_label': '13', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comprehensive Analysis of Brazilian Anti-Corruption Legislation: The Improbity Act, Sanctions, and Liabilities\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document regarding Brazilian anti-corruption legislation, here are three specific questions that can be answered using the context:\\n\\n1. **What are the three broad types of misconduct defined by the Brazilian Improbity Act (LIA)?**\\n   - The three types of misconduct are: (i) unjust enrichment, (ii) damage to the public treasury, and (iii) acts in violation of the principles that govern the Public Administration.\\n\\n2. **What are the main sanctions imposed under the Brazilian Improbity Act for individuals or entities found guilty of improbity acts?**\\n   - The main sanctions include: (i) forfeiture of unlawfully obtained assets, (ii) dismissal from public office, (iii) political blacklisting, (iv) payment of fines equivalent to unlawfully obtained amounts, (v) prohibition against contracting with the Government or receiving tax or credit incentives, and (vi) payment of fines.\\n\\n3. **What additional liabilities can private and public entities pursue in cases of corruption beyond the sanctions outlined in the LIA?**\\n   - Entities can request compensation for collective or moral damages under the Brazilian Class Action Law, file lawsuits for damages, and the Brazilian Federal Court of Accounts can impose sanctions for contract fraud discovered during audits. Additionally, the Brazilian Criminal Code allows for imprisonment of up to 16 years for individuals convicted of corruption.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f6cad862-e011-4155-ad8a-495abdfb0e32', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': 'WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_path': '/content/data/WSO2 LLC - Anti-Corruption Policy .docx.pdf', 'file_type': 'application/pdf', 'file_size': 503440, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='47a77dde2253c9f4195267adf69cef6aaa2aad1afda8c0800748644cb6499b2c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Anti-Corruption  Policy  V1.0  \\n Internal  Use  Only T\\nHE\\n \\nB\\nRAZILIAN\\n \\nI\\nMPROBITY\\n \\nA\\nCT\\n \\n1992  \\n \\n  In  a  broad  sense,  Law  No.  8,429/1992  (the  Brazilian  Improbity  Act  or  Lei  de  Improbidade  Administrativa   –  “ LIA ”)  is  applicable  to  (i)  facts  that  also  constitute  a  violation  to  the  LAC  but  happened  before  January   \\n  2014;  and  (ii)  facts  that  happened  after  January  2014  and  do  not  constitute  a  breach  to  the  LAC,  but  are  a  \\nbreach\\n \\nof\\n \\nthe\\n \\nLIA.\\n \\n   There  are  three  broad  types  of  misconduct  provided  for  in  the  LIA:  (i)  unjust  enrichment;  (ii)  damage  to  \\nthe\\n \\npublic\\n \\ntreasury;\\n \\nand\\n \\n(iii)\\n \\nacts\\n \\nin\\n \\nviolation\\n \\nof\\n \\nthe\\n \\nprinciples\\n \\nthat\\n \\ngovern\\n \\nthe\\n \\nPublic\\n \\nAdministration.\\n  \\nThe\\n \\nLIA\\n \\nprovides\\n \\nfor\\n \\nsanctions\\n \\non\\n \\npublic\\n \\nagents,\\n \\nas\\n \\nwell\\n \\nas\\n \\non\\n \\nprivate\\n \\nentities\\n \\nand\\n \\nindividuals\\n \\nthat\\n \\nwillfully\\n \\naided\\n \\nor\\n \\nparticipated\\n \\nin\\n \\nimprobity\\n \\nacts.\\n  \\nIts\\n \\nmain\\n \\nsanctions\\n \\nare\\n \\n(i)\\n \\nforfeiture\\n \\nof\\n \\nassets\\n \\nor\\n \\nvalues\\n \\nunlawfully\\n \\nobtained;\\n \\n(ii)\\n \\ndismissal\\n \\nfrom\\n \\npublic\\n \\noffice;\\n \\n(iii)\\n \\npolitical\\n \\nblacklisting;\\n \\n(iv)\\n \\npayment\\n \\nof\\n \\nfines\\n \\nequivalent\\n \\nto\\n \\nthe\\n \\nunlawfully\\n \\nobtained\\n \\namounts;\\n \\n(v)\\n \\nprohibition\\n \\nagainst\\n \\ncontracting\\n \\nwith\\n \\nthe\\n \\nGovernment\\n \\nor\\n \\nreceiving\\n \\ntax\\n \\nor\\n \\ncredit\\n \\nincentives,\\n \\ndirectly\\n \\nor\\n \\nindirectly;\\n \\nand\\n \\n(vi)\\n \\npayment\\n \\nof\\n \\nfines.\\n  \\nIn\\n \\norder\\n \\nfor\\n \\na\\n \\nsanction\\n \\nto\\n \\nbe\\n \\napplied,\\n \\nit\\n \\nis\\n \\nnecessary\\n \\nto\\n \\nshow\\n \\nintent\\n \\nof\\n \\nthe\\n \\nwrongdoer.\\n \\n   O\\nTHER\\n \\nP\\nOTENTIAL\\n \\nL\\nIABILITIES\\n \\n \\nIn  addition  to  the  LAC  and  the  LIA,  private  and  public  entities  can  request  compensation  for  collective  or  \\nmoral\\n \\ndamages\\n \\nresulting\\n \\nfrom\\n \\ncorruption\\n \\ncases,\\n \\nas\\n \\nprovided\\n \\nby\\n \\nBrazilian\\n \\nClass\\n \\nAction\\n \\nLaw,\\n \\nand\\n \\nentities\\n \\ndeemed\\n \\nto\\n \\nbe\\n \\nharmed/damaged\\n \\nby\\n \\nthe\\n \\nwrongdoing\\n \\ncan\\n \\nfile\\n \\na\\n \\nlawsuit\\n \\nclaiming\\n \\ncompensation\\n \\nfor\\n \\ndamages.\\n \\nAlso,\\n \\nthe\\n \\nBrazilian\\n \\nFederal\\n \\nCourt\\n \\nof\\n \\nAccounts\\n \\ncan\\n \\nimpose\\n \\nsanctions\\n \\nif\\n \\nthey\\n \\nfind\\n \\ncontract\\n \\nfraud\\n \\nwhile\\n \\nauditing\\n \\npublic\\n \\nentities.\\n \\n   Finally,  the  Brazilian  Criminal  Code  sets  forth  that  corruption  may  result  in  imprisonment  for  up  to  16  \\nyears\\n \\nfor\\n \\nindividuals.', mimetype='text/plain', start_char_idx=0, end_char_idx=2311, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n')]\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_3XLQTXTU5-",
        "outputId": "dfa2a413-dc97-47d9-b8d1-9338383b1804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'embedding': None,\n",
            " 'end_char_idx': 4119,\n",
            " 'excluded_embed_metadata_keys': ['file_name',\n",
            "                                  'file_type',\n",
            "                                  'file_size',\n",
            "                                  'creation_date',\n",
            "                                  'last_modified_date',\n",
            "                                  'last_accessed_date',\n",
            "                                  'page_label'],\n",
            " 'excluded_llm_metadata_keys': ['file_name',\n",
            "                                'file_type',\n",
            "                                'file_size',\n",
            "                                'creation_date',\n",
            "                                'last_modified_date',\n",
            "                                'last_accessed_date'],\n",
            " 'id_': 'a13bb56f-dedd-4a24-8a80-24203d73bffb',\n",
            " 'metadata': {'creation_date': '2025-09-01',\n",
            "              'document_title': '\"HealthGPT: A Unified Vision-Language Model '\n",
            "                                'for Enhanced Comprehension and Generation in '\n",
            "                                'Healthcare Applications\"',\n",
            "              'file_name': '2502.09838v3.pdf',\n",
            "              'file_path': '/content/data/2502.09838v3.pdf',\n",
            "              'file_size': 8786923,\n",
            "              'file_type': 'application/pdf',\n",
            "              'last_modified_date': '2025-09-01',\n",
            "              'page_label': '1',\n",
            "              'questions_this_excerpt_can_answer': 'Based on the provided '\n",
            "                                                   'context about HealthGPT, '\n",
            "                                                   'here are three specific '\n",
            "                                                   'questions that can be '\n",
            "                                                   'answered using the '\n",
            "                                                   'information in the '\n",
            "                                                   'excerpt:\\n'\n",
            "                                                   '\\n'\n",
            "                                                   '1. **What is the novel '\n",
            "                                                   'technique introduced in '\n",
            "                                                   'HealthGPT for adapting '\n",
            "                                                   'heterogeneous knowledge to '\n",
            "                                                   'pre-trained large language '\n",
            "                                                   'models?**\\n'\n",
            "                                                   '   - Answer: The novel '\n",
            "                                                   'technique introduced in '\n",
            "                                                   'HealthGPT for adapting '\n",
            "                                                   'heterogeneous knowledge is '\n",
            "                                                   'called heterogeneous '\n",
            "                                                   'low-rank adaptation '\n",
            "                                                   '(H-LoRA).\\n'\n",
            "                                                   '\\n'\n",
            "                                                   '2. **What types of medical '\n",
            "                                                   'imaging tasks does '\n",
            "                                                   'HealthGPT excel in, '\n",
            "                                                   'according to the '\n",
            "                                                   'experimental results?**\\n'\n",
            "                                                   '   - Answer: HealthGPT '\n",
            "                                                   'excels in various medical '\n",
            "                                                   'multi-modal comprehension '\n",
            "                                                   'tasks, such as X-Ray, CT, '\n",
            "                                                   'MRI, microscopy, OCT, '\n",
            "                                                   'fundus, and ultrasound '\n",
            "                                                   'comprehension, as well as '\n",
            "                                                   'generation tasks like '\n",
            "                                                   'CT2MRI, MRI2CT, image '\n",
            "                                                   'reconstruction, super '\n",
            "                                                   'resolution, and '\n",
            "                                                   'report-to-CXR.\\n'\n",
            "                                                   '\\n'\n",
            "                                                   '3. **What is the purpose '\n",
            "                                                   'of the VL-Health dataset '\n",
            "                                                   'mentioned in the context '\n",
            "                                                   'of HealthGPT?**\\n'\n",
            "                                                   '   - Answer: The VL-Health '\n",
            "                                                   'dataset is devised to '\n",
            "                                                   'effectively learn the '\n",
            "                                                   'HealthGPT model by '\n",
            "                                                   'providing a comprehensive '\n",
            "                                                   'medical domain-specific '\n",
            "                                                   'comprehension and '\n",
            "                                                   'generation dataset.\\n'\n",
            "                                                   '\\n'\n",
            "                                                   'These questions focus on '\n",
            "                                                   'specific aspects of '\n",
            "                                                   'HealthGPT that are '\n",
            "                                                   'detailed in the excerpt '\n",
            "                                                   'and are unlikely to be '\n",
            "                                                   'found in other sources.'},\n",
            " 'metadata_separator': '\\n',\n",
            " 'metadata_seperator': '\\n',\n",
            " 'metadata_template': '{key}: {value}',\n",
            " 'mimetype': 'text/plain',\n",
            " 'relationships': {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='487b9093-1ad3-4685-ab72-3428a9365856', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='afce7e3d8c0798d4ac08319b358679811e93662be69476f75dfdaf5712d628b2')},\n",
            " 'start_char_idx': 0,\n",
            " 'text': 'HealthGPT: A Medical Large Vision-Language Model for Unifying\\n'\n",
            "         'Comprehension and Generation via Heterogeneous Knowledge Adaptation\\n'\n",
            "         'Tianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, '\n",
            "         'Haoyuan Li3, Wanggui He3, Hao Jiang3,\\n'\n",
            "         'Mengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, '\n",
            "         'Yueting Zhuang1, Beng Chin Ooi5\\n'\n",
            "         '1Zhejiang University, 2University of Electronic Science and '\n",
            "         'Technology of China, 3Alibaba,\\n'\n",
            "         '4The Hong Kong University of Science and Technology,5National '\n",
            "         'University of Singapore\\n'\n",
            "         'Project Page\\n'\n",
            "         ' Code\\n'\n",
            "         '1. X-Ray \\n'\n",
            "         'Comprehension\\n'\n",
            "         '2. CT\\n'\n",
            "         'Comprehension\\n'\n",
            "         '3. MRI\\n'\n",
            "         'Comprehension\\n'\n",
            "         'Comp. Perf.\\n'\n",
            "         '7 Medical Multi-Modal Comprehension Tasks\\n'\n",
            "         'Gen. \\n'\n",
            "         'Performance\\n'\n",
            "         '5 Medical Multi-Modal Generation Tasks\\n'\n",
            "         'List all anatomical locations showing \\n'\n",
            "         'pulmonary edema, hazy opacity, or \\n'\n",
            "         'mediastinal displacement.\\n'\n",
            "         'Left hilar structures, left lung,\\n'\n",
            "         'right hilar structures, right lung.\\n'\n",
            "         'Which abdominal organ shows any \\n'\n",
            "         'indication of a lesion or abnormality \\n'\n",
            "         'in the CT image?\\n'\n",
            "         'No abdominal organs show\\n'\n",
            "         'any clear indications of lesions\\n'\n",
            "         'or abnormalities.\\n'\n",
            "         'Could you explain what this mass\\n'\n",
            "         'in the MRI means for my health? \\n'\n",
            "         'Is it very serious?\\n'\n",
            "         'Certainly, the MRI shows a\\n'\n",
            "         'defined mass in your left nasal\\n'\n",
            "         'cavity. There is no sign of the ...\\n'\n",
            "         '4. Microsopy Comprehension\\n'\n",
            "         ' What is the purpose of the different \\n'\n",
            "         'membrane treatments used in this \\n'\n",
            "         'study?\\n'\n",
            "         'The purpose of the different\\n'\n",
            "         'membrane treatments used…\\n'\n",
            "         '5. OCT Comprehension\\n'\n",
            "         ' What is the purpose of comparing \\n'\n",
            "         'the OCT structure image and OCTA \\n'\n",
            "         'image with H&E histology?\\n'\n",
            "         'To confirm the histological position\\n'\n",
            "         'of the obtained OCT brain images.\\n'\n",
            "         '6. Fundus\\n'\n",
            "         'Comprehension\\n'\n",
            "         ' What specific findings or pathological \\n'\n",
            "         'changes can be observed in this \\n'\n",
            "         'fundus image?\\n'\n",
            "         'The fundus image appears normal with\\n'\n",
            "         'no noticeable signs of pathology…\\n'\n",
            "         '7. Ultrasound\\n'\n",
            "         'Comprehension\\n'\n",
            "         ' What type of imaging technique \\n'\n",
            "         'is used in this image?\\n'\n",
            "         'The image is a sagittal gray-\\n'\n",
            "         'scale ultrasonographic…\\n'\n",
            "         '1. CT2MRI\\n'\n",
            "         'Generation\\n'\n",
            "         'I need a version of this CT representation \\n'\n",
            "         'in MRI.\\n'\n",
            "         'The image has\\n'\n",
            "         'been transformed\\n'\n",
            "         'into MRI.\\n'\n",
            "         '2. MRI2CT\\n'\n",
            "         'Generation\\n'\n",
            "         'Transform the MRI display into a \\n'\n",
            "         'CT image.\\n'\n",
            "         'Here is the CT\\n'\n",
            "         'version of the\\n'\n",
            "         'MRI image.\\n'\n",
            "         '3. Image Reconstruction\\n'\n",
            "         'Reconstruct the following \\n'\n",
            "         'medical images.\\n'\n",
            "         'Here is the reconstructed\\n'\n",
            "         'medical image you need.\\n'\n",
            "         '4. Super Resolution\\n'\n",
            "         'Could you improve the quality\\n'\n",
            "         'of this MRI image?\\n'\n",
            "         'Here is the image with\\n'\n",
            "         'improved resolution.\\n'\n",
            "         '5. Report-to-CXR\\n'\n",
            "         'The X-ray shows no \\n'\n",
            "         'pleural effusion or \\n'\n",
            "         'pneumothorax.\\n'\n",
            "         'Here is the\\n'\n",
            "         'chest X-ray\\n'\n",
            "         'image for\\n'\n",
            "         'you.\\n'\n",
            "         'Gen. Perf.\\n'\n",
            "         'Figure 1: HealthGPT enables medical multi-modal comprehension and '\n",
            "         'generation , outperforming both state-of-the-art\\n'\n",
            "         'unified visual models and medical-specific models across various '\n",
            "         'tasks. This highlights its superior capability in tackling com-\\n'\n",
            "         'plex tasks in healthcare applications. Comp.Perf. and Gen.Perf. '\n",
            "         'denote the results of comprehension and generation.\\n'\n",
            "         'Abstract\\n'\n",
            "         'We present HealthGPT, a powerful Medical Large Vision-\\n'\n",
            "         'Language Model (Med-LVLM) that integrates medical vi-\\n'\n",
            "         'sual comprehension and generation capabilities within a uni-\\n'\n",
            "         'fied autoregressive paradigm. Our bootstrapping philosophy\\n'\n",
            "         'is to progressively adapt heterogeneous comprehension and\\n'\n",
            "         'generation knowledge to pre-trained large language mod-\\n'\n",
            "         'els (LLMs). This is achieved through a novel heterogeneous\\n'\n",
            "         'low-rank adaptation (H-LoRA) technique, which is com-\\n'\n",
            "         'plemented by a tailored hierarchical visual perception ap-\\n'\n",
            "         'proach and a three-stage learning strategy. To effectively\\n'\n",
            "         'learn the HealthGPT, we devise a comprehensive medi-\\n'\n",
            "         'cal domain-specific comprehension and generation dataset\\n'\n",
            "         'called VL-Health. Experimental results demonstrate ex-\\n'\n",
            "         'ceptional performance and scalability of HealthGPT in\\n'\n",
            "         'medical visual unified tasks. Our project can be accessed at\\n'\n",
            "         'https://github.com/DCDmllm/HealthGPT.\\n'\n",
            "         '1 Introduction\\n'\n",
            "         'Large Vision-Language Models (LVLMs) (Liu et al. 2023;\\n'\n",
            "         'OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\\n'\n",
            "         'demonstrated outstanding open-world visual comprehension\\n'\n",
            "         'and reasoning abilities through language-based interactive\\n'\n",
            "         'dialogue over the past years, simultaneously opening up\\n'\n",
            "         'new opportunities for applications in specialized domains.\\n'\n",
            "         '1\\n'\n",
            "         'arXiv:2502.09838v3  [cs.CV]  21 Feb 2025',\n",
            " 'text_template': '[Excerpt from document]\\n'\n",
            "                  '{metadata_str}\\n'\n",
            "                  'Excerpt:\\n'\n",
            "                  '-----\\n'\n",
            "                  '{content}\\n'\n",
            "                  '-----\\n'}\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(nodes[0].__dict__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FhZwNc7TU8v",
        "outputId": "7dd6f6c2-6c50-44ca-b1ae-db86909349cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Excerpt from document]\n",
            "page_label: 1\n",
            "file_path: /content/data/2502.09838v3.pdf\n",
            "document_title: \"HealthGPT: A Unified Vision-Language Model for Enhanced Comprehension and Generation in Healthcare Applications\"\n",
            "questions_this_excerpt_can_answer: Based on the provided context about HealthGPT, here are three specific questions that can be answered using the information in the excerpt:\n",
            "\n",
            "1. **What is the novel technique introduced in HealthGPT for adapting heterogeneous knowledge to pre-trained large language models?**\n",
            "   - Answer: The novel technique introduced in HealthGPT for adapting heterogeneous knowledge is called heterogeneous low-rank adaptation (H-LoRA).\n",
            "\n",
            "2. **What types of medical imaging tasks does HealthGPT excel in, according to the experimental results?**\n",
            "   - Answer: HealthGPT excels in various medical multi-modal comprehension tasks, such as X-Ray, CT, MRI, microscopy, OCT, fundus, and ultrasound comprehension, as well as generation tasks like CT2MRI, MRI2CT, image reconstruction, super resolution, and report-to-CXR.\n",
            "\n",
            "3. **What is the purpose of the VL-Health dataset mentioned in the context of HealthGPT?**\n",
            "   - Answer: The VL-Health dataset is devised to effectively learn the HealthGPT model by providing a comprehensive medical domain-specific comprehension and generation dataset.\n",
            "\n",
            "These questions focus on specific aspects of HealthGPT that are detailed in the excerpt and are unlikely to be found in other sources.\n",
            "Excerpt:\n",
            "-----\n",
            "HealthGPT: A Medical Large Vision-Language Model for Unifying\n",
            "Comprehension and Generation via Heterogeneous Knowledge Adaptation\n",
            "Tianwei Lin1, Wenqiao Zhang1, Sijing Li1, Yuqian Yuan1, Binhe Yu2, Haoyuan Li3, Wanggui He3, Hao Jiang3,\n",
            "Mengze Li4, Xiaohui Song1, Siliang Tang1, Jun Xiao1, Hui Lin1, Yueting Zhuang1, Beng Chin Ooi5\n",
            "1Zhejiang University, 2University of Electronic Science and Technology of China, 3Alibaba,\n",
            "4The Hong Kong University of Science and Technology,5National University of Singapore\n",
            "Project Page\n",
            " Code\n",
            "1. X-Ray \n",
            "Comprehension\n",
            "2. CT\n",
            "Comprehension\n",
            "3. MRI\n",
            "Comprehension\n",
            "Comp. Perf.\n",
            "7 Medical Multi-Modal Comprehension Tasks\n",
            "Gen. \n",
            "Performance\n",
            "5 Medical Multi-Modal Generation Tasks\n",
            "List all anatomical locations showing \n",
            "pulmonary edema, hazy opacity, or \n",
            "mediastinal displacement.\n",
            "Left hilar structures, left lung,\n",
            "right hilar structures, right lung.\n",
            "Which abdominal organ shows any \n",
            "indication of a lesion or abnormality \n",
            "in the CT image?\n",
            "No abdominal organs show\n",
            "any clear indications of lesions\n",
            "or abnormalities.\n",
            "Could you explain what this mass\n",
            "in the MRI means for my health? \n",
            "Is it very serious?\n",
            "Certainly, the MRI shows a\n",
            "defined mass in your left nasal\n",
            "cavity. There is no sign of the ...\n",
            "4. Microsopy Comprehension\n",
            " What is the purpose of the different \n",
            "membrane treatments used in this \n",
            "study?\n",
            "The purpose of the different\n",
            "membrane treatments used…\n",
            "5. OCT Comprehension\n",
            " What is the purpose of comparing \n",
            "the OCT structure image and OCTA \n",
            "image with H&E histology?\n",
            "To confirm the histological position\n",
            "of the obtained OCT brain images.\n",
            "6. Fundus\n",
            "Comprehension\n",
            " What specific findings or pathological \n",
            "changes can be observed in this \n",
            "fundus image?\n",
            "The fundus image appears normal with\n",
            "no noticeable signs of pathology…\n",
            "7. Ultrasound\n",
            "Comprehension\n",
            " What type of imaging technique \n",
            "is used in this image?\n",
            "The image is a sagittal gray-\n",
            "scale ultrasonographic…\n",
            "1. CT2MRI\n",
            "Generation\n",
            "I need a version of this CT representation \n",
            "in MRI.\n",
            "The image has\n",
            "been transformed\n",
            "into MRI.\n",
            "2. MRI2CT\n",
            "Generation\n",
            "Transform the MRI display into a \n",
            "CT image.\n",
            "Here is the CT\n",
            "version of the\n",
            "MRI image.\n",
            "3. Image Reconstruction\n",
            "Reconstruct the following \n",
            "medical images.\n",
            "Here is the reconstructed\n",
            "medical image you need.\n",
            "4. Super Resolution\n",
            "Could you improve the quality\n",
            "of this MRI image?\n",
            "Here is the image with\n",
            "improved resolution.\n",
            "5. Report-to-CXR\n",
            "The X-ray shows no \n",
            "pleural effusion or \n",
            "pneumothorax.\n",
            "Here is the\n",
            "chest X-ray\n",
            "image for\n",
            "you.\n",
            "Gen. Perf.\n",
            "Figure 1: HealthGPT enables medical multi-modal comprehension and generation , outperforming both state-of-the-art\n",
            "unified visual models and medical-specific models across various tasks. This highlights its superior capability in tackling com-\n",
            "plex tasks in healthcare applications. Comp.Perf. and Gen.Perf. denote the results of comprehension and generation.\n",
            "Abstract\n",
            "We present HealthGPT, a powerful Medical Large Vision-\n",
            "Language Model (Med-LVLM) that integrates medical vi-\n",
            "sual comprehension and generation capabilities within a uni-\n",
            "fied autoregressive paradigm. Our bootstrapping philosophy\n",
            "is to progressively adapt heterogeneous comprehension and\n",
            "generation knowledge to pre-trained large language mod-\n",
            "els (LLMs). This is achieved through a novel heterogeneous\n",
            "low-rank adaptation (H-LoRA) technique, which is com-\n",
            "plemented by a tailored hierarchical visual perception ap-\n",
            "proach and a three-stage learning strategy. To effectively\n",
            "learn the HealthGPT, we devise a comprehensive medi-\n",
            "cal domain-specific comprehension and generation dataset\n",
            "called VL-Health. Experimental results demonstrate ex-\n",
            "ceptional performance and scalability of HealthGPT in\n",
            "medical visual unified tasks. Our project can be accessed at\n",
            "https://github.com/DCDmllm/HealthGPT.\n",
            "1 Introduction\n",
            "Large Vision-Language Models (LVLMs) (Liu et al. 2023;\n",
            "OpenAI 2023; Liu et al. 2024c; Chen et al. 2024b) have\n",
            "demonstrated outstanding open-world visual comprehension\n",
            "and reasoning abilities through language-based interactive\n",
            "dialogue over the past years, simultaneously opening up\n",
            "new opportunities for applications in specialized domains.\n",
            "1\n",
            "arXiv:2502.09838v3  [cs.CV]  21 Feb 2025\n",
            "-----\n"
          ]
        }
      ],
      "source": [
        "print(nodes[0].get_content(metadata_mode=MetadataMode.LLM))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "GjNFmC6jTVCt"
      },
      "outputs": [],
      "source": [
        "%pip install -Uq llama-index-embeddings-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0Iny8ltTVFj",
        "outputId": "dc9deed1-7180-46c4-c1db-9f35babad21f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.0021344777196645737, -0.04909106716513634, 0.021025855094194412, 0.03132360428571701, -0.04531010240316391, -0.026405276730656624, -0.02897202968597412, 0.060341741889715195, -0.025713635608553886, -0.014808779582381248, 0.015446625649929047, -0.030047914013266563, -0.02039569430053234, -0.03338315337896347, 0.025821225717663765, 0.014232412911951542, -0.07002470642328262, 0.012418779544532299, 0.014824149198830128, 0.04884514957666397, 0.020749198272824287, -0.008837621659040451, -0.015123859979212284, -0.016614729538559914, 0.025959553197026253, -0.0028491723351180553, -0.024376465007662773, 0.024284247308969498, 0.001773288007825613, -0.05573080852627754, 0.02311614342033863, -0.04546380043029785, -0.00866855401545763, 0.0031392769888043404, 0.004530241712927818, 0.0017713668057695031, 0.026666563004255295, 0.010182477533817291, -0.012026850134134293, -0.011535017751157284, -0.014900998212397099, -0.023177623748779297, 0.025406241416931152, 0.036825984716415405, -0.035534922033548355, 0.021225661039352417, -0.06313904374837875, 0.040391772985458374, 0.05354830250144005, 0.06150984764099121, -0.03362907096743584, -0.006651270668953657, 0.02551382966339588, 0.10974020510911942, -0.00466472702100873, -0.03953106701374054, 0.007104679476469755, 0.0515194907784462, -0.026328427717089653, 0.027880774810910225, 0.03037067875266075, 0.020595500245690346, 0.01722951978445053, 0.012341930530965328, 0.0010528296697884798, 0.007035515271127224, -0.03707190230488777, 0.023531127721071243, -0.0106743099167943, 0.04082212597131729, -0.002113344380632043, 0.03140045329928398, -0.04275871813297272, -0.006735804490745068, -0.04721595346927643, -0.02174823358654976, -0.0456482358276844, 0.0010105628753080964, 9.347945888293907e-05, -0.04740039259195328, -0.03298354148864746, 0.02340817078948021, -0.052872031927108765, -0.056222643703222275, -0.022501353174448013, 0.018044117838144302, -0.035903796553611755, 0.0011844334658235312, -0.006001897621899843, -0.019827011972665787, 0.01124299131333828, 0.0009644535020925105, -0.020149776712059975, 0.04518714174628258, 0.04140617698431015, 0.00552159221842885, -0.007312171161174774, 0.03913145139813423, 0.05972695350646973, -0.008660868741571903, 0.027957623824477196, -0.023992221802473068, 0.026266949251294136, -0.015999937430024147, 0.00886836089193821, 0.011696400120854378, -0.00014829433348495513, -0.046170808374881744, -0.025021996349096298, 0.004603248089551926, -0.11213789135217667, -0.015369776636362076, 0.013494663871824741, -0.031200647354125977, 0.005164245143532753, 0.033075761049985886, 0.08145982027053833, -0.0574214868247509, 0.017306368798017502, -0.06793441623449326, 0.020841417834162712, 0.0026935534551739693, -0.017690613865852356, -0.05858958885073662, -0.05797479674220085, -0.0263130571693182, -0.010535981506109238, 0.0375329963862896, -0.041682835668325424, 0.007170001044869423, 0.02897202968597412, 0.026774151250720024, 0.016322702169418335, -0.0066435858607292175, -0.04703151807188988, 0.012749229557812214, -0.03562714159488678, -0.02285485714673996, -0.034397561103105545, 0.03971550241112709, 0.05370200052857399, -0.050996918231248856, 0.002747347578406334, -0.04199023172259331, -0.003471648320555687, -0.0419594906270504, -0.002084526000544429, 0.03301427885890007, -0.023208362981677055, 0.005636865738779306, 0.04386534169316292, 0.0585281103849411, -0.05800553783774376, 0.015246818773448467, 0.02940238267183304, -0.007300643716007471, 0.035350486636161804, -0.03504309058189392, -0.01113540306687355, 0.007408232428133488, 0.019903860986232758, 0.03190765529870987, -0.03661080822348595, -0.016138264909386635, -0.0114120589569211, 0.0035062304232269526, -0.0019500404596328735, 0.004134470131248236, -0.04773852601647377, 0.007070097140967846, 0.01722951978445053, 0.014124824665486813, -0.02303929440677166, -0.05619190260767937, -0.0135254031047225, 0.014970162883400917, 0.017429327592253685, -0.05241093784570694, -0.010681995190680027, -0.040729910135269165, 0.04152913764119148, 0.0629546046257019, 0.0050336020067334175, 0.01947350800037384, -0.039438847452402115, -0.013848168775439262, 0.004268955439329147, 0.02071845903992653, -0.0037540679331868887, 0.02443794533610344, -0.04850701615214348, 0.05692965164780617, 0.024945147335529327, -0.014824149198830128, 0.019242960959672928, -0.038117047399282455, 0.073590487241745, 0.010144053027033806, -0.008714663796126842, 0.031600259244441986, -0.014163249172270298, 0.0025821225717663765, -0.029894215986132622, -0.014124824665486813, -0.03427460044622421, -0.059942129999399185, 0.008399583399295807, 0.07568077743053436, -0.01238804031163454, -0.04285093769431114, 0.07205351442098618, -0.0193198099732399, -0.024422574788331985, -0.00012049664655933157, -0.0335061140358448, -0.007558087818324566, 0.025559939444065094, 0.016215113922953606, -0.0042920103296637535, -0.04023807495832443, -0.04395756125450134, 0.07051653414964676, 0.07463563233613968, 0.005679132416844368, 0.030893251299858093, 0.01470119133591652, 0.007070097140967846, -0.0051604025065898895, 0.03891627490520477, 0.009967300109565258, 0.016030676662921906, 0.012580161914229393, 0.019965339452028275, -0.034797172993421555, 0.0014245861675590277, -0.0061171711422502995, 0.03854740038514137, -0.020303474739193916, -0.014155563898384571, 0.0038770262617617846, -0.04973659664392471, 0.0004550894955173135, 0.019442766904830933, 0.036457110196352005, 0.04248206317424774, -0.013364020735025406, -0.03301427885890007, -0.050628043711185455, 0.015354407019913197, 0.02995569445192814, 0.0171219315379858, 0.038885533809661865, 0.005083553493022919, 0.011073923669755459, 0.034797172993421555, 0.0006133501301519573, -0.008745403029024601, 0.004803055431693792, 0.059757690876722336, -0.014831834472715855, -0.022532092407345772, -0.04927550256252289, -0.01741395704448223, -0.026220839470624924, -0.02105659432709217, -0.013901962898671627, 0.0015638747718185186, -0.0030297671910375357, 0.010297750122845173, -0.03854740038514137, 0.05111987888813019, -0.011611866764724255, 0.06356939673423767, -0.021471578627824783, 0.0005081632407382131, 0.023454278707504272, -0.01001340989023447, -0.012288136407732964, -0.040022898465394974, 0.03697968274354935, -0.02878759242594242, -0.0101209981366992, -0.011873152107000351, 0.04441865533590317, -0.06356939673423767, -0.0013544615358114243, 0.0563148632645607, -0.018228555098176003, 0.016722317785024643, -0.0017636818811297417, 0.017168041318655014, 0.04524862393736839, 0.03184617683291435, -0.012057590298354626, 0.009175756946206093, 0.012818394228816032, 0.007984599098563194, -0.017244890332221985, 0.02934090420603752, -0.02333132177591324, 0.07457415759563446, -0.00839189812541008, 0.017613764852285385, 0.014954792335629463, -0.009813602082431316, 0.03452051803469658, 0.023623347282409668, -0.051580969244241714, 0.06830328702926636, 0.017644504085183144, -0.03448978066444397, 0.004507186822593212, 0.046355247497558594, -0.024760710075497627, -0.004983650054782629, -0.03230727091431618, -0.004584035836160183, -0.038485921919345856, -0.01021321676671505, -0.04663190245628357, 0.028095951303839684, -0.05391717702150345, -0.0040307240560650826, 0.0037540679331868887, 0.026728041470050812, -0.008222830481827259, 0.05376347899436951, -0.0023458122741431, -0.008799197152256966, -0.005102765746414661, 0.014939422719180584, -0.034182384610176086, -0.012710805051028728, 0.00866855401545763, -0.0017242968315258622, -0.013241062872111797, -0.030970100313425064, 0.04546380043029785, -0.01696823351085186, 0.04776926711201668, -0.01586160995066166, -0.020119037479162216, 0.007208425085991621, -0.011166143231093884, 0.009852026589214802, -0.009091222658753395, 0.039592545479536057, -0.03996141999959946, 0.004714678972959518, 0.0023035453632473946, -0.0017550364136695862, 0.015846239402890205, 0.013148844242095947, -0.013494663871824741, -0.008453377522528172, -0.02145620808005333, 0.03722560033202171, -0.017951898276805878, -0.0007780949235893786, 0.02454553358256817, -0.01450138445943594, -0.021041223779320717, 0.0015609929105266929, -0.0006219956558197737, 0.05105839669704437, 0.004072990734130144, -0.01825929433107376, -0.05739074572920799, 0.0016426448710262775, 0.03836296126246452, -0.004023039247840643, 0.007792476564645767, 0.03150804340839386, -0.010958651080727577, -0.034397561103105545, -0.026989327743649483, 0.013287171721458435, -0.0059365760535001755, 0.04835331812500954, -0.009644534438848495, 0.02628231793642044, -0.005786721128970385, -0.016691576689481735, -0.013002831488847733, 0.011104663833975792, -0.016983604058623314, 0.01575402170419693, 0.013571512885391712, 0.0016618571244180202, -0.0008779984782449901, 0.05425531044602394, -0.012672380544245243, 0.02448405511677265, 0.010136367753148079, 0.00866855401545763, -0.011419744230806828, -0.0019634889904409647, -0.019811641424894333, -0.00533715495839715, -0.05517749860882759, 0.044633831828832626, -0.040729910135269165, 0.028311127796769142, 0.01744469627737999, -0.0644301027059555, -0.045586757361888885, -0.007043200079351664, 0.046908557415008545, 0.02013440802693367, -0.0274196807295084, 0.012472573667764664, -0.04143691807985306, 0.033106498420238495, 0.029387013986706734, -0.028772221878170967, -0.008015338331460953, -0.012710805051028728, -0.033444635570049286, 0.0064360941760241985, 0.014831834472715855, 0.0031834649853408337, 0.03547344356775284, -0.018412992358207703, 0.02702006697654724, -0.010812638327479362, 0.04408051818609238, 0.045955631881952286, -0.030831772834062576, 0.0011709848186001182, -0.06664334982633591, -0.014447590336203575, -0.012826078571379185, -0.021994151175022125, 0.014985532499849796, -0.0013141159433871508, -0.04346572980284691, -0.01477804034948349, -0.007788634393364191, 0.043189071118831635, 0.018674278631806374, 0.01194231677800417, 0.011073923669755459, -0.005755981430411339, -0.014908683486282825, -0.00876077264547348, -0.04475678876042366, -0.0083765285089612, -0.009467782452702522, -0.03433608263731003, -0.00793080497533083, 0.019427398219704628, -0.06658187508583069, 0.014462959952652454, 0.018136337399482727, -0.004856849554926157, 0.013471608981490135, 0.011127718724310398, -0.002409212524071336, -0.013878908008337021, 0.03378276899456978, -0.005971158389002085, -0.003177701262757182, -0.052103541791439056, 0.05019769072532654, 0.015415886417031288, -0.013287171721458435, -0.004219003487378359, 0.007496608421206474, -0.006720434874296188, -0.04057621210813522, -0.012749229557812214, 0.0035850005224347115, 0.019120002165436745, -0.013210322707891464, -0.031000839546322823, -0.005959630943834782, -0.030462898313999176, 0.009152702055871487, -0.02702006697654724, -0.0032603139989078045, 0.013456239365041256, -0.008845306932926178, 0.007558087818324566, 0.007170001044869423, 0.006989405956119299, 0.08453377336263657, -0.06486045569181442, 0.0016253539361059666, -0.03802482783794403, -0.016691576689481735, -0.021886562928557396, 0.06120245158672333, 0.028910549357533455, -0.020487911999225616, 0.001017287140712142, -0.04358868673443794, 0.006666640751063824, -0.025851964950561523, 0.014939422719180584, -0.04589415341615677, 0.004826109856367111, -0.012557107955217361, -0.011719455011188984, 1.726097934806603e-06, 0.04457235336303711, -0.019719423726201057, -0.0412217415869236, -0.02657434344291687, 0.02660508267581463, -0.02528328262269497, -0.038147784769535065, 0.012418779544532299, 0.0016455267323181033, -0.016138264909386635, 0.012049905024468899, -0.008975949138402939, 0.0023842365480959415, 0.018689649179577827, -0.009636850096285343, -0.02496051788330078, 0.01192694716155529, 0.035319745540618896, -0.019073892384767532, -0.03246096894145012, 0.01928906887769699, 0.010420708917081356, -0.032706886529922485, -0.010459133423864841, -0.0002352296287426725, 0.02675878070294857, 0.005494695156812668, -0.002343890955671668, 0.025129584595561028, -0.007000933401286602, -0.0222708061337471, -0.015769390389323235, -0.0014457196230068803, 0.011458168737590313, -0.004049936309456825, -0.028172800317406654, 0.01151964720338583, 0.019550355151295662, 0.028572414070367813, 0.010159422643482685, -0.023823153227567673, 0.0080076539888978, 0.03121601603925228, 0.011511962860822678, -0.0064822034910321236, 0.03857813775539398, 0.005352524574846029, -0.023900002241134644, -0.02274726890027523, -0.01341781485825777, 0.032922063022851944, -0.005279518198221922, -0.02240913361310959, -0.01201916579157114, -0.007923119701445103, 0.04140617698431015, 0.021963410079479218, 0.0005672408151440322, 0.023607976734638214, 0.009867396205663681, 0.002780008362606168, 0.03144656494259834, -0.020826047286391258, -0.04097582399845123, -0.024637751281261444, -0.038885533809661865, -0.014801095239818096, -0.02757337875664234, -0.028019102290272713, -0.019673313945531845, 0.0040307240560650826, 0.002436109585687518, 0.01649177074432373, 0.024945147335529327, 0.009014373645186424, -0.007235322613269091, -0.018428362905979156, -0.023085404187440872, -0.024653121829032898, 0.022977815940976143, -0.06175576150417328, 0.05874328687787056, 0.016799166798591614, 0.02234765514731407, 0.026451386511325836, -0.02377704530954361, 0.010205531492829323, 0.009252605959773064, 0.03261466696858406, 0.023254472762346268, -0.015200708992779255, -0.02874148264527321, -0.001605181023478508, -0.001142166554927826, -0.039469584822654724, 0.012065274640917778, 0.012603216804564, 0.0061632804572582245, 0.0076464638113975525, -0.04924476519227028, 0.01649177074432373, -0.024314986541867256, -0.02525254338979721, -0.010389968752861023, 0.007070097140967846, -0.029248684644699097, -0.032338012009859085, 0.038147784769535065, -0.00922186579555273, -0.04518714174628258, -0.0160153079777956, 0.031631000339984894, -0.012703120708465576, 0.004603248089551926, -0.004023039247840643, -0.019888490438461304, -0.011657975614070892, -0.065536729991436, 0.007577300071716309, 0.0253908708691597, 0.0003330918843857944, 0.012803023681044579, -0.0030912463553249836, -0.00620170496404171, -0.008330418728291988, 0.018674278631806374, 0.0360574945807457, 0.016169006004929543, 0.018582060933113098, -0.012195917777717113, -0.0101209981366992, -0.0037655953783541918, 0.0037694377824664116, -0.006340032909065485, -0.011581126600503922, -0.029617559164762497, 0.037563733756542206, 0.003210362046957016, 0.025928813964128494, -0.025882704183459282, 0.04847627505660057, 0.020995115861296654, 0.01755228452384472, 0.012902927584946156, 0.006658955942839384, -0.007116206455975771, 0.014063345268368721, 0.018705017864704132, 0.025483090430498123, -0.009029744192957878, 0.025314021855592728, 0.005414003971964121, -0.028280388563871384, -0.018059488385915756, -0.02491440810263157, 0.007723312824964523, -0.01604604721069336, 0.05785183981060982, -0.013755950145423412, -0.011096978560090065, -0.023054664954543114, -0.006574422121047974, -0.012510998174548149, 0.029786627739667892, -0.00515271769836545, -0.0010384205961599946, 0.0037886500358581543, -0.0018414913211017847, 0.039623282849788666, -0.004153682384639978, 0.04660116136074066, -0.006977878510951996, -0.009629164822399616, -0.04681634157896042, -0.006359245162457228, -0.033321674913167953, 0.006666640751063824, 0.03040141798555851, -0.04088360443711281, -0.03593453764915466, 0.00959074031561613, 0.01431694719940424, 0.013656046241521835, 0.018412992358207703, -0.0022305387537926435, -0.002989421598613262, -0.027465790510177612, -0.016630098223686218, 0.012457204051315784, 0.024284247308969498, -0.002251672325655818, -0.02878759242594242, 0.013079679571092129, 0.0009577292366884649, -0.011527332477271557, -0.05606894567608833, 0.028357237577438354, -0.01413250993937254, -0.013740580528974533, 0.006620531436055899, 0.0026474441401660442, -0.010681995190680027, 0.0027992206159979105, 0.025298653170466423, 0.03335241600871086, 0.0016887541860342026, -0.026051770895719528, -0.010843377560377121, -0.013041255995631218, -0.006074904464185238, -0.05142727121710777, 0.006236286833882332, -0.00942167267203331, -0.048015180975198746, 0.015208394266664982, 0.00942167267203331, -0.04509492591023445, 0.014954792335629463, -0.013287171721458435, 0.0035446546971797943, -0.010927910916507244, -0.008145981468260288, 0.009352508932352066, -0.024161288514733315, 0.02871074341237545, 0.018197815865278244, 0.023638715967535973, -0.01667620800435543, 0.026082511991262436, 0.03143119439482689, 0.0015667566331103444, -0.0038693412207067013, -0.050074733793735504, -0.010413023643195629, -0.006270868703722954, -0.03341389447450638, -0.01269543543457985, 0.0137098403647542, 0.016583988443017006, 0.03562714159488678, -0.003225731896236539, -0.019012413918972015, 0.0058558848686516285, 0.01759839430451393, -0.027696337550878525, 0.0015533081023022532, 0.004311222583055496, 0.013033570721745491, -0.025114215910434723, -0.008706978522241116, 0.009083538316190243, -0.03768669068813324, 0.003892395878210664, 0.000617192592471838, -0.007746367249637842, 0.025344761088490486, -0.006451463792473078, 0.022178588435053825, -0.007900064811110497, 0.040914345532655716, -0.018889455124735832, 0.011389004997909069, 0.023054664954543114, 0.005064341239631176, -0.001155615085735917, 0.005633023101836443, 0.04137543961405754, -0.012580161914229393, 0.010297750122845173, 0.006512942723929882, -0.0003326115838717669, -0.009667589329183102, -0.03897775337100029, 0.005314100068062544, -0.03156952187418938, -0.0021383201237767935, -0.037778910249471664, 0.005279518198221922, -0.02554456889629364, -0.016614729538559914, 0.006801126059144735, -0.03762521222233772, 0.006439936347305775, 0.018535951152443886, 0.019565725699067116, -0.039254408329725266, -0.021471578627824783, -0.03000180423259735, -0.011880837380886078, -0.030232351273298264, -0.002929863752797246, 0.05616116523742676, -0.013025885447859764, -0.041313961148262024, 0.01039765402674675, -0.004899116232991219, 0.012265081517398357, -0.02981736697256565, 0.030816402286291122, 0.02480681985616684, 0.002007676986977458, -0.012357300147414207, -0.008268940262496471, 0.0093832490965724, -0.00725069222971797, -0.005598441231995821, 0.006413039285689592, 0.0555771142244339, -0.015800129622220993, 0.005125820636749268, -0.0114120589569211, -0.050412867218256, 0.0131565285846591, -0.017383217811584473, 0.009168071672320366, 0.015277558006346226, 0.01275691483169794, -0.0025705951265990734, -0.03953106701374054, -0.038147784769535065, 0.06252425163984299, 0.018904825672507286, -0.01796726882457733, 0.02844945713877678, 0.012418779544532299, -0.022947076708078384, -0.013755950145423412, 0.014017236419022083, -0.007162315770983696, -0.010543666779994965, 0.0127876540645957, 0.042574282735586166, 0.002409212524071336, -0.024238137528300285, -0.029094986617565155, -0.023823153227567673, 0.016276594251394272, -0.00940630305558443, 0.01211138442158699, -0.012564792297780514, 0.059388816356658936, -0.006574422121047974, 0.01696823351085186, -0.008607074618339539, -0.0029087301809340715, 0.024929778650403023, -0.009429357945919037, -0.025052735581994057, 0.03087788075208664, -0.018090227618813515, -0.03562714159488678, 0.010912541300058365, -0.04494122788310051, -0.003102773567661643, -0.000663782237097621, -0.040760647505521774, -0.009291030466556549, 0.04500270634889603, 0.016353443264961243, 0.012649326585233212, 0.0233774296939373, -0.02451479434967041, 0.006908714771270752, 0.0009029743960127234, 0.03338315337896347, 0.031969133764505386, 0.022731900215148926, 0.0015014350647106767, -0.013264117762446404, -0.026989327743649483, 0.030816402286291122, 0.04623228684067726, -0.06313904374837875, 0.02657434344291687, -0.020487911999225616, -0.013802059926092625, 0.02124103158712387, 0.012518683448433876, 0.006524470169097185, -0.012349615804851055, -0.022624311968684196, -0.010881802067160606, -0.004084518179297447, 0.019796272739768028, 0.01553115900605917, -0.004430338274687529, 0.0006378457183018327, 0.003640715964138508, 0.004365016706287861, -0.007638779003173113, 0.005951946135610342, 0.00571371428668499, -0.0024899039417505264, 0.036272674798965454, 0.006459148600697517, 0.0206108707934618, -0.005871254485100508, -0.01533135212957859, -0.008630129508674145, 0.02734283357858658, 0.006124855950474739, -0.002610940719023347, -0.0014860653318464756, 0.0041459971107542515, 0.010920226573944092, 0.003928899299353361, -0.0030777978245168924, -0.047615569084882736, 0.023900002241134644, 0.006021109875291586, 0.04678560048341751, 0.0036791402380913496, -0.001558111165650189, 0.02443794533610344, 0.023823153227567673, -0.0004514871980063617, -0.01994997076690197, -0.03857813775539398, -0.018582060933113098, 0.01505469623953104, -0.012288136407732964, 0.013871223665773869, -0.04180579259991646, 0.06904103606939316, 0.013317911885678768, 0.006535997614264488, 0.03780965134501457, 0.015216078609228134, -0.028403347358107567, 0.01839762181043625, 0.011250676587224007, -0.01784431003034115, -0.04570971429347992, 0.033321674913167953, -0.01553115900605917, -0.01411714032292366, -0.029694408178329468, 0.01560800801962614, 0.011296785436570644, 0.006151753012090921, 0.06971731036901474, 0.015431256033480167, 0.001417861902154982, -0.0025052735581994057, -0.001542741316370666, -0.007546560373157263, -0.00839189812541008, -0.004438023082911968, 0.011342895217239857, -0.013725210912525654, -0.035534922033548355, 0.004042251501232386, 0.0592658594250679, -0.009168071672320366, -0.012895242311060429, 0.0005225724307820201, 0.002593649784103036, -0.003189228707924485, -0.02021125704050064, -0.0009231471922248602, -0.009114277549088001, -0.0158308707177639, -0.042789459228515625, -0.0187664981931448, -0.001034578075632453, 0.0036772191524505615, 0.0021171867847442627, -0.00780400400981307, 0.04325055330991745, 0.012380355037748814, -0.011235306970775127, 0.004522556904703379, -0.011657975614070892, 0.0202419962733984, 0.03264540433883667, 0.015546529553830624, 0.021855821833014488, 0.018935564905405045, 0.01420935895293951, -0.027834665030241013, -0.009075853042304516, -0.006970193702727556, 0.01076652854681015, 0.006420724093914032, -0.008315049111843109, -0.009267975576221943, -0.0007843389175832272, -0.009183441288769245, -0.039069972932338715, 0.02918720617890358, 0.02532939240336418, -0.018827976658940315, -0.022900966927409172, 0.010705049149692059, -0.026082511991262436, -0.000678671698551625, 0.04328129068017006, 0.027404312044382095, 0.022578202188014984, 0.01773672178387642, 0.0230085551738739, 0.0161997452378273, -0.003367902245372534, 0.022947076708078384, -0.00676270155236125, 0.009198810905218124, 0.017767461016774178, 0.02182508260011673, -0.023761674761772156, 0.0023419696372002363, 0.01183472853153944, 0.016169006004929543, -0.0112276216968894, 0.0202419962733984, -0.003777122590690851, -0.016368811950087547, -0.004518714267760515, -0.0046800971031188965, -0.011296785436570644, -0.022701160982251167, 0.0035119939129799604, -0.006628216244280338, 0.005406319163739681, 0.0005023995763622224, 0.008445692248642445, 0.047984443604946136, -0.019750162959098816, 0.007043200079351664, -0.0019634889904409647, 0.029801996424794197, 0.0012247790582478046, 0.03581158071756363, -0.030908621847629547, -0.026036402210593224, -0.046324506402015686, -0.0024437943939119577, -0.008046078495681286, -0.008906785398721695, -0.0032334167044609785, -0.004560981411486864, -0.0003146001254208386, -0.0006123895291239023, -0.03968476504087448, 0.02436109632253647, -0.0016954784514382482, -0.008737717755138874, -0.02002681978046894, 0.006935611832886934, -0.0195042472332716, 0.0019346706103533506, 0.011158457957208157, 0.021225661039352417, 0.0257597453892231, -0.0032122833654284477, -0.011542702093720436, 0.009513892233371735, -0.042574282735586166, -0.03087788075208664, -0.05185762792825699, 0.004341961815953255, 0.026328427717089653, -0.014747301116585732, 0.011112348176538944, 0.011273731477558613, 0.012180548161268234, 0.03996141999959946, -0.02657434344291687, 0.012849133461713791, -0.008622445166110992, 0.037041161209344864, -0.0062439716421067715, -0.026728041470050812, 0.011673345230519772, -0.030309200286865234, 0.020641610026359558, 0.0051949843764305115, -0.0014399559004232287, -0.013010515831410885, 0.015039326623082161, -0.02620546892285347, -0.022778009995818138, -0.01231119129806757, 0.030816402286291122, -0.011657975614070892, 0.022778009995818138, 0.018935564905405045, 0.012318875640630722, 0.0006575382431037724, -0.015008587390184402, -0.011304470710456371, -0.027004698291420937, 0.01578476093709469, 0.020918266847729683, 0.0272352434694767, 0.035903796553611755, -0.022701160982251167, 0.01332559622824192, 0.03648785129189491, 0.009529261849820614, -0.01029006578028202, -0.007020145654678345, 0.007108521647751331, -0.007900064811110497, 0.002975973067805171, -0.02422276884317398, 0.022132478654384613, 0.007892380468547344, 0.0031239071395248175, -0.013955757021903992, -0.004910643678158522, 0.029571451246738434, -0.006474518217146397, -0.02240913361310959, -0.013479294255375862, 0.0031834649853408337, -0.007792476564645767, 0.009621480479836464, -0.026113251224160194, 0.023254472762346268, -0.007830901071429253, -0.011511962860822678, 0.006528312806040049, 0.037410035729408264, 0.020810678601264954, -0.04414200037717819, 0.0007997087086550891, 0.017721353098750114, 0.0038635777309536934, 0.02749652974307537, -0.04285093769431114, 0.0072583770379424095, 0.04432643577456474, -0.020072927698493004, -0.010497557930648327, -0.023285211995244026, 0.02359260804951191, 0.03359833359718323, -0.02359260804951191, 0.0112276216968894, -0.0114120589569211, -0.049306243658065796, -0.008184405975043774, 0.015369776636362076, 0.13340966403484344, -0.022424504160881042, -0.02760411985218525, 0.0211641825735569, 0.018013378605246544, -0.003087403951212764, -0.014516754075884819, 0.007938489317893982, -0.004368858877569437, -0.008230515755712986, -0.0016868329839780927, -0.025160323828458786, -0.03697968274354935, -0.007308328989893198, -0.012541737407445908, 0.0195042472332716, -0.005141190253198147, -0.04444939270615578, -0.010735789313912392, -0.024376465007662773, 0.018136337399482727, -0.030309200286865234, -0.008791511878371239, 0.0076810456812381744, 0.011143088340759277, -0.0224552433937788, -0.016061415895819664, -0.011734824627637863, 0.010466817766427994, 0.05367125943303108, 0.009275659918785095, 0.0015340958489105105, 0.03295280039310455, 0.008545596152544022, -0.004122942686080933, 0.030155502259731293, -0.023100774735212326, -0.022147847339510918, -0.027558010071516037, 0.01194231677800417, -0.015500419773161411, 0.008438006974756718, 0.044111259281635284, -0.014009551145136356, -0.00959074031561613, -0.011327525600790977, 0.009959615767002106, -0.003438987536355853, 8.021102257771417e-05, -0.01240340992808342, -0.031600259244441986, 0.052872031927108765, 0.0038405228406190872, 0.007900064811110497, 0.011219937354326248, -0.00780400400981307, 0.033444635570049286, 0.034428298473358154, 0.05628412216901779, -0.004422653466463089, -0.007319855969399214, 0.002201720606535673, 0.014109455049037933, 0.023469649255275726, 0.015062381513416767, -0.0001833566348068416, -0.019350549206137657, 0.034981612116098404, -0.01001340989023447, 0.0393773689866066, -0.008107556961476803, 0.01678379625082016, 0.015408201143145561, 0.024576272815465927, 0.019565725699067116, 0.03731781616806984, -0.02554456889629364, 0.00040994075243361294, -0.016030676662921906, 0.030524376779794693, -0.013594567775726318, -0.01185009814798832, 0.013118104077875614, 0.018013378605246544, 0.018412992358207703, 0.011719455011188984, 0.007319855969399214, 0.019888490438461304, -0.012918297201395035, -0.02477608062326908, 0.007377492729574442, 0.002048022812232375, 0.025498459115624428, -0.012441834434866905, 0.001436113496311009, 0.0323994904756546, -0.049490682780742645, 0.01778283156454563, 0.00638614222407341, 0.016107525676488876, 0.014770355075597763, 0.023177623748779297, 0.004530241712927818, 0.022132478654384613, -0.006455306429415941, -0.007857798598706722, 0.003875104943290353, -0.04832257702946663, -0.04377312585711479, 0.004780000541359186, 0.0017531152116134763, -0.0156464334577322, -0.0021997992880642414, 0.025882704183459282, 0.04082212597131729, -0.06449158489704132, -0.028756851330399513, -0.025037366896867752, -0.0010528296697884798, 0.005417846143245697, 0.05437827110290527, 0.019888490438461304, 0.014985532499849796, -0.020779937505722046, 0.038731835782527924, -0.0010211295448243618, 0.05247241631150246, 0.0204264335334301, 0.045279361307621, -0.013310226611793041, -0.007800161838531494, -0.014201673679053783, 0.01681453548371792, 0.00885299127548933, 0.00647836085408926, 0.013294856995344162, 0.010451448149979115, -0.01752154529094696, 0.0021959568839520216, 0.016461031511425972, 0.0013803980546072125, -0.02237839438021183, 0.011166143231093884, -0.009882766753435135, -0.01361762173473835, 0.006685853004455566, 0.037164121866226196, -0.014931738376617432, -0.03218431398272514, -0.0537327378988266, -0.019166111946105957, -0.011950001120567322, 0.013241062872111797, -0.018044117838144302, 0.006309293210506439, 0.011980741284787655, -0.03470495715737343, -0.0026935534551739693, -0.00237463042140007, -0.023730935528874397, -0.03780965134501457, 0.004422653466463089, 0.007273746654391289, -0.0029990277253091335, -0.00965990498661995, 0.012718490324914455, -0.015369776636362076, 0.0016474479343742132, -0.022024890407919884, -0.007542717736214399, 0.04229762405157089, -0.04976733773946762, -0.022071000188589096, 0.03084714151918888, 0.026881739497184753, -0.013264117762446404, 0.006032637320458889, 0.002332363510504365, 0.029571451246738434, 0.01029006578028202, -0.045064184814691544, -0.026804890483617783, 0.03372129052877426, -0.010320805013179779, -0.023531127721071243, 0.024607012048363686, -0.017936529591679573, -0.0061171711422502995, -0.006693537812680006, -0.04226688668131828, 0.009437043219804764, -0.015323667787015438, -0.002601334825158119, -0.017721353098750114, -0.006555209867656231, -0.03069344349205494, -0.02053402177989483, -0.053025729954242706, -0.009560001082718372, -0.010182477533817291, -0.020749198272824287, -0.024607012048363686, 0.010651255026459694, 0.0020134407095611095, 0.013963442295789719, 0.02237839438021183, -0.015369776636362076, 0.008507171645760536, -0.0034812544472515583, -0.018351513892412186, 0.01150427758693695, 0.03230727091431618, 0.006121013779193163, 0.006063377019017935, 0.04377312585711479, 0.06280090659856796, -0.0041997916996479034, 0.020180515944957733, -7.666876626899466e-05, 0.0050681838765740395, 0.004084518179297447, -0.002578279934823513, 0.036088235676288605, -0.014632027596235275, -0.01019784715026617, 0.015369776636362076, 0.00391929317265749, -0.02039569430053234, -0.024929778650403023, 0.0040191966108977795, 0.025237172842025757, 0.03156952187418938, 0.018858715891838074, 0.04217466711997986, -0.003903923323377967, 0.013748264871537685, -0.004361174069344997, 0.00984434224665165, -0.00515271769836545, -0.011127718724310398, -0.008438006974756718, -0.02660508267581463, -0.007642621640115976, -0.010866432450711727, -0.014240098185837269, 0.0445416122674942, 0.039623282849788666, -0.01324874721467495, 0.0028338024858385324, 0.0005970197380520403, 0.010005724616348743, 0.032891321927309036, -0.017567655071616173, -0.008699293248355389, -0.016906755045056343, -0.006793441250920296, 0.004184421617537737, 0.016076786443591118, 0.01678379625082016, -0.003909687045961618, -0.03621119260787964, -0.012626271694898605, 0.023392800241708755, -0.03050900623202324, 0.006132540758699179, -0.03522752970457077, 0.04647820442914963, 0.020626239478588104, -0.0015485050389543176, 0.03243022784590721, 0.004368858877569437, -0.00849948637187481, 0.02124103158712387, -0.013971126638352871, -0.022962447255849838, 0.021486947312951088, 0.031338974833488464, -0.007365965284407139, 0.02105659432709217, -0.019335178658366203, -0.019196851179003716, -0.013556143268942833, -0.004330434370785952, 0.0060825892724096775, 0.007477396167814732, -0.002559067914262414, 0.011673345230519772, -0.009260290302336216, -0.02752727083861828, 0.000894328870344907, 0.05708334967494011, -0.0027050806675106287, 0.0015763627598062158, 0.025314021855592728, 0.016645468771457672, 0.023946112021803856, -0.02108733355998993, 0.02303929440677166, 0.03147730231285095, -0.008307364769279957, -0.03003254346549511, -0.000864069617819041, 0.023500388488173485, -0.018566690385341644, 0.008584020659327507, 0.027281353250145912, 0.036272674798965454, 0.006708907429128885, -0.024161288514733315, -0.01565411686897278, -0.002318914979696274, 0.014908683486282825, -0.0008030708413571119, -0.026804890483617783, 0.0028165115509182215, 0.0083765285089612, -0.020364955067634583, 0.029755888506770134, -0.022055629640817642, 0.03565788269042969, 0.022685790434479713, -0.00013100332580506802, 0.003296817187219858, 0.009183441288769245, 0.017828941345214844, -0.022947076708078384, 0.0021056593395769596, -0.016384182497859, 0.028464825823903084, -0.027742447331547737, 0.01477804034948349, 0.017106560990214348, 0.053025729954242706, -0.026774151250720024, 0.01689138449728489, -0.02274726890027523, -0.015400515869259834, 0.025851964950561523, 0.013294856995344162, -0.015308297239243984, 0.026482125744223595, 0.02638990618288517, 0.002605177229270339, 0.021118072792887688, 0.01715267077088356, 0.008430322632193565, 0.02248598262667656, -0.006113328505307436, -0.004945225547999144, 0.010359229519963264, -0.02678951993584633, 0.012426464818418026, 0.014163249172270298, 0.013740580528974533, -0.04023807495832443, 0.02090289629995823, -0.019550355151295662, -0.017398586496710777, 0.0026455228216946125, -0.026804890483617783, -0.02757337875664234, -0.03544270619750023, -0.012703120708465576, -0.0010720419231802225, -0.024653121829032898, 0.03694894164800644, -0.02094900608062744, -0.0003280486853327602, 0.026635823771357536, -0.028280388563871384, -0.002610940719023347, 0.0023054664488881826, 0.02219395712018013, -0.026451386511325836, 0.014770355075597763, -0.027035437524318695, -0.001371752587147057, -0.021302510052919388, 0.030155502259731293, 0.0069394540041685104, -0.005617653485387564, -0.017659872770309448, 0.007027830462902784, 0.004015353973954916, 0.01892019435763359, 0.007196898106485605, -0.0035254424437880516, -0.014009551145136356, -0.006113328505307436]\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# Example with text-embedding-3-small\n",
        "openai_embeddings = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Or with text-embedding-3-large for better quality\n",
        "# openai_embeddings = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "\n",
        "test_embed = openai_embeddings.get_text_embedding(\"Hello world\")\n",
        "print(test_embed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "wpmve0UETVIe"
      },
      "outputs": [],
      "source": [
        "# create index\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "index = VectorStoreIndex(nodes, embed_model=openai_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhOclA8ThLQF",
        "outputId": "3fac4bae-38db-4647-cfa8-6c1059794d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEEn9_f8PRBQ",
        "outputId": "d7c25dd6-3dab-4aaa-e081-967f8e46677f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model focuses on modality transformation and super-resolution reconstruction in medical imaging. It aims to convert images from one modality to another, such as from CT to MRI, and enhance the resolution of medical scans to restore essential image details. The model's effectiveness is evaluated through performance metrics and visual evidence demonstrating its capability in accurately transforming and reconstructing medical images.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm_querying = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llm_querying)\n",
        "response = query_engine.query(\"what does this model do? \")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6lQle_2PREF",
        "outputId": "aeac244d-82b7-4594-e7a3-fd70e44a12cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': \"The model focuses on modality transformation and super-resolution reconstruction in medical imaging. It aims to convert images from one modality to another, such as from CT to MRI, and enhance the resolution of medical scans to restore essential image details. The model's effectiveness is evaluated through performance metrics and visual evidence demonstrating its capability in accurately transforming and reconstructing medical images.\",\n",
              " 'source_nodes': [NodeWithScore(node=TextNode(id_='5ecdc967-6fef-4807-895e-0d25ff7b70c7', embedding=None, metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comparative Analysis of Modality Transformation and Super-Resolution Reconstruction in Medical Imaging: Performance Metrics and Human Evaluation\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"Comparative Analysis of Modality Transformation and Super-Resolution Reconstruction in Medical Imaging,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are the performance metrics (SSIM, PSNR, MSE) for the HealthGPT-M3 model in the MRI (Brain) reconstruction task?**\\n   - This question targets the specific performance metrics for a particular model and task, which is detailed in the table provided in the excerpt.\\n\\n2. **How does the performance of the Unified-IO model compare to the SEED-X model in the CT (Pelvis) reconstruction task based on the provided metrics?**\\n   - This question focuses on a comparative analysis of two specific models for a particular task, which is explicitly mentioned in the experimental results.\\n\\n3. **What visual evidence is provided in Figures 11 and 12 to support the effectiveness of the proposed method in modality transformation and super-resolution reconstruction?**\\n   - This question seeks to understand the qualitative results illustrated in the figures referenced in the excerpt, which are crucial for evaluating the method\\'s effectiveness.\\n\\nThese questions are tailored to extract specific information from the excerpt that is unlikely to be found in other sections of the document or in different sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5544de1c-168b-4191-8aa5-cd5eb4cdd619', node_type='4', metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='69b2a2d7bcc5d54d432fdb86523011f295e2dc2163aeb89b637895733be12461')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='C.6 Case Study\\nFigures 11 and 12 illustrate examples of modality transformation and super-resolution reconstruction. In Figure 11, the results\\ngenerated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively\\nguiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure 12\\ndemonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the\\nimage.\\nTable 11: The experimental results for the four reconstruction tasks.\\nCT(Brain) CT(Pelvis) MRI (Brain) MRI(Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\nSEED-X 20.18 27.66 112.11 21.53 28.02 102.87 4.90 27.62 112.86 6.31 27.89 106.21\\nUnified-IO 2 83.93 36.09 17.95 85.36 35.10 25.46 87.50 34.25 25.47 86.31 33.53 29.80\\nHealthGPT-M3 91.73 36.42 15.46 94.26 37.30 12.53 88.76 33.97 27.05 84.40 33.11 32.62\\n34.08\\n15.87\\n9.16\\n5.62\\n13.33\\n21.94\\nHealthGPT\\nLLaVA-Med\\nHuatuoGPT-Vision\\nLlama-3.2\\nInternVL-2\\nShow-o\\nHuman\\nEvaluation\\n(a) (b)\\nFigure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.\\n17', mimetype='text/plain', start_char_idx=0, end_char_idx=1208, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.3494817155569234),\n",
              "  NodeWithScore(node=TextNode(id_='98215f8a-0f6f-4515-8c91-432fa232f3bc', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comprehensive Evaluation of HealthGPT and Advanced Vision-Language Models in Medical Visual Comprehension and Modality Conversion Tasks\"', 'questions_this_excerpt_can_answer': \"Based on the provided excerpt, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What are the performance metrics (SSIM, PSNR, MSE) for HealthGPT-M3 and HealthGPT-L14 in the modality conversion tasks from CT to MRI for both brain and pelvis scans?**\\n   - This question targets the specific performance metrics of the HealthGPT models in a particular task, which is detailed in the table within the excerpt.\\n\\n2. **What datasets were utilized in the experiments for medical visual comprehension and modality conversion tasks in the study?**\\n   - This question seeks to identify the specific datasets mentioned in the context that were used for training and evaluation, which is crucial for understanding the experimental setup.\\n\\n3. **What model architecture and components were selected for the HealthGPT models, and how do they contribute to the model's adaptability across different tasks?**\\n   - This question focuses on the architectural choices and their implications for the model's performance, as described in the section about model details and the training strategy.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6d5a0570-973b-42f7-806f-dd3a7431788c', node_type='4', metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='41ef80bdf8bdf6da173d984639ed42defada78da75bf42b13d19e58704bcf6f0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e4278e05-0df6-4c34-9579-fb3769d9e7eb', node_type='1', metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='232719ef691a12285c1222e0448ea1a9f6e9182c4c75e0b14a8c0e64dc2312ba'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='fcb4ec01-76d3-4cd6-ac08-38050e91056f', node_type='1', metadata={}, hash='5111a362741ae96ee823a86637f1b0f39d8c50ecfd49992cfd7431625b5fd7d2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='CT to MRI (Brain) CT to MRI (Pelvis) MRI to CT (Brain) MRI to CT (Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\npix2pix 71.09 32.65 36.85 59.17 31.02 51.91 78.79 33.85 28.33 72.31 32.98 36.19\\nCycleGAN 54.76 32.23 40.56 54.54 30.77 55.00 63.75 31.02 52.78 50.54 29.89 67.78\\nBBDM 71.69 32.91 34.44 57.37 31.37 48.06 86.40 34.12 26.61 79.26 33.15 33.60\\nVmanba 69.54 32.67 36.42 63.01 31.47 46.99 79.63 34.12 26.49 77.45 33.53 31.85\\nDiffMa 71.47 32.74 35.77 62.56 31.43 47.38 79.00 34.13 26.45 78.53 33.68 30.51\\nHealthGPT-M3 79.38 33.03 33.48 71.81 31.83 43.45 85.06 34.40 25.49 84.23 34.29 27.99\\nHealthGPT-L14 79.73 33.10 32.96 71.92 31.87 43.09 85.31 34.29 26.20 84.96 34.14 28.13\\nmodality conversion). Notably, by this stage, the word em-\\nbedding layer and output head have been fine-tuned, only the\\nH-LoRA modules and adapter modules need to be trained.\\nThis strategy significantly improves the model’s adaptability\\nand flexibility across different tasks.\\n5 Experiments\\n5.1 Data and Experimental Setup\\nData Details. We curate VL-Health dataset (see Fig-\\nure 4). For medical visual comprehension, we leverage\\nmultiple medical-specific datasets, including PubMedVi-\\nsion (Chen et al. 2024a), LLaV A-Med (Li et al. 2024b),\\nPathVQA (He et al. 2020), MIMIC-CXR-VQA (Bae et al.\\n2024), SLAKE (Liu et al. 2021), and VQA-RAD (Lau\\net al. 2018). Additionally, we incorporate high-quality open-\\nworld data from LLaV A-1.5 (Liu et al. 2024b) to preserve\\nthe model’s general knowledge and instruction-following\\ncapabilities. For generation tasks, we construct a recon-\\nstruction dataset based on LLaV A-558k (Liu et al. 2024b),\\nand also explore two key tasks in personalized medical\\nimage enhancement—super-resolution and modality con-\\nversion—using the IXI (Davies et al. 2014) and Syn-\\nthRAD2023 (Thummerer et al. 2023) datasets. Detailed data\\nselection and instruction templates are in the Appendix.\\nModel Details. We select CLIP-L/14 (Radford et al. 2021)\\nas the visual encoder and used the hidden states of its\\nsecond and penultimate layers as concrete-grained and\\nabstract-grained features for model’s dynamic hierarchical\\nvisual perception. Drawing on the successful experiences of\\nLLaV A, we employ a MLP to align the multi-modal fea-\\nture embeddings. We choose the parameter-efficient phi-3-\\nmini (Abdin et al. 2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al.', mimetype='text/plain', start_char_idx=1293, end_char_idx=4042, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.3471436802736859)],\n",
              " 'metadata': {'5ecdc967-6fef-4807-895e-0d25ff7b70c7': {'page_label': '17',\n",
              "   'file_name': '2502.09838v3.pdf',\n",
              "   'file_path': '/content/data/2502.09838v3.pdf',\n",
              "   'file_type': 'application/pdf',\n",
              "   'file_size': 8786923,\n",
              "   'creation_date': '2025-09-01',\n",
              "   'last_modified_date': '2025-09-01',\n",
              "   'document_title': '\"Comparative Analysis of Modality Transformation and Super-Resolution Reconstruction in Medical Imaging: Performance Metrics and Human Evaluation\"',\n",
              "   'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"Comparative Analysis of Modality Transformation and Super-Resolution Reconstruction in Medical Imaging,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are the performance metrics (SSIM, PSNR, MSE) for the HealthGPT-M3 model in the MRI (Brain) reconstruction task?**\\n   - This question targets the specific performance metrics for a particular model and task, which is detailed in the table provided in the excerpt.\\n\\n2. **How does the performance of the Unified-IO model compare to the SEED-X model in the CT (Pelvis) reconstruction task based on the provided metrics?**\\n   - This question focuses on a comparative analysis of two specific models for a particular task, which is explicitly mentioned in the experimental results.\\n\\n3. **What visual evidence is provided in Figures 11 and 12 to support the effectiveness of the proposed method in modality transformation and super-resolution reconstruction?**\\n   - This question seeks to understand the qualitative results illustrated in the figures referenced in the excerpt, which are crucial for evaluating the method\\'s effectiveness.\\n\\nThese questions are tailored to extract specific information from the excerpt that is unlikely to be found in other sections of the document or in different sources.'},\n",
              "  '98215f8a-0f6f-4515-8c91-432fa232f3bc': {'page_label': '6',\n",
              "   'file_name': '2502.09838v3.pdf',\n",
              "   'file_path': '/content/data/2502.09838v3.pdf',\n",
              "   'file_type': 'application/pdf',\n",
              "   'file_size': 8786923,\n",
              "   'creation_date': '2025-09-01',\n",
              "   'last_modified_date': '2025-09-01',\n",
              "   'document_title': '\"Comprehensive Evaluation of HealthGPT and Advanced Vision-Language Models in Medical Visual Comprehension and Modality Conversion Tasks\"',\n",
              "   'questions_this_excerpt_can_answer': \"Based on the provided excerpt, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What are the performance metrics (SSIM, PSNR, MSE) for HealthGPT-M3 and HealthGPT-L14 in the modality conversion tasks from CT to MRI for both brain and pelvis scans?**\\n   - This question targets the specific performance metrics of the HealthGPT models in a particular task, which is detailed in the table within the excerpt.\\n\\n2. **What datasets were utilized in the experiments for medical visual comprehension and modality conversion tasks in the study?**\\n   - This question seeks to identify the specific datasets mentioned in the context that were used for training and evaluation, which is crucial for understanding the experimental setup.\\n\\n3. **What model architecture and components were selected for the HealthGPT models, and how do they contribute to the model's adaptability across different tasks?**\\n   - This question focuses on the architectural choices and their implications for the model's performance, as described in the section about model details and the training strategy.\"}}}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy9yPC0nh4nq",
        "outputId": "476a2d44-82b6-4f47-b18d-8ad8c5711437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NodeWithScore(node=TextNode(id_='5ecdc967-6fef-4807-895e-0d25ff7b70c7', embedding=None, metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comparative Analysis of Modality Transformation and Super-Resolution Reconstruction in Medical Imaging: Performance Metrics and Human Evaluation\"', 'questions_this_excerpt_can_answer': 'Based on the provided excerpt from the document titled \"Comparative Analysis of Modality Transformation and Super-Resolution Reconstruction in Medical Imaging,\" here are three specific questions that can be answered using the context:\\n\\n1. **What are the performance metrics (SSIM, PSNR, MSE) for the HealthGPT-M3 model in the MRI (Brain) reconstruction task?**\\n   - This question targets the specific performance metrics for a particular model and task, which is detailed in the table provided in the excerpt.\\n\\n2. **How does the performance of the Unified-IO model compare to the SEED-X model in the CT (Pelvis) reconstruction task based on the provided metrics?**\\n   - This question focuses on a comparative analysis of two specific models for a particular task, which is explicitly mentioned in the experimental results.\\n\\n3. **What visual evidence is provided in Figures 11 and 12 to support the effectiveness of the proposed method in modality transformation and super-resolution reconstruction?**\\n   - This question seeks to understand the qualitative results illustrated in the figures referenced in the excerpt, which are crucial for evaluating the method\\'s effectiveness.\\n\\nThese questions are tailored to extract specific information from the excerpt that is unlikely to be found in other sections of the document or in different sources.'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5544de1c-168b-4191-8aa5-cd5eb4cdd619', node_type='4', metadata={'page_label': '17', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='69b2a2d7bcc5d54d432fdb86523011f295e2dc2163aeb89b637895733be12461')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='C.6 Case Study\\nFigures 11 and 12 illustrate examples of modality transformation and super-resolution reconstruction. In Figure 11, the results\\ngenerated by our method in the CT (MRI) to MRI (CT) transformation task are highly close to the ground truth, effectively\\nguiding the model in the transformation across different regions. For the MRI super-resolution reconstruction task, Figure 12\\ndemonstrates the accuracy of our method in restoring scan image details, accurately reconstructing the essential details of the\\nimage.\\nTable 11: The experimental results for the four reconstruction tasks.\\nCT(Brain) CT(Pelvis) MRI (Brain) MRI(Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\nSEED-X 20.18 27.66 112.11 21.53 28.02 102.87 4.90 27.62 112.86 6.31 27.89 106.21\\nUnified-IO 2 83.93 36.09 17.95 85.36 35.10 25.46 87.50 34.25 25.47 86.31 33.53 29.80\\nHealthGPT-M3 91.73 36.42 15.46 94.26 37.30 12.53 88.76 33.97 27.05 84.40 33.11 32.62\\n34.08\\n15.87\\n9.16\\n5.62\\n13.33\\n21.94\\nHealthGPT\\nLLaVA-Med\\nHuatuoGPT-Vision\\nLlama-3.2\\nInternVL-2\\nShow-o\\nHuman\\nEvaluation\\n(a) (b)\\nFigure 10: (a) Proportion of model responses selected as the best in human evaluation. (b) Human Evaluation Dataset.\\n17', mimetype='text/plain', start_char_idx=0, end_char_idx=1208, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.3494817155569234),\n",
            " NodeWithScore(node=TextNode(id_='98215f8a-0f6f-4515-8c91-432fa232f3bc', embedding=None, metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01', 'document_title': '\"Comprehensive Evaluation of HealthGPT and Advanced Vision-Language Models in Medical Visual Comprehension and Modality Conversion Tasks\"', 'questions_this_excerpt_can_answer': \"Based on the provided excerpt, here are three specific questions that can be answered using the context, which are unlikely to be found elsewhere:\\n\\n1. **What are the performance metrics (SSIM, PSNR, MSE) for HealthGPT-M3 and HealthGPT-L14 in the modality conversion tasks from CT to MRI for both brain and pelvis scans?**\\n   - This question targets the specific performance metrics of the HealthGPT models in a particular task, which is detailed in the table within the excerpt.\\n\\n2. **What datasets were utilized in the experiments for medical visual comprehension and modality conversion tasks in the study?**\\n   - This question seeks to identify the specific datasets mentioned in the context that were used for training and evaluation, which is crucial for understanding the experimental setup.\\n\\n3. **What model architecture and components were selected for the HealthGPT models, and how do they contribute to the model's adaptability across different tasks?**\\n   - This question focuses on the architectural choices and their implications for the model's performance, as described in the section about model details and the training strategy.\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date', 'page_label'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='6d5a0570-973b-42f7-806f-dd3a7431788c', node_type='4', metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='41ef80bdf8bdf6da173d984639ed42defada78da75bf42b13d19e58704bcf6f0'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e4278e05-0df6-4c34-9579-fb3769d9e7eb', node_type='1', metadata={'page_label': '6', 'file_name': '2502.09838v3.pdf', 'file_path': '/content/data/2502.09838v3.pdf', 'file_type': 'application/pdf', 'file_size': 8786923, 'creation_date': '2025-09-01', 'last_modified_date': '2025-09-01'}, hash='232719ef691a12285c1222e0448ea1a9f6e9182c4c75e0b14a8c0e64dc2312ba'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='fcb4ec01-76d3-4cd6-ac08-38050e91056f', node_type='1', metadata={}, hash='5111a362741ae96ee823a86637f1b0f39d8c50ecfd49992cfd7431625b5fd7d2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='CT to MRI (Brain) CT to MRI (Pelvis) MRI to CT (Brain) MRI to CT (Pelvis)Model SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓ SSIM↑ PSNR↑ MSE↓\\npix2pix 71.09 32.65 36.85 59.17 31.02 51.91 78.79 33.85 28.33 72.31 32.98 36.19\\nCycleGAN 54.76 32.23 40.56 54.54 30.77 55.00 63.75 31.02 52.78 50.54 29.89 67.78\\nBBDM 71.69 32.91 34.44 57.37 31.37 48.06 86.40 34.12 26.61 79.26 33.15 33.60\\nVmanba 69.54 32.67 36.42 63.01 31.47 46.99 79.63 34.12 26.49 77.45 33.53 31.85\\nDiffMa 71.47 32.74 35.77 62.56 31.43 47.38 79.00 34.13 26.45 78.53 33.68 30.51\\nHealthGPT-M3 79.38 33.03 33.48 71.81 31.83 43.45 85.06 34.40 25.49 84.23 34.29 27.99\\nHealthGPT-L14 79.73 33.10 32.96 71.92 31.87 43.09 85.31 34.29 26.20 84.96 34.14 28.13\\nmodality conversion). Notably, by this stage, the word em-\\nbedding layer and output head have been fine-tuned, only the\\nH-LoRA modules and adapter modules need to be trained.\\nThis strategy significantly improves the model’s adaptability\\nand flexibility across different tasks.\\n5 Experiments\\n5.1 Data and Experimental Setup\\nData Details. We curate VL-Health dataset (see Fig-\\nure 4). For medical visual comprehension, we leverage\\nmultiple medical-specific datasets, including PubMedVi-\\nsion (Chen et al. 2024a), LLaV A-Med (Li et al. 2024b),\\nPathVQA (He et al. 2020), MIMIC-CXR-VQA (Bae et al.\\n2024), SLAKE (Liu et al. 2021), and VQA-RAD (Lau\\net al. 2018). Additionally, we incorporate high-quality open-\\nworld data from LLaV A-1.5 (Liu et al. 2024b) to preserve\\nthe model’s general knowledge and instruction-following\\ncapabilities. For generation tasks, we construct a recon-\\nstruction dataset based on LLaV A-558k (Liu et al. 2024b),\\nand also explore two key tasks in personalized medical\\nimage enhancement—super-resolution and modality con-\\nversion—using the IXI (Davies et al. 2014) and Syn-\\nthRAD2023 (Thummerer et al. 2023) datasets. Detailed data\\nselection and instruction templates are in the Appendix.\\nModel Details. We select CLIP-L/14 (Radford et al. 2021)\\nas the visual encoder and used the hidden states of its\\nsecond and penultimate layers as concrete-grained and\\nabstract-grained features for model’s dynamic hierarchical\\nvisual perception. Drawing on the successful experiences of\\nLLaV A, we employ a MLP to align the multi-modal fea-\\nture embeddings. We choose the parameter-efficient phi-3-\\nmini (Abdin et al. 2024) and phi-4 (Abdin et al. 2024) as the\\nbase model. For visual comprehension and generation tasks,\\nwe set the rank of H-LoRA to 16 and 64, with four experts.\\nAdditionally, we use the f8-8192 version of VQGAN as the\\nimage indexing and upsampling module.\\n5.2 Main Experiments\\nComprehension. We compare HealthGPT with several\\nexisting models, including medical-specific LVLMs (e.g.,\\nMed-Flamingo (Moor et al.', mimetype='text/plain', start_char_idx=1293, end_char_idx=4042, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.3471436802736859)]\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "pprint.pprint(response.source_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "RptBJdR4h4k1"
      },
      "outputs": [],
      "source": [
        "index.storage_context.persist(persist_dir=\"./vectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFcfr1yKh4h-",
        "outputId": "bfa7e84b-ded1-4aeb-a5cf-d28ad6a37c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ./vectors/docstore.json.\n",
            "Loading llama_index.core.storage.kvstore.simple_kvstore from ./vectors/index_store.json.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "# rebuild storage context\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./vectors\")\n",
        "\n",
        "# load index\n",
        "index_from_storage = load_index_from_storage(storage_context, embed_model=openai_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "qL4V8ylgh4fV"
      },
      "outputs": [],
      "source": [
        "qa = index_from_storage.as_query_engine(llm=llm_querying)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGV2BKc-h4cv",
        "outputId": "497bdefa-b140-4030-ddfc-dac8de866094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model is designed for adaptive multi-modal learning, focusing on enhancing medical image reconstruction and visual comprehension. It trains on image-VQ pairs to ensure pixel-level consistency between the outputs of a pre-trained language model and visual inputs. The training process includes a second stage that fine-tunes specific components to address bias and scale inconsistencies, while the third stage introduces task-specific data to improve performance in various downstream tasks, such as medical question answering, report generation, and image processing tasks like super-resolution and denoising.\n"
          ]
        }
      ],
      "source": [
        "response = qa.query(\"what does this model do?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmeVMCWDh4Z-",
        "outputId": "0770a06c-e6ae-49c7-eb3f-e4a487d6335c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m61.4/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -Uq chromadb\n",
        "%pip install -Uq llama-index-vector-stores-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "vwv8ZD1Ih4Xu"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "\n",
        "# initialize client, setting path to save data\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# create collection\n",
        "chroma_collection = db.get_or_create_collection(\"healthGPT\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# create your index\n",
        "index = VectorStoreIndex(\n",
        "    nodes, storage_context=storage_context, embed_model=openai_embeddings\n",
        ")\n",
        "\n",
        "# You can also load from documents and apply transformations in place\n",
        "# index = VectorStoreIndex.from_documents(\n",
        "#     documents, storage_context=storage_context, transformations=[]\n",
        "# )\n",
        "\n",
        "# Or you can initialize your index from your vector store and then add the nodes\n",
        "# index = VectorStoreIndex.from_vector_store(\n",
        "#     vector_store=vector_store, embed_model=openai_embeddings\n",
        "# )\n",
        "# index.insert_nodes(nodes)\n",
        "\n",
        "\n",
        "# create a query engine and query\n",
        "query_engine = index.as_query_engine(llm=llm_querying)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMRPmumth4U7",
        "outputId": "3b1808d3-3689-4d2f-fe4d-6b77a6fcfd09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model excels in super-resolution tasks, achieving high scores in performance metrics such as SSIM, PSNR, and MSE. It demonstrates superior performance in modality conversion tasks, particularly in transforming CT to MRI images, and effectively restores essential details in medical imaging. Additionally, it outperforms both medical-specific and general-purpose models in medical visual comprehension tasks, showcasing its capability in handling complex medical imaging challenges.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"What is this model good at?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myGtVX4Gh4SL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZqBHYZ4h4PV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUYNI3o5h4MJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "128566737cdb4a2f903f0e4e09d2ab9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44588a93f7744c4297533d51b56faade": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_128566737cdb4a2f903f0e4e09d2ab9b",
            "placeholder": "​",
            "style": "IPY_MODEL_8abbcaf169fc442cbe75c8e4a80cb9d7",
            "value": "Parsing nodes: 100%"
          }
        },
        "45f58ec4d3dd425d93202e88eb2e01b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53375159ab5a4d49a0c8516222fcd996": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b89590db5124cdf9081e8f279ad16d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c4818679d764e1989bff99c0fb6ea81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8abbcaf169fc442cbe75c8e4a80cb9d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7f2685485b94f34959998b0bfed8a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53375159ab5a4d49a0c8516222fcd996",
            "max": 32,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcfc6f87549b4c91a09cc0b48ea29393",
            "value": 32
          }
        },
        "f05e497a43a34c889b671d2b1cf13ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45f58ec4d3dd425d93202e88eb2e01b6",
            "placeholder": "​",
            "style": "IPY_MODEL_6b89590db5124cdf9081e8f279ad16d8",
            "value": " 32/32 [00:00&lt;00:00, 279.01it/s]"
          }
        },
        "f2800b59d087429991dcbce631b5b368": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44588a93f7744c4297533d51b56faade",
              "IPY_MODEL_a7f2685485b94f34959998b0bfed8a92",
              "IPY_MODEL_f05e497a43a34c889b671d2b1cf13ce6"
            ],
            "layout": "IPY_MODEL_7c4818679d764e1989bff99c0fb6ea81"
          }
        },
        "fcfc6f87549b4c91a09cc0b48ea29393": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
